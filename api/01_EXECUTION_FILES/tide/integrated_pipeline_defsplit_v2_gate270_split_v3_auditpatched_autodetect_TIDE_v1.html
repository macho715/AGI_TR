<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1 API documentation</title>
<meta name="description" content="INTEGRATED PIPELINE (Definition-split aware)
Purpose
- Orchestrate 4 scripts as one repeatable pipeline.
- Prevent the exact confusion you …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1</code></h1>
</header>
<section id="section-intro">
<p>==============================================================================
INTEGRATED PIPELINE (Definition-split aware)
==============================================================================
Purpose
- Orchestrate 4 scripts as one repeatable pipeline.
- Prevent the exact confusion you described: Forecast Tide vs Required WL, Draft vs
Freeboard/UKC, and mixed gate-sets.</p>
<p>Why the previous "download" failed (root cause)
- The prior response pasted code in-chat but did not create a physical file.
- Also, the earlier pipeline logic had a sequencing bug: it attempted to extract a
stage table BEFORE running Step-1 (so stage CSV could be missing and Step-3 fails).</p>
<p>This version fixes:
- Produces a real file (this script).
- Correct step order:
Step-1 (TR Excel) -&gt; Step-1b (stage_results.csv) -&gt; build stage_table.csv
-&gt; Step-2 (OPS integrated) -&gt; Step-3 (Gate solver) -&gt; Step-4 (Optimizer)
- Corrects x_from_midship sign convention when converting tank_catalog JSON:
x_from_midship = MIDSHIP_FROM_AP - LCG_from_AP
(Aligns with ops_final_r3_integrated_defs_split_v4.py logic)</p>
<p>Inputs expected in your working folder (or provide &ndash;inputs_dir):
- tank_catalog_from_tankmd.json
(object with {"tanks":[&hellip;]} format)
- bplus_inputs/Hydro_Table_Engineering.json
(or specify &ndash;hydro)
- (optional) stage_results.csv (will be generated by Step-1b if possible)</p>
<p>Scripts required (defaults assume same folder as this pipeline):
- agi_tr_patched_v6_6_defsplit_v1.py
- ops_final_r3_integrated_defs_split_v4.py
- ballast_gate_solver_v4.py
- Untitled-2_patched_defsplit_v1_1.py</p>
<h2 id="usage-examples">Usage Examples</h2>
<h1 id="run-all-agi-defaults">Run all (AGI defaults)</h1>
<p>python integrated_pipeline_defsplit_v2.py</p>
<h1 id="run-only-steps-13-skip-optimizer-step">Run only steps 1..3 (skip optimizer step)</h1>
<p>python integrated_pipeline_defsplit_v2.py &ndash;to_step 3</p>
<h1 id="use-custom-inputs-folder">Use custom inputs folder</h1>
<p>python integrated_pipeline_defsplit_v2.py &ndash;inputs_dir ./LCF</p>
<h1 id="override-gates-important-captain-aft-minimum">Override gates (important: Captain AFT minimum)</h1>
<p>python integrated_pipeline_defsplit_v2.py &ndash;fwd_max 2.70 &ndash;aft_min 2.70</p>
<h1 id="provide-tidedepth-for-ukc-checks-in-solver">Provide tide/depth for UKC checks in solver</h1>
<h2 id="python-integrated_pipeline_defsplit_v2py-forecast_tide-200-depth_ref-550-ukc_min-050">python integrated_pipeline_defsplit_v2.py &ndash;forecast_tide 2.00 &ndash;depth_ref 5.50 &ndash;ukc_min 0.50</h2>
<p>Notes
- This pipeline is site-agnostic. For AGI vs DAS, keep the same definition split,
but you may set different gates, depth_ref, etc.
==============================================================================</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.add_split_270_gates"><code class="name flex">
<span>def <span class="ident">add_split_270_gates</span></span>(<span>stage_df,<br>aft_min_m: float = 2.7,<br>fwd_max_m_cd: float = 2.7,<br>critical_only: bool = False,<br>critical_regex: str = '(preballast.*critical|6a.*critical|stage\\s*5.*preballast|stage\\s*6a)',<br>critical_stage_list: Optional[List[str]] = None,<br>tol_m: float = 0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_split_270_gates(
    stage_df,
    aft_min_m: float = CAPTAIN_AFT_MIN_DRAFT_M,
    fwd_max_m_cd: float = MAMMOET_FWD_MAX_DRAFT_M_CD,
    critical_only: bool = False,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    critical_stage_list: Optional[List[str]] = None,
    tol_m: float = 0.0,
):
    &#34;&#34;&#34;
    Add two split 2.70m gates into stage_df:
      - GateA_AFT_MIN_2p70_PASS (Captain): AFT &gt;= 2.70
      - GateB_FWD_MAX_2p70_CD_PASS (Mammoet): FWD &lt;= 2.70 for critical stages only
    Also adds margin columns (m) and applicability flag for Gate-B.

    Args:
        tol_m: Guard-band tolerance (m). Default 0.0 (strict). Use 0.02 for 2cm guard-band.
               Applied to both Gate-A (AFT &gt;= 2.70 - tol_m) and Gate-B (FWD &lt;= 2.70 + tol_m).
    &#34;&#34;&#34;
    df = stage_df.copy()

    # Stage / Draft column inference (adjust candidates if your df uses different names)
    col_stage = _infer_col(df, [&#34;Stage&#34;, &#34;stage&#34;, &#34;stage_name&#34;, &#34;StageName&#34;])
    col_fwd = _infer_col(
        df,
        [
            &#34;Draft_FWD_m&#34;,
            &#34;draft_fwd_m&#34;,
            &#34;new_fwd_m&#34;,
            &#34;New_FWD_m&#34;,
            &#34;FWD_m&#34;,
            &#34;fwd_m&#34;,
            &#34;fwd_draft_m&#34;,
            &#34;ForwardDraft_m&#34;,
        ],
    )
    col_aft = _infer_col(
        df,
        [
            &#34;Draft_AFT_m&#34;,
            &#34;draft_aft_m&#34;,
            &#34;new_aft_m&#34;,
            &#34;New_AFT_m&#34;,
            &#34;AFT_m&#34;,
            &#34;aft_m&#34;,
            &#34;aft_draft_m&#34;,
            &#34;AftDraft_m&#34;,
        ],
    )

    if col_fwd is None or col_aft is None:
        raise ValueError(
            f&#34;[add_split_270_gates] Cannot infer FWD/AFT draft columns. fwd={col_fwd}, aft={col_aft}&#34;
        )

    # Gate-B uses Chart Datum reference. Convert to CD if tide is available.
    col_tide = _infer_col(
        df,
        [&#34;Forecast_Tide_m&#34;, &#34;forecast_tide_m&#34;, &#34;Tide_m&#34;, &#34;tide_m&#34;],
    )
    col_fwd_gate = col_fwd
    if col_tide is not None:
        try:
            df[&#34;Draft_FWD_m_CD&#34;] = pd.to_numeric(
                df[col_fwd], errors=&#34;coerce&#34;
            ) - pd.to_numeric(df[col_tide], errors=&#34;coerce&#34;)
            col_fwd_gate = &#34;Draft_FWD_m_CD&#34;
        except Exception:
            col_fwd_gate = col_fwd

    # Gate-A (Captain): AFT &gt;= 2.70 (with guard-band tolerance)
    df[&#34;AFT_MIN_2p70_m&#34;] = float(aft_min_m)
    df[&#34;AFT_Margin_2p70_m&#34;] = (df[col_aft] - aft_min_m).round(2)
    # Guard-band 적용: v &gt;= -tol_m (tol_m=0.02면 2.68m 이상 PASS)
    df[&#34;Gate_AFT_MIN_2p70&#34;] = df[&#34;AFT_Margin_2p70_m&#34;].apply(
        lambda v: &#34;OK&#34; if pd.notna(v) and v &gt;= -tol_m else &#34;NG&#34;
    )
    df[&#34;Gate_AFT_MIN_2p70_PASS&#34;] = df[&#34;Gate_AFT_MIN_2p70&#34;] == &#34;OK&#34;
    # Legacy aliases
    df[&#34;GateA_AFT_MIN_2p70_PASS&#34;] = df[&#34;Gate_AFT_MIN_2p70_PASS&#34;]
    df[&#34;GateA_AFT_MIN_2p70_Margin_m&#34;] = df[&#34;AFT_Margin_2p70_m&#34;]

    # Gate-B (Mammoet): FWD &lt;= 2.70 (CD ref) for critical stages
    # ALWAYS compute applicability by critical matcher when stage column exists
    if col_stage is None:
        df[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;] = True
    else:
        df[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;] = df[col_stage].apply(
            lambda s: _is_critical_stage(
                s, patterns=critical_stage_list, regex=critical_regex
            )
        )
    df[&#34;FWD_MAX_applicable&#34;] = df[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;]

    # PASS rule: if not applicable -&gt; PASS (but excluded from fail count via applicability flag)
    df[&#34;GateB_FWD_MAX_2p70_CD_PASS&#34;] = True
    mask_app = df[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;] == True
    # Guard-band 적용: fwd_max_m_cd + tol_m까지 허용 (tol_m=0.02면 2.72m까지 PASS)
    df.loc[mask_app, &#34;GateB_FWD_MAX_2p70_CD_PASS&#34;] = (
        df.loc[mask_app, col_fwd_gate] &lt;= fwd_max_m_cd + tol_m
    )
    df[&#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34;] = (fwd_max_m_cd - df[col_fwd_gate]).round(2)
    df.loc[~mask_app, &#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34;] = None

    # SSOT labels (critical-only)
    df[&#34;FWD_MAX_2p70_m&#34;] = float(fwd_max_m_cd)
    df[&#34;Gate_B_Applies&#34;] = df[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;]
    df[&#34;FWD_Margin_2p70_m&#34;] = (fwd_max_m_cd - df[col_fwd_gate]).round(2)
    df[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] = &#34;N/A&#34;
    # Guard-band 적용: margin &gt;= -tol_m면 OK
    df.loc[mask_app, &#34;Gate_FWD_MAX_2p70_critical_only&#34;] = df.loc[
        mask_app, &#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34;
    ].apply(lambda v: &#34;OK&#34; if pd.notna(v) and v &gt;= -tol_m else &#34;NG&#34;)
    df[&#34;Gate_FWD_MAX_2p70_critical_only_PASS&#34;] = (
        df[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] == &#34;OK&#34;
    )

    return df</code></pre>
</details>
<div class="desc"><p>Add two split 2.70m gates into stage_df:
- GateA_AFT_MIN_2p70_PASS (Captain): AFT &gt;= 2.70
- GateB_FWD_MAX_2p70_CD_PASS (Mammoet): FWD &lt;= 2.70 for critical stages only
Also adds margin columns (m) and applicability flag for Gate-B.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tol_m</code></strong></dt>
<dd>Guard-band tolerance (m). Default 0.0 (strict). Use 0.02 for 2cm guard-band.
Applied to both Gate-A (AFT &gt;= 2.70 - tol_m) and Gate-B (FWD &lt;= 2.70 + tol_m).</dd>
</dl></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_dnv_mitigation_section_to_gate_report"><code class="name flex">
<span>def <span class="ident">append_dnv_mitigation_section_to_gate_report</span></span>(<span>report_md: Path, stage_qa_csv: Path, propeller_diameter_m: float = 1.38) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_dnv_mitigation_section_to_gate_report(
    report_md: Path,
    stage_qa_csv: Path,
    propeller_diameter_m: float = 1.38,
) -&gt; None:
    &#34;&#34;&#34;Append DNV mitigation measures for incomplete propeller immersion (Gate-A failures).&#34;&#34;&#34;
    if not report_md.exists() or not stage_qa_csv.exists():
        return

    try:
        import numpy as np

        existing = report_md.read_text(encoding=&#34;utf-8&#34;)
        if &#34;DNV Mitigation Measures (Incomplete Propeller Immersion)&#34; in existing:
            return
    except Exception:
        existing = &#34;&#34;

    try:
        qa = pd.read_csv(stage_qa_csv, encoding=&#34;utf-8-sig&#34;)

        gate_a_col = None
        if &#34;GateA_AFT_MIN_2p70_PASS&#34; in qa.columns:
            gate_a_col = &#34;GateA_AFT_MIN_2p70_PASS&#34;
        elif &#34;Gate_AFT_Min&#34; in qa.columns:
            gate_a_col = &#34;Gate_AFT_Min&#34;
        if gate_a_col is None:
            return

        def _is_fail(val) -&gt; bool:
            if isinstance(val, (bool, np.bool_)):
                return not bool(val)
            s = str(val).strip().lower()
            return s in (&#34;ng&#34;, &#34;fail&#34;, &#34;false&#34;, &#34;0&#34;)

        failing_stages = qa[qa[gate_a_col].apply(_is_fail)]
        if failing_stages.empty:
            return

        lines = []
        lines.append(&#34;\n---\n&#34;)
        lines.append(&#34;## 8) DNV Mitigation Measures (Incomplete Propeller Immersion)\n&#34;)
        lines.append(
            &#34;**Reference:** DNV Guidance on incomplete propeller immersion risk\n&#34;
        )
        lines.append(
            &#34;**Risk:** Aft bearing damage and propulsion loss due to excessive eccentric thrust.\n&#34;
        )
        lines.append(&#34;### Stages Requiring Mitigation:\n&#34;)

        for _, row in failing_stages.iterrows():
            stage_name = str(row.get(&#34;Stage&#34;, &#34;Unknown&#34;))
            aft_draft = row.get(&#34;Draft_AFT_m&#34;, &#34;N/A&#34;)
            aft_min = row.get(&#34;AFT_MIN_m&#34;, 2.70)
            deficit = None
            if pd.notna(aft_draft) and pd.notna(aft_min):
                try:
                    deficit = float(aft_min) - float(aft_draft)
                except Exception:
                    deficit = None

            lines.append(f&#34;#### {stage_name}&#34;)
            lines.append(f&#34;- AFT Draft: {aft_draft} m (Required: {aft_min} m)&#34;)
            if deficit is not None:
                lines.append(f&#34;- Deficit: {deficit:.2f} m&#34;)
            lines.append(&#34;&#34;)
            lines.append(&#34;Required Mitigation Measures (DNV):&#34;)
            lines.append(&#34;1. RPM/Power reduction (specify % based on engine guidance)&#34;)
            lines.append(&#34;2. Steering angle limitation (specify degrees)&#34;)
            lines.append(
                &#34;3. Tug standby (immediately available; operational control with tug)&#34;
            )
            lines.append(&#34;4. Abort criteria:&#34;)
            lines.append(&#34;   - AFT draft falls below 2.50 m&#34;)
            lines.append(
                &#34;   - Propeller shaft bearing temperature exceeds normal range&#34;
            )
            lines.append(&#34;   - Loss of propulsion effectiveness detected&#34;)
            lines.append(&#34;   - Weather exceeds operational limits&#34;)
            lines.append(&#34;5. Monitoring: 4-corner draft + bearing temperature&#34;)
            lines.append(&#34;&#34;)

        lines.append(&#34;### ITTC Shaft Centreline Immersion Note&#34;)
        lines.append(
            &#34;**Important:** Record shaft centreline immersion (not AFT draft) in approval docs.&#34;
        )
        lines.append(f&#34;- Propeller Diameter (D): {propeller_diameter_m} m&#34;)
        lines.append(f&#34;- Minimum: 1.5D = {1.5 * propeller_diameter_m:.2f} m&#34;)
        lines.append(f&#34;- Recommended: 2.0D = {2.0 * propeller_diameter_m:.2f} m&#34;)
        lines.append(&#34;- Calculation: immersion_shaftCL = draft_at_prop - z_shaftCL&#34;)
        lines.append(
            &#34;- Do not confuse AFT draft with shaft immersion in approval documentation.&#34;
        )

        report_md.write_text(existing + &#34;\n&#34;.join(lines), encoding=&#34;utf-8&#34;)
    except Exception as e:
        print(
            f&#34;[WARN] Failed to append DNV mitigation section: {type(e).__name__}: {e}&#34;
        )</code></pre>
</details>
<div class="desc"><p>Append DNV mitigation measures for incomplete propeller immersion (Gate-A failures).</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_solver_section_to_gate_report"><code class="name flex">
<span>def <span class="ident">append_solver_section_to_gate_report</span></span>(<span>report_md: Path,<br>solver_out_summary: Path,<br>solver_out_plan: Path,<br>solver_out_stage_plan: Path) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_solver_section_to_gate_report(
    report_md: Path,
    solver_out_summary: Path,
    solver_out_plan: Path,
    solver_out_stage_plan: Path,
) -&gt; None:
    &#34;&#34;&#34;Append solver output pointers + summary to an existing gate report.&#34;&#34;&#34;
    if not report_md.exists():
        return

    lines = []
    lines.append(&#34;\n---\n&#34;)
    lines.append(&#34;## 7) Step 3 (Gate Solver) 결과 요약\n&#34;)
    lines.append(
        f&#34;- solver_ballast_summary.csv: {solver_out_summary.name} ({&#39;OK&#39; if solver_out_summary.exists() else &#39;MISSING&#39;})&#34;
    )
    lines.append(
        f&#34;- solver_ballast_plan.csv: {solver_out_plan.name} ({&#39;OK&#39; if solver_out_plan.exists() else &#39;MISSING&#39;})&#34;
    )
    lines.append(
        f&#34;- solver_ballast_stage_plan.csv: {solver_out_stage_plan.name} ({&#39;OK&#39; if solver_out_stage_plan.exists() else &#39;MISSING&#39;})\n&#34;
    )

    if solver_out_summary.exists():
        try:
            s = pd.read_csv(solver_out_summary, encoding=&#34;utf-8-sig&#34;)
            if not s.empty:
                row = s.iloc[0].to_dict()
                keys = [
                    &#34;New_FWD_m&#34;,
                    &#34;New_AFT_m&#34;,
                    &#34;Freeboard_MIN_m&#34;,
                    &#34;UKC_m&#34;,
                    &#34;viol_fwd_max_m&#34;,
                    &#34;viol_aft_min_m&#34;,
                    &#34;viol_fb_min_m&#34;,
                    &#34;viol_ukc_min_m&#34;,
                ]
                lines.append(&#34;### Solver Summary (first row)\n&#34;)
                for k in keys:
                    if k in row:
                        lines.append(f&#34;- {k}: {row.get(k)}&#34;)
        except Exception as e:
            lines.append(
                f&#34;- [WARN] Solver summary parse failed: {type(e).__name__}: {e}&#34;
            )
    report_md.write_text(
        report_md.read_text(encoding=&#34;utf-8&#34;) + &#34;\n&#34;.join(lines), encoding=&#34;utf-8&#34;
    )</code></pre>
</details>
<div class="desc"><p>Append solver output pointers + summary to an existing gate report.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_site_profile_overrides"><code class="name flex">
<span>def <span class="ident">apply_site_profile_overrides</span></span>(<span>args: argparse.Namespace, profile: Dict[str, object], argv: List[str]) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_site_profile_overrides(
    args: argparse.Namespace, profile: Dict[str, object], argv: List[str]
) -&gt; None:
    &#34;&#34;&#34;Apply profile values to argparse args, unless user explicitly passed the CLI flag.&#34;&#34;&#34;
    keymap = {
        &#34;fwd_max_m&#34;: (&#34;fwd_max&#34;, &#34;--fwd_max&#34;),
        &#34;aft_min_m&#34;: (&#34;aft_min&#34;, &#34;--aft_min&#34;),
        &#34;aft_max_m&#34;: (&#34;aft_max&#34;, &#34;--aft_max&#34;),
        &#34;trim_abs_limit_m&#34;: (&#34;trim_abs_limit&#34;, &#34;--trim_abs_limit&#34;),
        &#34;pump_rate_tph&#34;: (&#34;pump_rate&#34;, &#34;--pump_rate&#34;),
        &#34;forecast_tide_m&#34;: (&#34;forecast_tide&#34;, &#34;--forecast_tide&#34;),
        &#34;depth_ref_m&#34;: (&#34;depth_ref&#34;, &#34;--depth_ref&#34;),
        &#34;ukc_min_m&#34;: (&#34;ukc_min&#34;, &#34;--ukc_min&#34;),
        &#34;squat_m&#34;: (&#34;squat&#34;, &#34;--squat&#34;),
        &#34;safety_allow_m&#34;: (&#34;safety_allow&#34;, &#34;--safety_allow&#34;),
        &#34;tank_keywords&#34;: (&#34;tank_keywords&#34;, &#34;--tank_keywords&#34;),
        &#34;current_t_csv&#34;: (&#34;current_t_csv&#34;, &#34;--current_t_csv&#34;),
        &#34;trim_limit_enforced&#34;: (&#34;trim_limit_enforced&#34;, &#34;--trim-limit-enforced&#34;),
        &#34;freeboard_min_enforced&#34;: (
            &#34;freeboard_min_enforced&#34;,
            &#34;--freeboard-min-enforced&#34;,
        ),
        &#34;freeboard_min_m&#34;: (&#34;freeboard_min_m&#34;, &#34;--freeboard-min-m&#34;),
    }
    for k, (attr, flag) in keymap.items():
        if k not in profile:
            continue
        if _argv_has_flag(flag, argv):
            continue
        if hasattr(args, attr):
            setattr(args, attr, profile[k])</code></pre>
</details>
<div class="desc"><p>Apply profile values to argparse args, unless user explicitly passed the CLI flag.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tank_overrides_from_profile"><code class="name flex">
<span>def <span class="ident">apply_tank_overrides_from_profile</span></span>(<span>tank_ssot_csv: Path,<br>profile: Optional[Dict[str, object]],<br>out_csv: Optional[Path] = None) ‑> Dict[str, object]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_tank_overrides_from_profile(
    tank_ssot_csv: Path,
    profile: Optional[Dict[str, object]],
    out_csv: Optional[Path] = None,
) -&gt; Dict[str, object]:
    &#34;&#34;&#34;
    Apply per-tank overrides from site profile JSON into Tank SSOT CSV.

    Expected profile structure (extra keys are ignored):
      {
        &#34;tank_overrides&#34;: {
          &#34;VOID3&#34;:    {&#34;mode&#34;:&#34;FIXED&#34;,&#34;use_flag&#34;:&#34;Y&#34;},
          &#34;FWCARGO1&#34;: {&#34;mode&#34;:&#34;FIXED&#34;,&#34;use_flag&#34;:&#34;N&#34;},
          &#34;FWCARGO2&#34;: {&#34;mode&#34;:&#34;FIXED&#34;,&#34;use_flag&#34;:&#34;N&#34;}
        }
      }

    Matching logic:
      - Exact match on Tank name (case-insensitive)
      - Base match is allowed only when explicitly enabled in override
        (e.g., {&#34;match&#34;:&#34;base&#34;})

    Supported override columns (if present in Tank SSOT):
      - use_flag (str)
      - mode (str)  [solver recognizes: FILL_DISCHARGE / DISCHARGE_ONLY / FILL_ONLY / FIXED / BLOCKED]
      - pump_rate_tph (float)
      - Min_t, Max_t (float)
      - priority_weight (float)
    &#34;&#34;&#34;
    stats: Dict[str, object] = {
        &#34;overrides_applied&#34;: 0,
        &#34;tanks_touched&#34;: 0,
        &#34;missing_keys&#34;: [],
        &#34;base_skipped&#34;: 0,
        &#34;base_skipped_keys&#34;: [],
    }
    if not profile or not isinstance(profile, dict):
        return stats

    overrides = (
        profile.get(&#34;tank_overrides&#34;)
        or profile.get(&#34;tank_override&#34;)
        or profile.get(&#34;tanks_overrides&#34;)
    )
    if not overrides or not isinstance(overrides, dict):
        return stats

    out_csv = out_csv or tank_ssot_csv
    df = pd.read_csv(tank_ssot_csv, encoding=&#34;utf-8-sig&#34;)

    def _norm(s: object) -&gt; str:
        return str(s).strip().upper()

    def _base(tank_name: object) -&gt; str:
        t = _norm(tank_name)
        return t.split(&#34;.&#34;, 1)[0] if &#34;.&#34; in t else t

    if &#34;Tank&#34; not in df.columns:
        raise ValueError(f&#34;Tank SSOT missing required column &#39;Tank&#39;: {tank_ssot_csv}&#34;)

    df[&#34;_tank_norm&#34;] = df[&#34;Tank&#34;].map(_norm)
    df[&#34;_tank_base&#34;] = df[&#34;_tank_norm&#34;].map(_base)

    editable_cols = {
        &#34;use_flag&#34;: &#34;str&#34;,
        &#34;mode&#34;: &#34;str&#34;,
        &#34;pump_rate_tph&#34;: &#34;float&#34;,
        &#34;Min_t&#34;: &#34;float&#34;,
        &#34;Max_t&#34;: &#34;float&#34;,
        &#34;priority_weight&#34;: &#34;float&#34;,
    }

    def _as_match_mode(ov: Dict[str, object]) -&gt; str:
        return str(ov.get(&#34;match&#34;, &#34;&#34;)).strip().lower()

    touched = set()
    processed_tanks = set()

    # Phase 1: exact matches only (keys with &#39;.&#39; OR match==&#34;exact&#34;)
    for key, ov in overrides.items():
        if not isinstance(ov, dict):
            continue
        k_norm = _norm(key)
        match_mode = _as_match_mode(ov)
        is_exact_only = (&#34;.&#34; in k_norm) or (match_mode == &#34;exact&#34;)
        if not is_exact_only:
            continue
        exact_mask = df[&#34;_tank_norm&#34;] == k_norm
        if not bool(exact_mask.any()):
            stats[&#34;missing_keys&#34;].append(key)
            continue
        for col, typ in editable_cols.items():
            if col not in ov or col not in df.columns:
                continue
            val = ov.get(col)
            if val is None:
                continue
            try:
                if typ == &#34;str&#34;:
                    df.loc[exact_mask, col] = str(val).strip()
                else:
                    df.loc[exact_mask, col] = float(val)
            except Exception:
                continue
        stats[&#34;overrides_applied&#34;] = int(stats[&#34;overrides_applied&#34;]) + 1
        for t in df.loc[exact_mask, &#34;_tank_norm&#34;].tolist():
            touched.add(t)
            processed_tanks.add(t)

    # Phase 2: base matches (explicitly enabled via match==&#34;base&#34; and not already processed by exact)
    for key, ov in overrides.items():
        if not isinstance(ov, dict):
            continue
        k_norm = _norm(key)
        match_mode = _as_match_mode(ov)
        if &#34;.&#34; in k_norm or match_mode == &#34;exact&#34;:
            continue
        if match_mode != &#34;base&#34;:
            stats[&#34;base_skipped&#34;] = int(stats[&#34;base_skipped&#34;]) + 1
            stats[&#34;base_skipped_keys&#34;].append(key)
            continue
        k_base = _base(k_norm)
        base_mask = (df[&#34;_tank_base&#34;] == k_base) &amp; (
            ~df[&#34;_tank_norm&#34;].isin(processed_tanks)
        )
        if not bool(base_mask.any()):
            stats[&#34;missing_keys&#34;].append(key)
            continue
        for col, typ in editable_cols.items():
            if col not in ov or col not in df.columns:
                continue
            val = ov.get(col)
            if val is None:
                continue
            try:
                if typ == &#34;str&#34;:
                    df.loc[base_mask, col] = str(val).strip()
                else:
                    df.loc[base_mask, col] = float(val)
            except Exception:
                continue
        stats[&#34;overrides_applied&#34;] = int(stats[&#34;overrides_applied&#34;]) + 1
        for t in df.loc[base_mask, &#34;_tank_norm&#34;].tolist():
            touched.add(t)
            processed_tanks.add(t)

    df.drop(columns=[&#34;_tank_norm&#34;, &#34;_tank_base&#34;], inplace=True, errors=&#34;ignore&#34;)
    df.to_csv(out_csv, index=False, encoding=&#34;utf-8-sig&#34;)

    stats[&#34;tanks_touched&#34;] = len(touched)
    return stats</code></pre>
</details>
<div class="desc"><p>Apply per-tank overrides from site profile JSON into Tank SSOT CSV.</p>
<p>Expected profile structure (extra keys are ignored):
{
"tank_overrides": {
"VOID3":
{"mode":"FIXED","use_flag":"Y"},
"FWCARGO1": {"mode":"FIXED","use_flag":"N"},
"FWCARGO2": {"mode":"FIXED","use_flag":"N"}
}
}</p>
<p>Matching logic:
- Exact match on Tank name (case-insensitive)
- Base match is allowed only when explicitly enabled in override
(e.g., {"match":"base"})</p>
<p>Supported override columns (if present in Tank SSOT):
- use_flag (str)
- mode (str)
[solver recognizes: FILL_DISCHARGE / DISCHARGE_ONLY / FILL_ONLY / FIXED / BLOCKED]
- pump_rate_tph (float)
- Min_t, Max_t (float)
- priority_weight (float)</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tide_ukc_profile_fallbacks"><code class="name flex">
<span>def <span class="ident">apply_tide_ukc_profile_fallbacks</span></span>(<span>args: argparse.Namespace, profile: Optional[Dict[str, object]], argv: List[str]) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_tide_ukc_profile_fallbacks(
    args: argparse.Namespace, profile: Optional[Dict[str, object]], argv: List[str]
) -&gt; None:
    &#34;&#34;&#34;Apply tide_ukc profile defaults when CLI flags are not provided.&#34;&#34;&#34;
    if not isinstance(profile, dict):
        return
    tide_ukc = profile.get(&#34;tide_ukc&#34;, {})
    if not isinstance(tide_ukc, dict):
        return
    keymap = {
        &#34;forecast_tide_m&#34;: (&#34;forecast_tide&#34;, &#34;--forecast_tide&#34;),
        &#34;depth_ref_m&#34;: (&#34;depth_ref&#34;, &#34;--depth_ref&#34;),
        &#34;ukc_min_m&#34;: (&#34;ukc_min&#34;, &#34;--ukc_min&#34;),
        &#34;squat_m&#34;: (&#34;squat&#34;, &#34;--squat&#34;),
        &#34;safety_allow_m&#34;: (&#34;safety_allow&#34;, &#34;--safety_allow&#34;),
    }
    for k, (attr, flag) in keymap.items():
        if _argv_has_flag(flag, argv):
            continue
        if hasattr(args, attr) and k in tide_ukc:
            current_val = getattr(args, attr)
            # For parameters with default 0.0 (squat_m, safety_allow_m),
            # apply profile value even if current value is 0.0 (not None)
            # For other parameters (forecast_tide_m, depth_ref_m, ukc_min_m),
            # only apply if current value is None
            if current_val is None or (
                k in (&#34;squat_m&#34;, &#34;safety_allow_m&#34;) and current_val == 0.0
            ):
                setattr(args, attr, tide_ukc[k])
                print(
                    f&#34;[OK] Applied {k}={tide_ukc[k]} from profile (was {current_val})&#34;
                )</code></pre>
</details>
<div class="desc"><p>Apply tide_ukc profile defaults when CLI flags are not provided.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.build_stage_table_from_stage_results"><code class="name flex">
<span>def <span class="ident">build_stage_table_from_stage_results</span></span>(<span>stage_results_csv: Path,<br>out_csv: Path,<br>fwd_max_m: float,<br>aft_min_m: float,<br>aft_max_m_for_optimizer: float,<br>trim_abs_limit_m: float,<br>critical_only: bool = False,<br>critical_stage_list: Optional[List[str]] = None,<br>critical_regex: str = '(preballast.*critical|6a.*critical|stage\\s*5.*preballast|stage\\s*6a)',<br>forecast_tide_m: Optional[float] = None,<br>depth_ref_m: Optional[float] = None,<br>ukc_min_m: Optional[float] = None,<br>d_vessel_m: float = 3.65,<br>hydro_json_path: Optional[Path] = None,<br>strict_hardstop: bool = False,<br>draft_tol_m: float = 1e-06,<br>hydro_disp_tol_t: float = 0.001) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_stage_table_from_stage_results(
    stage_results_csv: Path,
    out_csv: Path,
    fwd_max_m: float,
    aft_min_m: float,
    aft_max_m_for_optimizer: float,
    trim_abs_limit_m: float,
    critical_only: bool = False,
    critical_stage_list: Optional[List[str]] = None,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    forecast_tide_m: Optional[float] = None,
    depth_ref_m: Optional[float] = None,
    ukc_min_m: Optional[float] = None,
    d_vessel_m: float = D_VESSEL_M,
    hydro_json_path: Optional[Path] = None,
    strict_hardstop: bool = False,
    draft_tol_m: float = 1e-6,
    hydro_disp_tol_t: float = 1e-3,
) -&gt; Path:
    &#34;&#34;&#34;
    stage_results.csv -&gt; stage_table_unified.csv

    PATCH (HardStop + 신규 게이트 준비):
      - stage_results에서 Disp_t/Tmean_m/Trim_cm (가능하면) stage_table로 carry
      - Hydro table displacement 범위(min/max) 산출 후 stage_table에 기록
      - HardStop 플래그(HS_*) 계산
        * strict_hardstop=True이면 ValueError로 즉시 중단
        * strict_hardstop=False이면 HS 플래그만 남기고 다음 단계(QA)로 진행
    &#34;&#34;&#34;
    import math
    import numpy as np

    # Compatibility mapping: aft_max_m_for_optimizer -&gt; aft_max_m
    aft_max_m = aft_max_m_for_optimizer

    stage_results_csv = Path(stage_results_csv)
    out_csv = Path(out_csv)

    df = pd.read_csv(stage_results_csv)

    # -----------------------------
    # 1) Flexible column detection
    # -----------------------------
    stage_col = &#34;Stage&#34; if &#34;Stage&#34; in df.columns else None
    if stage_col is None:
        for c in df.columns:
            if str(c).strip().lower() in (&#34;stage_name&#34;, &#34;name&#34;):
                stage_col = c
                break
    if stage_col is None:
        raise ValueError(
            f&#34;[HARDSTOP] stage_results.csv missing Stage column. &#34;
            f&#34;Expected one of: Stage / stage_name / name. Columns={list(df.columns)}&#34;
        )

    dfwd_col = (
        &#34;Dfwd_m&#34;
        if &#34;Dfwd_m&#34; in df.columns
        else (&#34;FWD_m&#34; if &#34;FWD_m&#34; in df.columns else None)
    )
    if dfwd_col is None:
        for c in df.columns:
            if str(c).strip().lower() in (
                &#34;fwd draft(m)&#34;,
                &#34;fwd draft (m)&#34;,
                &#34;fwd_draft_m&#34;,
            ):
                dfwd_col = c
                break
    if dfwd_col is None:
        raise ValueError(
            f&#34;[HARDSTOP] stage_results.csv missing Forward draft column. &#34;
            f&#34;Expected one of: Dfwd_m / FWD_m / Fwd Draft(m). Columns={list(df.columns)}&#34;
        )

    daft_col = (
        &#34;Daft_m&#34;
        if &#34;Daft_m&#34; in df.columns
        else (&#34;AFT_m&#34; if &#34;AFT_m&#34; in df.columns else None)
    )
    if daft_col is None:
        for c in df.columns:
            if str(c).strip().lower() in (
                &#34;aft draft(m)&#34;,
                &#34;aft draft (m)&#34;,
                &#34;aft_draft_m&#34;,
            ):
                daft_col = c
                break
    if daft_col is None:
        raise ValueError(
            f&#34;[HARDSTOP] stage_results.csv missing Aft draft column. &#34;
            f&#34;Expected one of: Daft_m / AFT_m / Aft Draft(m). Columns={list(df.columns)}&#34;
        )

    # Optional fields (for traceability + new gate)
    disp_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            &#34;disp_t&#34;,
            &#34;disp&#34;,
            &#34;displacement_t&#34;,
            &#34;displacement&#34;,
        ):
            disp_col = c
            break

    tmean_col = None
    for c in df.columns:
        if str(c).strip().lower() in (&#34;tmean_m&#34;, &#34;tmean&#34;, &#34;mean_draft_m&#34;, &#34;mean_draft&#34;):
            tmean_col = c
            break

    trim_col = None
    for c in df.columns:
        if str(c).strip().lower() in (&#34;trim_cm&#34;, &#34;trim&#34;):
            trim_col = c
            break

    linkspan_fb_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            &#34;linkspan_freeboard_m&#34;,
            &#34;linkspan freeboard (m)&#34;,
            &#34;linkspan_freeboard&#34;,
        ):
            linkspan_fb_col = c
            break

    ramp_angle_col = None
    for c in df.columns:
        if str(c).strip().lower() in (&#34;ramp_angle_deg&#34;, &#34;ramp angle (deg)&#34;):
            ramp_angle_col = c
            break

    deck_flood_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            &#34;gate_deck_flooding_pass&#34;,
            &#34;deck_flooding_pass&#34;,
            &#34;gate_deck_flooding&#34;,
        ):
            deck_flood_col = c
            break

    # -----------------------------
    # 2) Build stage table (existing required columns)
    # -----------------------------
    # Draft 값 물리적 제한: Molded Depth를 초과하지 않도록 제한
    draft_fwd_raw = pd.to_numeric(df[dfwd_col], errors=&#34;coerce&#34;)
    draft_aft_raw = pd.to_numeric(df[daft_col], errors=&#34;coerce&#34;)

    # Molded Depth 초과 방지 (물리적 불가능 상태 제거)
    draft_fwd_clipped = draft_fwd_raw.clip(upper=float(d_vessel_m))
    draft_aft_clipped = draft_aft_raw.clip(upper=float(d_vessel_m))

    # 제한된 Stage 로깅
    clipped_mask = (draft_fwd_raw &gt; float(d_vessel_m)) | (
        draft_aft_raw &gt; float(d_vessel_m)
    )
    if clipped_mask.any():
        clipped_stages = df.loc[clipped_mask, stage_col].values
        print(
            f&#34;[WARNING] Draft values clipped to D_vessel ({d_vessel_m}m) for {len(clipped_stages)} stages:&#34;
        )
        for stage_name, fwd_raw, aft_raw, fwd_clipped, aft_clipped in zip(
            clipped_stages,
            draft_fwd_raw[clipped_mask],
            draft_aft_raw[clipped_mask],
            draft_fwd_clipped[clipped_mask],
            draft_aft_clipped[clipped_mask],
        ):
            if fwd_raw &gt; float(d_vessel_m) or aft_raw &gt; float(d_vessel_m):
                print(
                    f&#34;  - {stage_name}: FWD {fwd_raw:.2f} -&gt; {fwd_clipped:.2f}m, &#34;
                    f&#34;AFT {aft_raw:.2f} -&gt; {aft_clipped:.2f}m&#34;
                )

    out = pd.DataFrame(
        {
            &#34;Stage&#34;: df[stage_col].astype(str).str.strip(),
            &#34;Current_FWD_m&#34;: draft_fwd_clipped,
            &#34;Current_AFT_m&#34;: draft_aft_clipped,
            &#34;AFT_MIN_m&#34;: float(aft_min_m),
            &#34;D_vessel_m&#34;: float(d_vessel_m),
        }
    )

    fwd_limit = float(fwd_max_m) if fwd_max_m is not None else float(&#34;nan&#34;)
    if critical_only:
        if stage_col is None:
            out[&#34;FWD_MAX_applicable&#34;] = True
        else:
            out[&#34;FWD_MAX_applicable&#34;] = out[&#34;Stage&#34;].apply(
                lambda s: _is_critical_stage(
                    s, patterns=critical_stage_list, regex=critical_regex
                )
            )
        out[&#34;FWD_MAX_m&#34;] = np.where(out[&#34;FWD_MAX_applicable&#34;], fwd_limit, np.nan)
    else:
        out[&#34;FWD_MAX_applicable&#34;] = True
        out[&#34;FWD_MAX_m&#34;] = fwd_limit

    # Optimizer-only columns (keep existing meaning)
    out[&#34;FWD_Limit_m&#34;] = out[
        &#34;FWD_MAX_m&#34;
    ]  # SSOT: same as FWD_MAX_m unless overridden upstream
    out[&#34;AFT_Limit_m&#34;] = float(aft_max_m) if aft_max_m is not None else 3.50
    out[&#34;Trim_Abs_Limit_m&#34;] = (
        float(trim_abs_limit_m) if trim_abs_limit_m is not None else 0.50
    )

    # Definition-split inputs (optional)
    # 항상 컬럼을 추가하되, 값이 없으면 NaN으로 설정 (enrich_stage_table_with_tide_ukc에서 채움)
    if &#34;Forecast_Tide_m&#34; not in out.columns:
        out[&#34;Forecast_Tide_m&#34;] = np.nan
    if forecast_tide_m is not None:
        # CLI 값이 명시적으로 제공되면 모든 Stage에 직접 적용 (우선순위 최상)
        # 이렇게 하면 enrich_stage_table_with_tide_ukc에서 stage_tide_csv보다 CLI 값이 우선됨
        out[&#34;Forecast_Tide_m&#34;] = float(forecast_tide_m)
        print(
            f&#34;[OK] Applied forecast_tide_m={forecast_tide_m} to stage_table (CLI override, all stages)&#34;
        )

    if &#34;DepthRef_m&#34; not in out.columns:
        out[&#34;DepthRef_m&#34;] = np.nan
    if depth_ref_m is not None:
        out[&#34;DepthRef_m&#34;] = out[&#34;DepthRef_m&#34;].fillna(float(depth_ref_m))
        print(
            f&#34;[OK] Applied depth_ref_m={depth_ref_m} to stage_table (default for all stages)&#34;
        )

    if &#34;UKC_Min_m&#34; not in out.columns:
        out[&#34;UKC_Min_m&#34;] = np.nan
    if ukc_min_m is not None:
        out[&#34;UKC_Min_m&#34;] = out[&#34;UKC_Min_m&#34;].fillna(float(ukc_min_m))
        print(
            f&#34;[OK] Applied ukc_min_m={ukc_min_m} to stage_table (default for all stages)&#34;
        )

    # Carry trace fields (optional)
    if disp_col is not None:
        out[&#34;Input_Disp_t&#34;] = pd.to_numeric(df[disp_col], errors=&#34;coerce&#34;)
    if tmean_col is not None:
        out[&#34;Input_Tmean_m&#34;] = pd.to_numeric(df[tmean_col], errors=&#34;coerce&#34;)
    if trim_col is not None:
        out[&#34;Input_Trim_cm&#34;] = pd.to_numeric(df[trim_col], errors=&#34;coerce&#34;)
    if linkspan_fb_col is not None:
        out[&#34;Linkspan_freeboard_m&#34;] = pd.to_numeric(
            df[linkspan_fb_col], errors=&#34;coerce&#34;
        )
    if ramp_angle_col is not None:
        out[&#34;Ramp_angle_deg&#34;] = pd.to_numeric(df[ramp_angle_col], errors=&#34;coerce&#34;)
    if deck_flood_col is not None:
        out[&#34;Gate_Deck_Flooding_pass&#34;] = df[deck_flood_col].astype(str)

    # -----------------------------
    # 3) HARDSTOP flags: physical validity
    # -----------------------------
    out[&#34;HS_DraftNaN&#34;] = out[&#34;Current_FWD_m&#34;].isna() | out[&#34;Current_AFT_m&#34;].isna()
    out[&#34;HS_DraftNegative&#34;] = (out[&#34;Current_FWD_m&#34;] &lt; -draft_tol_m) | (
        out[&#34;Current_AFT_m&#34;] &lt; -draft_tol_m
    )
    out[&#34;HS_DraftOverDepth&#34;] = (
        out[&#34;Current_FWD_m&#34;] &gt; float(d_vessel_m) + draft_tol_m
    ) | (out[&#34;Current_AFT_m&#34;] &gt; float(d_vessel_m) + draft_tol_m)

    # -----------------------------
    # 4) HARDSTOP flags: hydro disp range (optional)
    # -----------------------------
    out[&#34;Hydro_Disp_Min_t&#34;] = math.nan
    out[&#34;Hydro_Disp_Max_t&#34;] = math.nan
    out[&#34;HS_HydroOutOfRange&#34;] = False

    def _auto_find_hydro_json(start: Path):
        candidates = [
            start.parent / &#34;bplus_inputs&#34; / &#34;Hydro_Table_Engineering.json&#34;,
            start.parent.parent / &#34;bplus_inputs&#34; / &#34;Hydro_Table_Engineering.json&#34;,
            start.parent / &#34;Hydro_Table_Engineering.json&#34;,
        ]
        for p in candidates:
            if p.exists():
                return p
        return None

    hydro_path = (
        Path(hydro_json_path)
        if hydro_json_path
        else _auto_find_hydro_json(stage_results_csv)
    )

    if disp_col is not None and hydro_path is not None and hydro_path.exists():
        try:
            with open(hydro_path, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
                hydro_obj = json.load(f)

            # Normalize to list[dict]
            rows = None
            if isinstance(hydro_obj, list):
                rows = hydro_obj
            elif isinstance(hydro_obj, dict):
                for k in (&#34;rows&#34;, &#34;table&#34;, &#34;data&#34;, &#34;HydroTable&#34;):
                    if k in hydro_obj and isinstance(hydro_obj[k], list):
                        rows = hydro_obj[k]
                        break
                if rows is None and all(
                    isinstance(v, dict) for v in hydro_obj.values()
                ):
                    rows = list(hydro_obj.values())

            disp_vals = []
            if isinstance(rows, list):
                for r in rows:
                    if not isinstance(r, dict):
                        continue
                    for k, v in r.items():
                        if str(k).strip().lower() in (
                            &#34;disp_t&#34;,
                            &#34;disp&#34;,
                            &#34;displacement_t&#34;,
                            &#34;displacement&#34;,
                        ):
                            try:
                                disp_vals.append(float(v))
                            except Exception:
                                pass
                            break

            if disp_vals:
                hmin = min(disp_vals)
                hmax = max(disp_vals)
                out[&#34;Hydro_Disp_Min_t&#34;] = hmin
                out[&#34;Hydro_Disp_Max_t&#34;] = hmax
                out[&#34;HS_HydroOutOfRange&#34;] = (
                    out[&#34;Input_Disp_t&#34;].isna()
                    | (out[&#34;Input_Disp_t&#34;] &lt; hmin - hydro_disp_tol_t)
                    | (out[&#34;Input_Disp_t&#34;] &gt; hmax + hydro_disp_tol_t)
                )
        except Exception:
            # If hydro parse fails, leave HS_HydroOutOfRange=False and range NaN
            pass

    out[&#34;HS_Any&#34;] = (
        out[&#34;HS_DraftNaN&#34;]
        | out[&#34;HS_DraftNegative&#34;]
        | out[&#34;HS_DraftOverDepth&#34;]
        | out[&#34;HS_HydroOutOfRange&#34;]
    )

    out.to_csv(out_csv, index=False, encoding=&#34;utf-8-sig&#34;)

    # 기본은 &#34;QA에서 HardStop&#34;으로 유도 (strict_hardstop=False)
    if strict_hardstop and bool(out[&#34;HS_Any&#34;].any()):
        bad = out.loc[
            out[&#34;HS_Any&#34;] == True, [&#34;Stage&#34;, &#34;Current_FWD_m&#34;, &#34;Current_AFT_m&#34;]
        ].copy()
        if &#34;Input_Disp_t&#34; in out.columns:
            bad[&#34;Input_Disp_t&#34;] = out.loc[out[&#34;HS_Any&#34;] == True, &#34;Input_Disp_t&#34;].values
        msg = [
            &#34;[HARDSTOP] Invalid stage_results detected while building stage_table_unified.csv&#34;,
            f&#34;  - stage_results_csv: {stage_results_csv}&#34;,
            f&#34;  - out_csv          : {out_csv}&#34;,
            &#34;  - failing stages (Stage, FWD, AFT, Disp if available):&#34;,
        ]
        for _, r in bad.iterrows():
            disp_str = (
                f&#34;, Disp={r[&#39;Input_Disp_t&#39;]}&#34; if &#34;Input_Disp_t&#34; in bad.columns else &#34;&#34;
            )
            msg.append(
                f&#34;    * {r[&#39;Stage&#39;]}: FWD={r[&#39;Current_FWD_m&#39;]}, AFT={r[&#39;Current_AFT_m&#39;]}{disp_str}&#34;
            )
        raise ValueError(&#34;\n&#34;.join(msg))

    return out_csv</code></pre>
</details>
<div class="desc"><p>stage_results.csv -&gt; stage_table_unified.csv</p>
<p>PATCH (HardStop + 신규 게이트 준비):
- stage_results에서 Disp_t/Tmean_m/Trim_cm (가능하면) stage_table로 carry
- Hydro table displacement 범위(min/max) 산출 후 stage_table에 기록
- HardStop 플래그(HS_*) 계산
* strict_hardstop=True이면 ValueError로 즉시 중단
* strict_hardstop=False이면 HS 플래그만 남기고 다음 단계(QA)로 진행</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.check_dependencies"><code class="name flex">
<span>def <span class="ident">check_dependencies</span></span>(<span>base_dir: Path) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_dependencies(base_dir: Path) -&gt; bool:
    &#34;&#34;&#34;
    Check and auto-install required packages from requirements.txt.
    Returns True if all core dependencies are available, False otherwise.
    &#34;&#34;&#34;
    requirements_file = base_dir / &#34;requirements.txt&#34;

    # Core dependencies required for pipeline execution
    core_packages = {
        &#34;pandas&#34;: &#34;pandas&gt;=1.5.0&#34;,
        &#34;numpy&#34;: &#34;numpy&gt;=1.23.0&#34;,
        &#34;openpyxl&#34;: &#34;openpyxl&gt;=3.0.0&#34;,
        &#34;scipy&#34;: &#34;scipy&gt;=1.9.0&#34;,
    }

    if not requirements_file.exists():
        print(f&#34;[INFO] Creating requirements.txt at {requirements_file}&#34;)
        try:
            with open(requirements_file, &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
                f.write(&#34;# Core dependencies for AGI RORO TR Pipeline\n&#34;)
                for spec in core_packages.values():
                    f.write(f&#34;{spec}\n&#34;)
                f.write(&#34;\n# Optional dependencies\n&#34;)
                f.write(&#34;pyyaml&gt;=6.0\n&#34;)
                f.write(&#34;loguru&gt;=0.6.0\n&#34;)
            print(&#34;[OK] Created requirements.txt&#34;)
        except Exception as e:
            print(f&#34;[WARN] Failed to create requirements.txt: {e}&#34;)

    missing_specs = []
    missing_names = []

    for import_name, package_spec in core_packages.items():
        try:
            __import__(import_name)
        except ImportError:
            missing_specs.append(package_spec)
            missing_names.append(package_spec.split(&#34;&gt;=&#34;)[0])

    if missing_specs:
        print(f&#34;[INFO] Missing packages detected: {&#39;, &#39;.join(missing_names)}&#34;)
        print(&#34;[INFO] Auto-installing dependencies...&#34;)
        try:
            install_cmd = [
                sys.executable,
                &#34;-m&#34;,
                &#34;pip&#34;,
                &#34;install&#34;,
                &#34;--quiet&#34;,
                &#34;--upgrade&#34;,
            ] + missing_specs
            result = subprocess.run(
                install_cmd,
                check=False,
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                print(&#34;[ERROR] Failed to install packages:&#34;)
                print(f&#34;  stdout: {result.stdout}&#34;)
                print(f&#34;  stderr: {result.stderr}&#34;)
                print(
                    f&#34;[INFO] Please install manually: pip install {&#39; &#39;.join(missing_specs)}&#34;
                )
                return False
            print(f&#34;[OK] Successfully installed: {&#39;, &#39;.join(missing_names)}&#34;)
        except subprocess.TimeoutExpired:
            print(&#34;[ERROR] Installation timeout (exceeded 5 minutes)&#34;)
            return False
        except Exception as e:
            print(f&#34;[ERROR] Installation failed: {e}&#34;)
            print(
                f&#34;[INFO] Please install manually: pip install {&#39; &#39;.join(missing_specs)}&#34;
            )
            return False

        still_missing = []
        for import_name in core_packages:
            try:
                __import__(import_name)
            except ImportError:
                still_missing.append(import_name)
        if still_missing:
            print(
                f&#34;[ERROR] Some packages still missing after installation: {&#39;, &#39;.join(still_missing)}&#34;
            )
            return False

    # Try to read requirements.txt to check version requirements (optional)
    try:
        with open(requirements_file, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
            requirements_content = f.read()
            # Check for core packages in requirements.txt
            for spec in core_packages.values():
                if spec.split(&#34;&gt;=&#34;)[0] not in requirements_content:
                    print(f&#34;[WARN] {spec} not found in requirements.txt&#34;)
    except Exception as e:
        print(f&#34;[WARN] Could not read requirements.txt: {e}&#34;)

    print(
        f&#34;[OK] Core dependencies check passed (requirements.txt: {requirements_file})&#34;
    )
    return True</code></pre>
</details>
<div class="desc"><p>Check and auto-install required packages from requirements.txt.
Returns True if all core dependencies are available, False otherwise.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.collect_all_output_files"><code class="name flex">
<span>def <span class="ident">collect_all_output_files</span></span>(<span>base_dir: Path, out_dir: Path, site: str, inputs_dir: Optional[Path] = None) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collect_all_output_files(
    base_dir: Path, out_dir: Path, site: str, inputs_dir: Optional[Path] = None
) -&gt; None:
    &#34;&#34;&#34;
    Collect all output files generated by pipeline steps into the output directory.
    모든 파이프라인 단계에서 생성된 출력 파일을 출력 디렉토리로 수집합니다.
    Also copies input/dependency files to outputs/inputs/ subdirectory.
    의존성 파일 및 기본 로우 데이터 파일도 outputs/inputs/ 하위 디렉토리로 복사합니다.
    &#34;&#34;&#34;
    collected_count = 0

    # Step 1 outputs (TR Excel 및 관련 파일)
    # TR 스크립트는 base_dir.parent에 파일을 생성할 수 있음
    step1_patterns = [
        f&#34;LCT_BUSHRA_{site}_TR_Final_v*.xlsx&#34;,
        &#34;RORO_Summary.png&#34;,
    ]

    # base_dir과 base_dir.parent 모두 검색
    search_dirs = [base_dir, base_dir.parent]

    for pattern in step1_patterns:
        for search_dir in search_dirs:
            for src_file in search_dir.glob(pattern):
                if src_file.is_file():
                    dest_file = out_dir / src_file.name
                    # 이미 수집된 파일은 스킵
                    if dest_file.exists():
                        continue
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-1 output: {src_file.name} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {src_file.name}: {type(e).__name__}: {e}&#34;
                        )

    # Step 2 outputs (이미 수집되었을 수 있음)
    # Step 2 스크립트도 base_dir.parent에 파일을 생성할 수 있음
    step2_files = [
        f&#34;OPS_FINAL_R3_{site}_Ballast_Integrated.xlsx&#34;,
        &#34;OPS_FINAL_R3_Report_Integrated.md&#34;,
    ]

    for filename in step2_files:
        # base_dir과 base_dir.parent 모두 검색
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-2 output: {filename} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                        break  # 파일을 찾았으면 다음 파일로
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {filename}: {type(e).__name__}: {e}&#34;
                        )

    # Step 0 outputs (SPMT unified)
    step0_files = [
        &#34;AGI_SPMT_Shuttle_Output.xlsx&#34;,
        &#34;stage_loads.csv&#34;,
        &#34;stage_summary.csv&#34;,
    ]

    for filename in step0_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-0 output: {filename} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {filename}: {type(e).__name__}: {e}&#34;
                        )

    # Step 5 outputs (Bryan template)
    step5_files = [
        &#34;Bryan_Submission_Data_Pack_Template.xlsx&#34;,
        &#34;Bryan_Submission_Data_Pack_Populated.xlsx&#34;,
    ]

    for filename in step5_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-5 output: {filename} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {filename}: {type(e).__name__}: {e}&#34;
                        )

    # Step 4b outputs (Ballast Sequence &amp; Checklist)
    step4b_files = [
        &#34;BALLAST_SEQUENCE.csv&#34;,
        &#34;BALLAST_OPTION.csv&#34;,
        &#34;BALLAST_EXEC.csv&#34;,
        &#34;BALLAST_SEQUENCE.xlsx&#34;,
        &#34;BALLAST_OPERATIONS_CHECKLIST.md&#34;,
        &#34;HOLD_POINT_SUMMARY.csv&#34;,
    ]

    for filename in step4b_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-4b output: {filename} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {filename}: {type(e).__name__}: {e}&#34;
                        )

    # Step 4c outputs (Valve Lineup)
    step4c_files = [
        &#34;BALLAST_SEQUENCE_WITH_VALVES.md&#34;,
    ]

    for filename in step4c_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f&#34;[OK] Collected Step-4c output: {filename} -&gt; {out_dir.name}/ (from {search_dir.name})&#34;
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to collect {filename}: {type(e).__name__}: {e}&#34;
                        )

    # Copy input/dependency files (의존성 파일 및 기본 로우 데이터 파일)
    if inputs_dir is None:
        inputs_dir = base_dir

    input_files = [
        &#34;tank_catalog_from_tankmd.json&#34;,
        &#34;bplus_inputs/Hydro_Table_Engineering.json&#34;,
        &#34;GM_Min_Curve.json&#34;,
        &#34;Acceptance_Criteria.json&#34;,
        &#34;Structural_Limits.json&#34;,
        &#34;Securing_Input.json&#34;,
        &#34;ISCODE_Criteria.json&#34;,
    ]

    # Create inputs subdirectory in output
    inputs_out_dir = out_dir / &#34;inputs&#34;
    inputs_out_dir.mkdir(exist_ok=True)

    for input_file in input_files:
        src_file = inputs_dir / input_file
        if src_file.exists() and src_file.is_file():
            # Preserve directory structure for bplus_inputs
            if input_file.startswith(&#34;bplus_inputs/&#34;):
                dest_file = inputs_out_dir / input_file
                dest_file.parent.mkdir(parents=True, exist_ok=True)
            else:
                dest_file = inputs_out_dir / Path(input_file).name

            if not dest_file.exists():
                try:
                    shutil.copy2(src_file, dest_file)
                    print(f&#34;[OK] Collected input file: {input_file} -&gt; inputs/&#34;)
                    collected_count += 1
                except Exception as e:
                    print(
                        f&#34;[WARN] Failed to collect {input_file}: {type(e).__name__}: {e}&#34;
                    )

    if collected_count &gt; 0:
        print(f&#34;[OK] Total {collected_count} file(s) collected to output directory&#34;)
    else:
        print(
            &#34;[INFO] No additional files to collect (all outputs already in output directory)&#34;
        )</code></pre>
</details>
<div class="desc"><p>Collect all output files generated by pipeline steps into the output directory.
모든 파이프라인 단계에서 생성된 출력 파일을 출력 디렉토리로 수집합니다.
Also copies input/dependency files to outputs/inputs/ subdirectory.
의존성 파일 및 기본 로우 데이터 파일도 outputs/inputs/ 하위 디렉토리로 복사합니다.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_hydro_engineering_json_to_solver_csv"><code class="name flex">
<span>def <span class="ident">convert_hydro_engineering_json_to_solver_csv</span></span>(<span>hydro_json: Path, out_csv: Path, lbp_m: float = 60.302) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_hydro_engineering_json_to_solver_csv(
    hydro_json: Path,
    out_csv: Path,
    lbp_m: float = LPP_M,
) -&gt; Path:
    &#34;&#34;&#34;
    Convert Hydro_Table_Engineering.json into the solver-required columns.

    solver expects columns:
      Tmean_m, TPC_t_per_cm, MTC_t_m_per_cm, LCF_m, LBP_m
    In your engineering table, MTC may be stored as MCTC_t_m_per_cm and LCF as LCF_m_from_midship.
    &#34;&#34;&#34;
    # #region agent log
    debug_log(
        &#34;convert_hydro_engineering_json_to_solver_csv:237&#34;,
        &#34;Function entry&#34;,
        {&#34;hydro_json&#34;: str(hydro_json), &#34;out_csv&#34;: str(out_csv)},
        &#34;A&#34;,
    )
    # #endregion

    try:
        file_content = hydro_json.read_text(encoding=&#34;utf-8&#34;)
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:243&#34;,
            &#34;File read success&#34;,
            {&#34;file_size&#34;: len(file_content)},
            &#34;B&#34;,
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:246&#34;,
            &#34;File read error&#34;,
            {&#34;error&#34;: str(e), &#34;error_type&#34;: type(e).__name__},
            &#34;B&#34;,
        )
        # #endregion
        raise

    try:
        raw = json.loads(file_content)
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:252&#34;,
            &#34;JSON parse success&#34;,
            {&#34;raw_type&#34;: type(raw).__name__},
            &#34;A&#34;,
        )
        # #endregion
    except json.JSONDecodeError as e:
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:255&#34;,
            &#34;JSON parse error&#34;,
            {&#34;error&#34;: str(e), &#34;position&#34;: getattr(e, &#34;pos&#34;, None)},
            &#34;A&#34;,
        )
        # #endregion
        raise
    if isinstance(raw, list):
        df = pd.DataFrame(raw)
    elif isinstance(raw, dict) and &#34;rows&#34; in raw:
        df = pd.DataFrame(raw.get(&#34;rows&#34;, []))
    else:
        # last resort: try dict itself
        df = pd.DataFrame(raw)

    # Normalize columns
    colmap = {}
    if &#34;MCTC_t_m_per_cm&#34; in df.columns and &#34;MTC_t_m_per_cm&#34; not in df.columns:
        colmap[&#34;MCTC_t_m_per_cm&#34;] = &#34;MTC_t_m_per_cm&#34;
    if &#34;LCF_m_from_midship&#34; in df.columns and &#34;LCF_m&#34; not in df.columns:
        colmap[&#34;LCF_m_from_midship&#34;] = &#34;LCF_m&#34;
    df = df.rename(columns=colmap)

    # Required columns
    required = [&#34;Tmean_m&#34;, &#34;TPC_t_per_cm&#34;, &#34;MTC_t_m_per_cm&#34;, &#34;LCF_m&#34;]
    # #region agent log
    debug_log(
        &#34;convert_hydro_engineering_json_to_solver_csv:262&#34;,
        &#34;Before column check&#34;,
        {&#34;df_columns&#34;: list(df.columns), &#34;required&#34;: required},
        &#34;E&#34;,
    )
    # #endregion
    missing = [c for c in required if c not in df.columns]
    if missing:
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:267&#34;,
            &#34;Missing columns error&#34;,
            {&#34;missing&#34;: missing, &#34;available_columns&#34;: list(df.columns)},
            &#34;E&#34;,
        )
        # #endregion
        raise ValueError(f&#34;Hydro table missing required columns: {missing}&#34;)

    if &#34;LBP_m&#34; not in df.columns:
        df[&#34;LBP_m&#34;] = float(lbp_m)

    try:
        df = df[[&#34;Tmean_m&#34;, &#34;TPC_t_per_cm&#34;, &#34;MTC_t_m_per_cm&#34;, &#34;LCF_m&#34;, &#34;LBP_m&#34;]].copy()
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:275&#34;,
            &#34;DataFrame column selection&#34;,
            {&#34;final_columns&#34;: list(df.columns), &#34;shape&#34;: df.shape},
            &#34;E&#34;,
        )
        # #endregion
    except KeyError as e:
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:279&#34;,
            &#34;Column selection KeyError&#34;,
            {&#34;error&#34;: str(e), &#34;available_columns&#34;: list(df.columns)},
            &#34;E&#34;,
        )
        # #endregion
        raise

    try:
        df.to_csv(out_csv, index=False, encoding=&#34;utf-8-sig&#34;)
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:285&#34;,
            &#34;CSV write success&#34;,
            {&#34;out_csv&#34;: str(out_csv)},
            &#34;D&#34;,
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_hydro_engineering_json_to_solver_csv:289&#34;,
            &#34;CSV write error&#34;,
            {&#34;error&#34;: str(e), &#34;error_type&#34;: type(e).__name__},
            &#34;D&#34;,
        )
        # #endregion
        raise

    return out_csv</code></pre>
</details>
<div class="desc"><p>Convert Hydro_Table_Engineering.json into the solver-required columns.</p>
<p>solver expects columns:
Tmean_m, TPC_t_per_cm, MTC_t_m_per_cm, LCF_m, LBP_m
In your engineering table, MTC may be stored as MCTC_t_m_per_cm and LCF as LCF_m_from_midship.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_tank_catalog_json_to_solver_csv"><code class="name flex">
<span>def <span class="ident">convert_tank_catalog_json_to_solver_csv</span></span>(<span>tank_catalog_json: Path,<br>out_csv: Path,<br>pump_rate_tph: float = 100.0,<br>include_keywords: Optional[List[str]] = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_tank_catalog_json_to_solver_csv(
    tank_catalog_json: Path,
    out_csv: Path,
    pump_rate_tph: float = 100.0,
    include_keywords: Optional[List[str]] = None,
) -&gt; Path:
    &#34;&#34;&#34;
    Convert tank_catalog_from_tankmd.json (object with {&#34;tanks&#34;:[...]}) into
    Tank SSOT CSV format for ballast_gate_solver_v4.py.

    IMPORTANT: Coordinate convention (align to ops_final_r3 script):
        x_from_mid_m = MIDSHIP_FROM_AP_M - lcg_from_ap_m
        (+AFT / -FWD)

    The solver requires at least:
        Tank, x_from_mid_m, Current_t, Min_t, Max_t, mode, use_flag, pump_rate_tph, priority_weight
    &#34;&#34;&#34;
    # #region agent log
    debug_log(
        &#34;convert_tank_catalog_json_to_solver_csv:168&#34;,
        &#34;Function entry&#34;,
        {&#34;tank_catalog_json&#34;: str(tank_catalog_json), &#34;out_csv&#34;: str(out_csv)},
        &#34;A&#34;,
    )
    # #endregion

    try:
        file_content = tank_catalog_json.read_text(encoding=&#34;utf-8&#34;)
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:172&#34;,
            &#34;File read success&#34;,
            {&#34;file_size&#34;: len(file_content)},
            &#34;B&#34;,
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:175&#34;,
            &#34;File read error&#34;,
            {
                &#34;error&#34;: str(e),
                &#34;error_type&#34;: type(e).__name__,
                &#34;traceback&#34;: traceback.format_exc(),
            },
            &#34;B&#34;,
        )
        # #endregion
        raise

    try:
        obj = json.loads(file_content)
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:181&#34;,
            &#34;JSON parse success&#34;,
            {&#34;obj_keys&#34;: list(obj.keys()) if isinstance(obj, dict) else &#34;not_dict&#34;},
            &#34;A&#34;,
        )
        # #endregion
    except json.JSONDecodeError as e:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:185&#34;,
            &#34;JSON parse error&#34;,
            {
                &#34;error&#34;: str(e),
                &#34;error_type&#34;: type(e).__name__,
                &#34;position&#34;: getattr(e, &#34;pos&#34;, None),
            },
            &#34;A&#34;,
        )
        # #endregion
        raise
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:189&#34;,
            &#34;JSON parse unexpected error&#34;,
            {&#34;error&#34;: str(e), &#34;error_type&#34;: type(e).__name__},
            &#34;A&#34;,
        )
        # #endregion
        raise

    tanks = obj.get(&#34;tanks&#34;, [])
    if not isinstance(tanks, list):
        raise ValueError(f&#34;Unexpected tank catalog structure: {tank_catalog_json}&#34;)

    include_keywords = include_keywords or [&#34;BALLAST&#34;, &#34;VOID&#34;, &#34;FWB&#34;, &#34;FW&#34;, &#34;DB&#34;]
    # #region agent log
    debug_log(
        &#34;convert_tank_catalog_json_to_solver_csv:195&#34;,
        &#34;Before tank loop&#34;,
        {&#34;tanks_count&#34;: len(tanks), &#34;include_keywords&#34;: include_keywords},
        &#34;C&#34;,
    )
    # #endregion

    rows = []
    for idx, t in enumerate(tanks):
        tid = str(t.get(&#34;id&#34;, &#34;&#34;)).strip()
        if not tid or tid == &#34;**Total**&#34;:
            continue

        cat = str(t.get(&#34;category&#34;, &#34;&#34;)).strip()
        try:
            cap_t_val = t.get(&#34;cap_t&#34;, 0.0) or 0.0
            cap_t = float(cap_t_val)
            # #region agent log
            debug_log(
                f&#34;convert_tank_catalog_json_to_solver_csv:207_tank_{idx}&#34;,
                &#34;cap_t conversion&#34;,
                {&#34;tid&#34;: tid, &#34;cap_t_val&#34;: cap_t_val, &#34;cap_t&#34;: cap_t},
                &#34;C&#34;,
            )
            # #endregion
        except (ValueError, TypeError) as e:
            # #region agent log
            debug_log(
                f&#34;convert_tank_catalog_json_to_solver_csv:212_tank_{idx}&#34;,
                &#34;cap_t conversion error&#34;,
                {&#34;tid&#34;: tid, &#34;cap_t_val&#34;: cap_t_val, &#34;error&#34;: str(e)},
                &#34;C&#34;,
            )
            # #endregion
            cap_t = 0.0

        try:
            lcg_val = t.get(&#34;lcg_m&#34;, 0.0) or 0.0
            lcg_from_ap = float(lcg_val)
            # #region agent log
            debug_log(
                f&#34;convert_tank_catalog_json_to_solver_csv:220_tank_{idx}&#34;,
                &#34;lcg_m conversion&#34;,
                {&#34;tid&#34;: tid, &#34;lcg_val&#34;: lcg_val, &#34;lcg_from_ap&#34;: lcg_from_ap},
                &#34;C&#34;,
            )
            # #endregion
        except (ValueError, TypeError) as e:
            # #region agent log
            debug_log(
                f&#34;convert_tank_catalog_json_to_solver_csv:225_tank_{idx}&#34;,
                &#34;lcg_m conversion error&#34;,
                {&#34;tid&#34;: tid, &#34;lcg_val&#34;: lcg_val, &#34;error&#34;: str(e)},
                &#34;C&#34;,
            )
            # #endregion
            lcg_from_ap = 0.0
        x_from_mid = MIDSHIP_FROM_AP_M - lcg_from_ap

        key_blob = f&#34;{tid} {cat}&#34;.upper()
        use_flag = &#34;Y&#34; if any(k in key_blob for k in include_keywords) else &#34;N&#34;

        # Priority heuristic: favor the usual discharge candidates if present
        pr = 5.0
        if &#34;FWB2&#34; in tid.upper():
            pr = 1.0
        elif &#34;VOIDDB2&#34; in tid.upper():
            pr = 2.0
        elif &#34;VOIDDB1&#34; in tid.upper():
            pr = 3.0

        rows.append(
            {
                &#34;Tank&#34;: tid,
                &#34;Capacity_t&#34;: cap_t,
                &#34;x_from_mid_m&#34;: round(x_from_mid, 4),
                &#34;Current_t&#34;: 0.0,
                &#34;Min_t&#34;: 0.0,
                &#34;Max_t&#34;: cap_t,
                &#34;mode&#34;: &#34;FILL_DISCHARGE&#34;,
                &#34;use_flag&#34;: use_flag,
                &#34;pump_rate_tph&#34;: float(pump_rate_tph),
                &#34;priority_weight&#34;: pr,
            }
        )

    # #region agent log
    debug_log(
        &#34;convert_tank_catalog_json_to_solver_csv:235&#34;,
        &#34;After tank loop&#34;,
        {&#34;rows_count&#34;: len(rows)},
        &#34;C&#34;,
    )
    # #endregion

    if not rows:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:239&#34;,
            &#34;No rows produced error&#34;,
            {},
            &#34;C&#34;,
        )
        # #endregion
        raise ValueError(&#34;No tank rows produced. Check tank catalog content/filters.&#34;)

    try:
        df = pd.DataFrame(rows)
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:245&#34;,
            &#34;DataFrame created&#34;,
            {&#34;columns&#34;: list(df.columns), &#34;shape&#34;: df.shape},
            &#34;D&#34;,
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:249&#34;,
            &#34;DataFrame creation error&#34;,
            {&#34;error&#34;: str(e), &#34;error_type&#34;: type(e).__name__},
            &#34;D&#34;,
        )
        # #endregion
        raise

    try:
        df.to_csv(out_csv, index=False, encoding=&#34;utf-8-sig&#34;)
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:255&#34;,
            &#34;CSV write success&#34;,
            {&#34;out_csv&#34;: str(out_csv)},
            &#34;D&#34;,
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            &#34;convert_tank_catalog_json_to_solver_csv:259&#34;,
            &#34;CSV write error&#34;,
            {&#34;error&#34;: str(e), &#34;error_type&#34;: type(e).__name__, &#34;out_csv&#34;: str(out_csv)},
            &#34;D&#34;,
        )
        # #endregion
        raise

    return out_csv</code></pre>
</details>
<div class="desc"><p>Convert tank_catalog_from_tankmd.json (object with {"tanks":[&hellip;]}) into
Tank SSOT CSV format for ballast_gate_solver_v4.py.</p>
<p>IMPORTANT: Coordinate convention (align to ops_final_r3 script):
x_from_mid_m = MIDSHIP_FROM_AP_M - lcg_from_ap_m
(+AFT / -FWD)</p>
<p>The solver requires at least:
Tank, x_from_mid_m, Current_t, Min_t, Max_t, mode, use_flag, pump_rate_tph, priority_weight</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.create_final_output_folder"><code class="name flex">
<span>def <span class="ident">create_final_output_folder</span></span>(<span>base_dir: Path, out_dir: Path, merged_excel: Optional[Path]) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_final_output_folder(
    base_dir: Path, out_dir: Path, merged_excel: Optional[Path]
) -&gt; Path:
    ts = now_tag()
    final_dir = base_dir / f&#34;final_output_{ts}&#34;
    final_dir.mkdir(parents=True, exist_ok=True)

    candidates: List[Path] = []
    ssot_dir = out_dir / &#34;ssot&#34;

    if merged_excel and Path(merged_excel).exists():
        candidates.append(Path(merged_excel))

    for rel in [
        &#34;pipeline_stage_QA.csv&#34;,
        &#34;stage_table_unified.csv&#34;,
        &#34;tank_ssot_for_solver.csv&#34;,
        &#34;tank_ssot_for_solver__aftmin.csv&#34;,
        &#34;hydro_table_for_solver.csv&#34;,
    ]:
        p = ssot_dir / rel
        if p.exists():
            candidates.append(p)

    for rel in [
        &#34;gate_fail_report.md&#34;,
        &#34;TUG_Operational_SOP_DNV_ST_N001.md&#34;,
        &#34;OPS_FINAL_R3_Report_Integrated.md&#34;,
        &#34;solver_ballast_stage_plan.csv&#34;,
        &#34;solver_ballast_summary.csv&#34;,
        &#34;BALLAST_SEQUENCE.csv&#34;,
        &#34;BALLAST_OPTION.csv&#34;,
        &#34;BALLAST_EXEC.csv&#34;,
        &#34;BALLAST_OPERATIONS_CHECKLIST.md&#34;,
        &#34;BALLAST_SEQUENCE_WITH_VALVES.md&#34;,
        &#34;HOLD_POINT_SUMMARY.csv&#34;,
    ]:
        p = out_dir / rel
        if p.exists():
            candidates.append(p)

    for p in out_dir.glob(&#34;*.xlsx&#34;):
        candidates.append(p)

    manifest = {
        &#34;created&#34;: ts,
        &#34;base_dir&#34;: str(base_dir),
        &#34;out_dir&#34;: str(out_dir),
        &#34;files&#34;: [],
    }
    seen = set()
    for p in candidates:
        if not p.exists():
            continue
        if p.resolve() in seen:
            continue
        seen.add(p.resolve())
        dst = final_dir / p.name
        try:
            shutil.copy2(p, dst)
            manifest[&#34;files&#34;].append(
                {
                    &#34;name&#34;: dst.name,
                    &#34;sha256&#34;: _sha256(dst),
                    &#34;size&#34;: dst.stat().st_size,
                }
            )
        except Exception as e:
            print(f&#34;[WARN] Failed to copy {p.name}: {e}&#34;)

    (final_dir / &#34;manifest.json&#34;).write_text(
        json.dumps(manifest, indent=2, ensure_ascii=False), encoding=&#34;utf-8&#34;
    )
    print(f&#34;[OK] Final output folder: {final_dir}&#34;)
    return final_dir</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_log"><code class="name flex">
<span>def <span class="ident">debug_log</span></span>(<span>location: str, message: str, data: dict = None, hypothesis_id: str = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def debug_log(
    location: str, message: str, data: dict = None, hypothesis_id: str = None
):
    &#34;&#34;&#34;Write debug log entry in NDJSON format&#34;&#34;&#34;
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        import json
        from datetime import datetime

        entry = {
            &#34;sessionId&#34;: DEBUG_SESSION_ID,
            &#34;runId&#34;: &#34;run1&#34;,
            &#34;timestamp&#34;: int(datetime.now().timestamp() * 1000),
            &#34;location&#34;: location,
            &#34;message&#34;: message,
            &#34;data&#34;: data or {},
        }
        if hypothesis_id:
            entry[&#34;hypothesisId&#34;] = hypothesis_id
        with DEBUG_LOG_PATH.open(&#34;a&#34;, encoding=&#34;utf-8&#34;) as f:
            f.write(json.dumps(entry, ensure_ascii=False) + &#34;\n&#34;)
            f.flush()  # Ensure data is written immediately
    except Exception as e:
        # Log exception to stderr for debugging (don&#39;t fail silently during development)
        import sys

        sys.stderr.write(f&#34;DEBUG_LOG_ERROR: {type(e).__name__}: {e}\n&#34;)
        sys.stderr.flush()</code></pre>
</details>
<div class="desc"><p>Write debug log entry in NDJSON format</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_report_step"><code class="name flex">
<span>def <span class="ident">debug_report_step</span></span>(<span>*,<br>out_dir: Path,<br>stage_qa_csv: Path,<br>hydro_json_path: Optional[Path] = None,<br>hydro_csv: Optional[Path] = None) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def debug_report_step(
    *,
    out_dir: Path,
    stage_qa_csv: Path,
    hydro_json_path: Optional[Path] = None,
    hydro_csv: Optional[Path] = None,
) -&gt; int:
    &#34;&#34;&#34;
    Generate feasibility debug report from pipeline QA and hydro table.
    &#34;&#34;&#34;
    try:
        root_dir = Path(__file__).resolve().parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))
        from debug_report import write_debug_report
    except Exception as exc:
        print(f&#34;[WARN] Debug report module unavailable: {exc}&#34;)
        return 0

    if not stage_qa_csv.exists():
        print(f&#34;[WARN] Stage QA CSV not found: {stage_qa_csv}&#34;)
        return 0

    debug_dir = ensure_dir(out_dir)
    hydro_csv_path = hydro_csv or (debug_dir / &#34;hydro_table_for_solver.csv&#34;)
    if not hydro_csv_path.exists():
        if hydro_json_path and Path(hydro_json_path).exists():
            try:
                with open(hydro_json_path, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
                    data = json.load(f)
                rows = data.get(&#34;rows&#34;, data.get(&#34;table&#34;, data.get(&#34;data&#34;, [])))
                hydro_df = pd.DataFrame(rows)
                hydro_df.to_csv(hydro_csv_path, index=False, encoding=&#34;utf-8-sig&#34;)
                print(f&#34;[OK] Hydro CSV created: {hydro_csv_path}&#34;)
            except Exception as exc:
                print(f&#34;[WARN] Failed to build hydro CSV: {exc}&#34;)
                return 0
        else:
            print(&#34;[WARN] Hydro source not available; skipping debug report.&#34;)
            return 0

    out_md = debug_dir / &#34;debug_feasibility_pipeline.md&#34;
    out_flags = debug_dir / &#34;debug_stage_flags.csv&#34;

    try:
        write_debug_report(
            stage_qa_csv=stage_qa_csv,
            hydro_csv=hydro_csv_path,
            out_md=out_md,
            out_flags_csv=out_flags,
        )
        print(&#34;[OK] Debug report generated&#34;)
        print(f&#34;  - {out_md}&#34;)
        print(f&#34;  - {out_flags}&#34;)
        return 0
    except Exception as exc:
        print(f&#34;[WARN] Debug report generation failed: {exc}&#34;)
        return 1</code></pre>
</details>
<div class="desc"><p>Generate feasibility debug report from pipeline QA and hydro table.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.enrich_stage_table_with_tide_ukc"><code class="name flex">
<span>def <span class="ident">enrich_stage_table_with_tide_ukc</span></span>(<span>stage_table_csv: Path,<br>*,<br>forecast_tide_m: Optional[float] = None,<br>depth_ref_m: Optional[float] = None,<br>ukc_min_m: Optional[float] = None,<br>squat_m: float = 0.0,<br>safety_allow_m: float = 0.0,<br>tide_tol_m: float = 0.1,<br>stage_tide_csv_path: Optional[Path] = None,<br>tide_table_path: Optional[Path] = None,<br>stage_schedule_path: Optional[Path] = None,<br>tide_strategy: str = 'keep_csv') ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enrich_stage_table_with_tide_ukc(
    stage_table_csv: Path,
    *,
    forecast_tide_m: Optional[float] = None,
    depth_ref_m: Optional[float] = None,
    ukc_min_m: Optional[float] = None,
    squat_m: float = 0.0,
    safety_allow_m: float = 0.0,
    tide_tol_m: float = DEFAULT_TIDE_TOL_M,
    stage_tide_csv_path: Optional[Path] = None,
    tide_table_path: Optional[Path] = None,
    stage_schedule_path: Optional[Path] = None,
    tide_strategy: str = &#34;keep_csv&#34;,
) -&gt; Path:
    &#34;&#34;&#34;
    AGI 기준: stage_table_unified.csv에 Tide/UKC 파생 필드 통합(선택).

    - Forecast_Tide_m 우선순위:
        1) stage_table 내 값(존재 시)
        2) (--tide_table + --stage_schedule)로 보간(옵션)
        3) CLI --forecast_tide (fallback)

    - Output columns (추가/갱신):
        Tide_required_m, Forecast_tide_m, Tide_margin_m,
        UKC_fwd_m, UKC_aft_m, UKC_min_actual_m,
        Tide_verification, Tide_note,
        Squat_m, SafetyAllow_m (없으면 생성)

    NOTE:
    - stage_table은 Solver 입력 SSOT이며, 본 함수는 SSOT에 &#34;파생 필드&#34;를 추가하는 성격.
    - Solver/QA는 여전히 Forecast_Tide_m/DepthRef_m/UKC_Min_m를 기준으로 Gate 계산.
    &#34;&#34;&#34;
    import numpy as np

    stage_table_csv = Path(stage_table_csv)
    df = pd.read_csv(stage_table_csv, encoding=&#34;utf-8-sig&#34;)

    # Ensure base numeric inputs
    if &#34;DepthRef_m&#34; not in df.columns:
        df[&#34;DepthRef_m&#34;] = np.nan
    df[&#34;DepthRef_m&#34;] = pd.to_numeric(df[&#34;DepthRef_m&#34;], errors=&#34;coerce&#34;)
    if depth_ref_m is not None:
        filled_count = df[&#34;DepthRef_m&#34;].isna().sum()
        df[&#34;DepthRef_m&#34;] = df[&#34;DepthRef_m&#34;].fillna(float(depth_ref_m))
        print(
            f&#34;[OK] Applied depth_ref_m={depth_ref_m} to stage_table (filled {filled_count}/{len(df)} rows)&#34;
        )
    else:
        print(f&#34;[WARN] depth_ref_m is None, DepthRef_m will remain empty&#34;)

    if &#34;UKC_Min_m&#34; not in df.columns:
        df[&#34;UKC_Min_m&#34;] = np.nan
    df[&#34;UKC_Min_m&#34;] = pd.to_numeric(df[&#34;UKC_Min_m&#34;], errors=&#34;coerce&#34;)
    if ukc_min_m is not None:
        filled_count = df[&#34;UKC_Min_m&#34;].isna().sum()
        df[&#34;UKC_Min_m&#34;] = df[&#34;UKC_Min_m&#34;].fillna(float(ukc_min_m))
        print(
            f&#34;[OK] Applied ukc_min_m={ukc_min_m} to stage_table (filled {filled_count}/{len(df)} rows)&#34;
        )
    else:
        print(f&#34;[WARN] ukc_min_m is None, UKC_Min_m will remain empty&#34;)

    if &#34;Squat_m&#34; not in df.columns:
        df[&#34;Squat_m&#34;] = float(squat_m)
    else:
        df[&#34;Squat_m&#34;] = pd.to_numeric(df[&#34;Squat_m&#34;], errors=&#34;coerce&#34;).fillna(
            float(squat_m)
        )

    if &#34;SafetyAllow_m&#34; not in df.columns:
        df[&#34;SafetyAllow_m&#34;] = float(safety_allow_m)
    else:
        df[&#34;SafetyAllow_m&#34;] = pd.to_numeric(
            df[&#34;SafetyAllow_m&#34;], errors=&#34;coerce&#34;
        ).fillna(float(safety_allow_m))

    if &#34;Forecast_Tide_m&#34; not in df.columns:
        df[&#34;Forecast_Tide_m&#34;] = np.nan
    df[&#34;Forecast_Tide_m&#34;] = pd.to_numeric(df[&#34;Forecast_Tide_m&#34;], errors=&#34;coerce&#34;)

    # Priority 0: CLI forecast_tide_m (최우선 - 명시적으로 제공된 경우)
    # CLI 값이 이미 build_stage_table_from_stage_results에서 설정되었을 수 있지만,
    # 명시적으로 CLI 값이 제공되면 stage_tide_csv보다 우선 적용
    if forecast_tide_m is not None:
        # CLI 값이 있으면 모든 Stage에 직접 적용 (stage_tide_csv보다 우선)
        df[&#34;Forecast_Tide_m&#34;] = float(forecast_tide_m)
        print(
            f&#34;[OK] CLI forecast_tide_m={forecast_tide_m} applied (override stage_tide_csv and tide_table)&#34;
        )
    else:
        # CLI 값이 없을 때만 stage_tide_csv 사용
        # Priority 1: Direct stage_tide_csv (highest priority when CLI not provided)
        if stage_tide_csv_path and Path(stage_tide_csv_path).exists():
            try:
                df_tide = pd.read_csv(stage_tide_csv_path, encoding=&#34;utf-8-sig&#34;)

                # Normalize stage key for matching
                def _norm_stage_key(s):
                    return str(s).strip()

                # Find stage column in tide CSV
                stage_col_tide = None
                for c in df_tide.columns:
                    if str(c).strip().lower() in (&#34;stage&#34;, &#34;stagekey&#34;, &#34;stage_key&#34;):
                        stage_col_tide = c
                        break
                if stage_col_tide is None:
                    stage_col_tide = df_tide.columns[0]  # Fallback to first column

                # Find Forecast_tide_m column
                tide_col = None
                for c in df_tide.columns:
                    if str(c).strip().lower() in (
                        &#34;forecast_tide_m&#34;,
                        &#34;forecast_tide&#34;,
                        &#34;tide_m&#34;,
                    ):
                        tide_col = c
                        break

                if stage_col_tide and tide_col:
                    # Create mapping
                    df_tide[&#34;_stage_norm&#34;] = (
                        df_tide[stage_col_tide].astype(str).map(_norm_stage_key)
                    )
                    tide_map = dict(
                        zip(
                            df_tide[&#34;_stage_norm&#34;],
                            pd.to_numeric(df_tide[tide_col], errors=&#34;coerce&#34;),
                        )
                    )

                    # Apply to df
                    df[&#34;_stage_norm&#34;] = df[&#34;Stage&#34;].astype(str).map(_norm_stage_key)
                    df[&#34;Forecast_Tide_m&#34;] = (
                        df[&#34;_stage_norm&#34;].map(tide_map).fillna(df[&#34;Forecast_Tide_m&#34;])
                    )
                    df = df.drop(columns=[&#34;_stage_norm&#34;])

                    assigned = df[&#34;Forecast_Tide_m&#34;].notna().sum()
                    print(
                        f&#34;[OK] Loaded stage_tide_csv: {assigned}/{len(df)} stages assigned from {stage_tide_csv_path.name}&#34;
                    )
                else:
                    print(
                        f&#34;[WARN] stage_tide_csv missing required columns (Stage/StageKey, Forecast_tide_m)&#34;
                    )
            except Exception as e:
                print(f&#34;[WARN] Failed to load stage_tide_csv: {type(e).__name__}: {e}&#34;)

    # Priority 2: Optional: per-stage tide assignment via tide_table + stage_schedule
    if (
        tide_table_path
        and stage_schedule_path
        and load_tide_table_any
        and load_stage_schedule_any
        and apply_forecast_tide_from_table
    ):
        try:
            tide_df = load_tide_table_any(Path(tide_table_path))
            sched_df = load_stage_schedule_any(Path(stage_schedule_path))
            strategy = (
                &#34;override&#34;
                if str(tide_strategy).lower()
                in (&#34;override_from_table&#34;, &#34;override&#34;, &#34;force&#34;)
                else &#34;fillna&#34;
            )
            df = apply_forecast_tide_from_table(
                df,
                tide_df,
                sched_df,
                stage_col=&#34;Stage&#34;,
                out_col=&#34;Forecast_Tide_m&#34;,
                strategy=strategy,
            )
        except Exception as e:
            print(f&#34;[WARN] Tide table apply failed: {type(e).__name__}: {e}&#34;)

    # Fallback: CLI forecast tide to fill missing (CLI가 없을 때만)
    # Note: CLI 값이 이미 Priority 0에서 적용되었으므로, 여기는 실행되지 않음
    # 하지만 안전을 위해 유지 (CLI가 None인 경우를 대비)
    if forecast_tide_m is not None:
        # 이미 Priority 0에서 적용되었으므로, fillna만 수행 (중복 방지)
        df[&#34;Forecast_Tide_m&#34;] = df[&#34;Forecast_Tide_m&#34;].fillna(float(forecast_tide_m))

    # Derived fields
    df[&#34;Forecast_tide_m&#34;] = df[&#34;Forecast_Tide_m&#34;]

    req_list = []
    margin_list = []
    uf_list = []
    ua_list = []
    umin_list = []
    stat_list = []
    note_list = []

    for _, r in df.iterrows():
        dfwd = r.get(&#34;Current_FWD_m&#34;, np.nan)
        daft = r.get(&#34;Current_AFT_m&#34;, np.nan)
        draft_ref = np.nan
        if pd.notna(dfwd) and pd.notna(daft):
            draft_ref = max(float(dfwd), float(daft))
        elif pd.notna(dfwd):
            draft_ref = float(dfwd)
        elif pd.notna(daft):
            draft_ref = float(daft)

        depth = r.get(&#34;DepthRef_m&#34;, np.nan)
        ukc_min = r.get(&#34;UKC_Min_m&#34;, np.nan)
        wl = r.get(&#34;Forecast_Tide_m&#34;, np.nan)
        squat = r.get(&#34;Squat_m&#34;, float(squat_m))
        safety = r.get(&#34;SafetyAllow_m&#34;, float(safety_allow_m))

        if required_tide_m and ukc_fwd_aft_min and verify_tide:
            req = required_tide_m(
                float(depth) if pd.notna(depth) else None,
                float(draft_ref) if pd.notna(draft_ref) else None,
                float(ukc_min) if pd.notna(ukc_min) else None,
                float(squat) if pd.notna(squat) else 0.0,
                float(safety) if pd.notna(safety) else 0.0,
                clamp_zero=True,
            )
            uf, ua, umin = ukc_fwd_aft_min(
                float(depth) if pd.notna(depth) else None,
                float(wl) if pd.notna(wl) else None,
                float(dfwd) if pd.notna(dfwd) else None,
                float(daft) if pd.notna(daft) else None,
                float(squat) if pd.notna(squat) else 0.0,
                float(safety) if pd.notna(safety) else 0.0,
            )
            margin = float(wl - req) if pd.notna(wl) and pd.notna(req) else np.nan
            stat, note = verify_tide(
                req, float(wl) if pd.notna(wl) else None, tolerance_m=float(tide_tol_m)
            )
        else:
            req = np.nan
            uf = ua = umin = np.nan
            margin = np.nan
            stat, note = (&#34;VERIFY&#34;, &#34;tide_ukc_engine not available&#34;)

        req_list.append(req)
        margin_list.append(margin)
        uf_list.append(uf)
        ua_list.append(ua)
        umin_list.append(umin)
        stat_list.append(stat)
        note_list.append(note)

    df[&#34;Tide_required_m&#34;] = req_list
    df[&#34;Tide_margin_m&#34;] = margin_list
    df[&#34;UKC_fwd_m&#34;] = uf_list
    df[&#34;UKC_aft_m&#34;] = ua_list
    df[&#34;UKC_min_actual_m&#34;] = umin_list
    df[&#34;Tide_verification&#34;] = stat_list
    df[&#34;Tide_note&#34;] = note_list

    df.to_csv(stage_table_csv, index=False, encoding=&#34;utf-8-sig&#34;)
    return stage_table_csv</code></pre>
</details>
<div class="desc"><p>AGI 기준: stage_table_unified.csv에 Tide/UKC 파생 필드 통합(선택).</p>
<ul>
<li>
<p>Forecast_Tide_m 우선순위:
1) stage_table 내 값(존재 시)
2) (&ndash;tide_table + &ndash;stage_schedule)로 보간(옵션)
3) CLI &ndash;forecast_tide (fallback)</p>
</li>
<li>
<p>Output columns (추가/갱신):
Tide_required_m, Forecast_tide_m, Tide_margin_m,
UKC_fwd_m, UKC_aft_m, UKC_min_actual_m,
Tide_verification, Tide_note,
Squat_m, SafetyAllow_m (없으면 생성)</p>
</li>
</ul>
<p>NOTE:
- stage_table은 Solver 입력 SSOT이며, 본 함수는 SSOT에 "파생 필드"를 추가하는 성격.
- Solver/QA는 여전히 Forecast_Tide_m/DepthRef_m/UKC_Min_m를 기준으로 Gate 계산.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_bplus_inputs_data_dir"><code class="name flex">
<span>def <span class="ident">ensure_bplus_inputs_data_dir</span></span>(<span>base_dir: Path) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ensure_bplus_inputs_data_dir(base_dir: Path) -&gt; None:
    &#34;&#34;&#34;
    Ensure bplus_inputs/data exists and has Frame_x_from_mid_m.json.
    This prevents Step-1 fallback warnings.
    &#34;&#34;&#34;
    data_dir = base_dir / &#34;bplus_inputs&#34; / &#34;data&#34;
    data_dir.mkdir(parents=True, exist_ok=True)
    dest = data_dir / &#34;Frame_x_from_mid_m.json&#34;
    if dest.exists():
        return

    candidates = [
        base_dir / &#34;Frame_x_from_mid_m.json&#34;,
        base_dir / &#34;bplus_inputs&#34; / &#34;Frame_x_from_mid_m.json&#34;,
        base_dir.parent / &#34;02_RAW_DATA&#34; / &#34;Frame_x_from_mid_m.json&#34;,
        base_dir.parent / &#34;02_RAW_DATA&#34; / &#34;bplus_inputs&#34; / &#34;Frame_x_from_mid_m.json&#34;,
    ]

    for src in candidates:
        if src.exists():
            try:
                shutil.copy2(src, dest)
                print(f&#34;[OK] Copied Frame_x_from_mid_m.json -&gt; {dest}&#34;)
                return
            except Exception as e:
                print(
                    f&#34;[WARN] Failed to copy Frame_x_from_mid_m.json from {src}: {type(e).__name__}: {e}&#34;
                )
                return

    print(
        &#34;[WARN] Frame_x_from_mid_m.json not found in expected locations. &#34;
        &#34;Proceeding with fallback.&#34;
    )</code></pre>
</details>
<div class="desc"><p>Ensure bplus_inputs/data exists and has Frame_x_from_mid_m.json.
This prevents Step-1 fallback warnings.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_dir"><code class="name flex">
<span>def <span class="ident">ensure_dir</span></span>(<span>p: Path) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ensure_dir(p: Path) -&gt; Path:
    p.mkdir(parents=True, exist_ok=True)
    return p</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.find_first_existing"><code class="name flex">
<span>def <span class="ident">find_first_existing</span></span>(<span>paths: List[Path]) ‑> pathlib.Path | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_first_existing(paths: List[Path]) -&gt; Optional[Path]:
    for p in paths:
        if p.exists():
            return p
    return None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_gate_fail_report_md"><code class="name flex">
<span>def <span class="ident">generate_gate_fail_report_md</span></span>(<span>*,<br>out_md: Path,<br>site: str,<br>profile_path: Optional[Path],<br>stage_qa_csv: Path,<br>tank_ssot_csv: Path,<br>sensor_stats: Optional[Dict[str, object]] = None,<br>ukc_inputs: Optional[Dict[str, object]] = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_gate_fail_report_md(
    *,
    out_md: Path,
    site: str,
    profile_path: Optional[Path],
    stage_qa_csv: Path,
    tank_ssot_csv: Path,
    sensor_stats: Optional[Dict[str, object]] = None,
    ukc_inputs: Optional[Dict[str, object]] = None,
) -&gt; Path:
    &#34;&#34;&#34;Generate a Markdown report summarizing gate failures and likely root causes.

    PATCH: Extended with Gate_Hydro_Range and HardStop summary support.
    &#34;&#34;&#34;

    # Helper functions
    def _norm(s):
        return str(s).strip() if pd.notna(s) else &#34;&#34;

    def _is_ng_gate(v):
        vv = _norm(v).upper()
        return vv in (&#34;NG&#34;, &#34;FAIL&#34;, &#34;NOK&#34;)

    def _is_true(v):
        vv = _norm(v).upper()
        return vv in (&#34;Y&#34;, &#34;YES&#34;, &#34;TRUE&#34;, &#34;1&#34;, &#34;HARDSTOP&#34;, &#34;STOP&#34;)

    def _first_existing_col(header, candidates):
        for c in candidates:
            if c in header:
                return c
        return None

    qa = pd.read_csv(stage_qa_csv, encoding=&#34;utf-8-sig&#34;)
    tanks = pd.read_csv(tank_ssot_csv, encoding=&#34;utf-8-sig&#34;)

    def _derive_fwd_max_from_qa(df: pd.DataFrame) -&gt; Optional[float]:
        if &#34;FWD_MAX_m&#34; not in df.columns:
            return None
        vals = pd.to_numeric(df[&#34;FWD_MAX_m&#34;], errors=&#34;coerce&#34;).dropna()
        if vals.empty:
            return None
        return float(vals.iloc[0])

    profile_obj: Optional[Dict[str, object]] = None
    crit_list: Optional[List[str]] = None
    crit_regex = DEFAULT_CRITICAL_STAGE_REGEX
    gateb_critical_only = False
    gateb_fwd_max = None

    if profile_path and Path(profile_path).exists():
        try:
            profile_obj = load_site_profile_json(Path(profile_path))
        except Exception:
            profile_obj = None

    if profile_obj:
        crit_list = _normalize_stage_patterns(
            _profile_last_value(profile_obj, PROFILE_CRITICAL_STAGE_KEYS)
        )
        crit_regex_val = _profile_last_value(profile_obj, PROFILE_CRITICAL_REGEX_KEYS)
        if isinstance(crit_regex_val, str) and crit_regex_val.strip():
            crit_regex = crit_regex_val
        gateb_critical_only = _profile_truthy_any(
            profile_obj, PROFILE_GATEB_CRITICAL_ONLY_KEYS
        )
        gateb_fwd_max = _coerce_float(
            _profile_last_value(profile_obj, PROFILE_GATEB_FWD_MAX_KEYS), None
        )

    if gateb_fwd_max is None:
        gateb_fwd_max = _derive_fwd_max_from_qa(qa)
    if gateb_fwd_max is None:
        gateb_fwd_max = MAMMOET_FWD_MAX_DRAFT_M_CD

    # ----------------------------------------------------------
    # PATCH: add split 2.70m gates (Captain vs Mammoet)
    #   - GateA_AFT_MIN_2p70_PASS
    #   - GateB_FWD_MAX_2p70_CD_PASS (profile-driven critical stages)
    # ----------------------------------------------------------
    qa = add_split_270_gates(
        qa,
        aft_min_m=CAPTAIN_AFT_MIN_DRAFT_M,
        fwd_max_m_cd=gateb_fwd_max,
        critical_only=gateb_critical_only,
        critical_regex=crit_regex,
        critical_stage_list=crit_list,
    )

    # Basic tank stats
    tanks_current = pd.to_numeric(tanks.get(&#34;Current_t&#34;, 0.0), errors=&#34;coerce&#34;).fillna(
        0.0
    )
    tanks_zero = int((tanks_current.abs() &lt;= 1e-9).sum())
    tanks_total = int(len(tanks))
    tanks_sum = float(tanks_current.sum())

    # Gate column auto-detection
    base_gate_cols = [&#34;Gate_FWD_Max&#34;, &#34;Gate_AFT_Min&#34;, &#34;Gate_Freeboard&#34;, &#34;Gate_UKC&#34;]

    hydro_gate_col = _first_existing_col(
        qa.columns,
        [
            &#34;Gate_Hydro_Range&#34;,
            &#34;Gate_HydroRange&#34;,
            &#34;Gate_Hydro_OutOfRange&#34;,
            &#34;HydroOutOfRange&#34;,
            &#34;Hydro_OutOfRange&#34;,
        ],
    )
    hardstop_col = _first_existing_col(
        qa.columns,
        [&#34;HardStop&#34;, &#34;HardStop_Flag&#34;, &#34;HardStop_YN&#34;, &#34;Hard_Stop&#34;, &#34;HS_Any&#34;],
    )
    hardstop_reason_col = _first_existing_col(
        qa.columns,
        [
            &#34;HardStop_Reason&#34;,
            &#34;HardStop_Reasons&#34;,
            &#34;HardStopReason&#34;,
            &#34;HardStop_Tag&#34;,
            &#34;HardStop_Tags&#34;,
        ],
    )

    gate_cols = list(base_gate_cols)
    if hydro_gate_col and hydro_gate_col not in gate_cols:
        gate_cols.append(hydro_gate_col)
    if hardstop_col and hardstop_col not in gate_cols:
        gate_cols.append(hardstop_col)
    # Add split 2.70m gates
    gate_cols += [&#34;GateA_AFT_MIN_2p70_PASS&#34;, &#34;GateB_FWD_MAX_2p70_CD_PASS&#34;]

    # Gate stats with extended columns
    def _cnt_ng(col: str) -&gt; int:
        if col not in qa.columns:
            return 0
        if col == hardstop_col:
            return int(qa[col].astype(str).apply(lambda x: _is_true(x)).sum())
        if col == &#34;Gate_FWD_Max&#34; and &#34;FWD_MAX_applicable&#34; in qa.columns:
            mask_app = qa[&#34;FWD_MAX_applicable&#34;] == True
            return int(
                (
                    (qa[col].astype(str).str.upper() == &#34;NG&#34;) &amp; mask_app.fillna(False)
                ).sum()
            )
        # Gate-B counts only where applicable
        if (
            col == &#34;GateB_FWD_MAX_2p70_CD_PASS&#34;
            and &#34;GateB_FWD_MAX_2p70_CD_applicable&#34; in qa.columns
        ):
            mask_app = qa[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;] == True
            return int(((~qa[col]) &amp; mask_app).sum())
        # For boolean pass columns (True=PASS, False=FAIL)
        if qa[col].dtype == bool:
            return int((~qa[col]).sum())
        return int((qa[col].astype(str).str.upper() == &#34;NG&#34;).sum())

    cnt_fwd = _cnt_ng(&#34;Gate_FWD_Max&#34;)
    cnt_aft = _cnt_ng(&#34;Gate_AFT_Min&#34;)
    cnt_fb = _cnt_ng(&#34;Gate_Freeboard&#34;)
    cnt_ukc = _cnt_ng(&#34;Gate_UKC&#34;)
    cnt_hydro = _cnt_ng(hydro_gate_col) if hydro_gate_col else 0
    cnt_hardstop = _cnt_ng(hardstop_col) if hardstop_col else 0
    cnt_gate_a = _cnt_ng(&#34;GateA_AFT_MIN_2p70_PASS&#34;)
    cnt_gate_b = _cnt_ng(&#34;GateB_FWD_MAX_2p70_CD_PASS&#34;)

    # UKC N/A count
    na_ukc = 0
    if &#34;Gate_UKC&#34; in qa.columns:
        na_ukc = int((qa[&#34;Gate_UKC&#34;].astype(str).str.upper() == &#34;N/A&#34;).sum())

    # HardStop detailed summary
    hardstop_stages = []
    hardstop_reason_to_stages = defaultdict(list)

    if hardstop_col and hardstop_col in qa.columns:
        hardstop_mask = qa[hardstop_col].astype(str).apply(lambda x: _is_true(x))
        hardstop_rows = qa[hardstop_mask]

        for _, row in hardstop_rows.iterrows():
            stg = _norm(row.get(&#34;Stage&#34;, &#34;&#34;)) or &#34;(UnknownStage)&#34;
            hardstop_stages.append(stg)

            # Reason extraction
            reason = (
                _norm(row.get(hardstop_reason_col))
                if hardstop_reason_col and hardstop_reason_col in qa.columns
                else &#34;&#34;
            )

            if not reason:
                # Fallback: derive reason from other columns
                fbmin = row.get(&#34;Freeboard_Min_m&#34;)
                try:
                    fbmin_v = float(fbmin) if pd.notna(fbmin) else None
                except Exception:
                    fbmin_v = None
                if fbmin_v is not None and fbmin_v &lt; 0:
                    reason = &#34;OverDepth/DeckWet&#34;
                elif hydro_gate_col and _is_ng_gate(row.get(hydro_gate_col)):
                    reason = &#34;HydroOutOfRange&#34;
                else:
                    reason = &#34;HardStop(Unspecified)&#34;

            hardstop_reason_to_stages[reason].append(stg)

    # Heuristics
    heuristics: List[str] = []
    if tanks_total &gt; 0 and tanks_zero == tanks_total:
        heuristics.append(
            &#34;- **SSOT Current_t가 전 탱크 0.0**: Deballast(Discharge) 기반 해결이 불가/왜곡될 수 있습니다. (센서/초기값 주입 필요)&#34;
        )
    elif tanks_total &gt; 0 and (tanks_zero / float(tanks_total)) &gt;= 0.95:
        heuristics.append(
            f&#34;- **SSOT Current_t가 대부분 0.0** ({tanks_zero}/{tanks_total}): Gate Solver/Optimizer 결과가 실제 운용과 불일치할 가능성이 큽니다.&#34;
        )

    if &#34;Freeboard_Min_m&#34; in qa.columns:
        fb_min = pd.to_numeric(qa[&#34;Freeboard_Min_m&#34;], errors=&#34;coerce&#34;).min()
        if pd.notna(fb_min) and float(fb_min) &lt; 0:
            heuristics.append(
                &#34;- **Freeboard_Min_m &lt; 0**: 일부 Stage에서 Draft가 Molded Depth를 초과(또는 입력/계산 불일치)했습니다. Stage 입력/하중/수조선 데이터 재검증 필요.&#34;
            )

    if &#34;Gate_UKC&#34; in qa.columns:
        ukc_vals = set(qa[&#34;Gate_UKC&#34;].astype(str).str.upper().unique().tolist())
        if ukc_vals.issubset({&#34;N/A&#34;, &#34;NA&#34;, &#34;&#34;, &#34;NONE&#34;}):
            heuristics.append(
                &#34;- **UKC Gate 미평가(N/A)**: forecast_tide / depth_ref / ukc_min 입력이 없으면 UKC 원인 분석이 불가합니다.&#34;
            )

    # Add Hydro Range and HardStop heuristics
    if hydro_gate_col and cnt_hydro &gt; 0:
        heuristics.append(
            f&#34;- **Hydro Range 위반 감지**: `{hydro_gate_col}` FAIL. Hydro_Table_Engineering.json 범위 확장 또는 Stage Displacement 입력 재검증 필요.&#34;
        )
    if hardstop_col and cnt_hardstop &gt; 0:
        heuristics.append(
            f&#34;- **HardStop 감지**: `{hardstop_col}`=TRUE Stage 존재. 이 케이스는 Solver/Optimizer로 해결하기 전에 입력/정의역(DeckWet, Hydro out-of-range 등)부터 정정 필요.&#34;
        )

    if not heuristics:
        heuristics.append(
            &#34;- 자동 분류 가능한 주요 원인이 발견되지 않았습니다. (입력값/제약/목표 재확인 필요)&#34;
        )

    # Build a compact violations table (Markdown) with extended columns
    view_cols = [
        &#34;Stage&#34;,
        &#34;Gate_FWD_Max&#34;,
        &#34;FWD_Margin_m&#34;,
        &#34;Gate_AFT_Min&#34;,
        &#34;AFT_Margin_m&#34;,
        &#34;Gate_Freeboard&#34;,
        &#34;Freeboard_Min_m&#34;,
        &#34;Gate_UKC&#34;,
    ]
    # Add split 2.70m gate columns
    view_cols.extend(
        [
            &#34;GateA_AFT_MIN_2p70_PASS&#34;,
            &#34;GateA_AFT_MIN_2p70_Margin_m&#34;,
            &#34;GateB_FWD_MAX_2p70_CD_PASS&#34;,
            &#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34;,
            &#34;GateB_FWD_MAX_2p70_CD_applicable&#34;,
        ]
    )
    # Add extended gate columns if they exist
    if hydro_gate_col and hydro_gate_col not in view_cols:
        view_cols.append(hydro_gate_col)
    if hardstop_col and hardstop_col not in view_cols:
        view_cols.append(hardstop_col)
    if hardstop_reason_col and hardstop_reason_col not in view_cols:
        view_cols.append(hardstop_reason_col)

    cols = [c for c in view_cols if c in qa.columns]
    qa_view = qa[cols].copy()

    # Sort rows by priority: HardStop &gt; HydroRange &gt; other gate fails
    def _row_score(row):
        score = 0
        if hardstop_col and hardstop_col in qa.columns:
            if _is_true(row.get(hardstop_col)):
                score += 1000
        if hydro_gate_col and hydro_gate_col in qa.columns:
            if _is_ng_gate(row.get(hydro_gate_col)):
                score += 200
        # Other gate fails
        for g in base_gate_cols:
            if g in qa.columns and _is_ng_gate(row.get(g)):
                score += 10
        return score

    qa_view[&#34;_sort_score&#34;] = qa_view.apply(_row_score, axis=1)
    qa_view = qa_view.sort_values(&#34;_sort_score&#34;, ascending=False).drop(
        &#34;_sort_score&#34;, axis=1
    )

    def _to_md_table(df: pd.DataFrame, max_rows: int = 50) -&gt; str:
        df2 = df.head(max_rows)
        return df2.to_markdown(index=False)

    md = []
    md.append(&#34;# Gate FAIL 원인별 자동 리포트&#34;)
    md.append(&#34;&#34;)
    md.append(f&#34;- **Generated:** {datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#34;)
    md.append(f&#34;- **Site:** {site}&#34;)
    md.append(
        f&#34;- **Profile:** {str(profile_path) if profile_path else &#39;N/A (defaults/CLI)&#39;}&#34;
    )
    md.append(f&#34;- **Stage QA:** {stage_qa_csv.name}&#34;)
    md.append(f&#34;- **Tank SSOT:** {tank_ssot_csv.name}&#34;)
    md.append(&#34;&#34;)
    md.append(&#34;## 1) Gate 위반 요약&#34;)
    md.append(&#34;&#34;)
    md.append(&#34;### Exec Summary (Captain vs Mammoet 기준 2.70m 정의)&#34;)
    md.append(&#34;&#34;)
    md.append(
        f&#34;* **Captain(선장) 기준 {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m:** **AFT 최소 흘수(AFT draft ≥ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m)** — 비상 시 **프로펠러 효율/추진 확보** 목적.&#34;
    )
    md.append(
        f&#34;* **Mammoet(계산/시뮬레이션/플랜) 기준 {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m:** **FWD 흘수 상한(FWD draft ≤ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m, Chart Datum reference)** — **Critical RoRo 단계에서 Forward draft를 {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m limit 내**로 유지.&#34;
    )
    md.append(
        f&#34;* 따라서 {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m는 단일 조건이 아니라 **“AFT_MIN(≥{CAPTAIN_AFT_MIN_DRAFT_M:.2f})” + “FWD_MAX(≤{MAMMOET_FWD_MAX_DRAFT_M_CD:.2f})” 2개 Gate**로 분리해 표기한다.&#34;
    )
    md.append(&#34;&#34;)
    md.append(
        f&#34;ENG-KR 1L: *Captain = AFT minimum {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m; Mammoet = FWD limit {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m (critical RoRo, chart datum referenced).*&#34;
    )
    md.append(&#34;&#34;)
    md.append(&#34;### Visual-first (Gate 정의표 — 문서/계산에 그대로 복붙)&#34;)
    md.append(&#34;&#34;)
    md.append(
        &#34;| Owner/Source | Gate Name | Definition | Direction | When applied | Evidence |&#34;
    )
    md.append(&#34;|---|---|---|---|---|---|&#34;)
    md.append(
        f&#34;| **Captain** | **AFT_MIN_DRAFT** | AFT draft shall be **≥ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m** for propeller effectiveness in emergency | Min | Emergency propulsion 필요 운영구간(특히 RoRo 포함) | |&#34;
    )
    md.append(
        f&#34;| **Mammoet / Ballast Plan** | **FWD_MAX_DRAFT (CD ref)** | Forward draft (Chart Datum referenced) shall be **≤ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m** during **critical RoRo stages** | Max | Critical RoRo stages (램프/롤온 피크 구간) | |&#34;
    )
    md.append(&#34;&#34;)
    md.append(&#34;### (승인/검토용) “Chart Datum”이 붙는 이유 — 정의만&#34;)
    md.append(&#34;&#34;)
    md.append(
        &#34;* **Chart Datum**은 해도 수심(Charted depth)과 조위(Height of tide)의 기준면으로, **charted depth + tide height**로 해당 시간의 수심을 산정할 수 있다. ([IHO C-51](https://docs.iho.int/iho_pubs/CB/C_51/C_51_Ed500_062014.pdf))&#34;
    )
    md.append(
        &#34;* BLU Code Ship–Shore Safety Checklist는 **berth available depth** 및 **arrival/departure draft**를 ship–shore가 합의해 기입하도록 요구한다. ([IMO BLU Code checklist](https://www.imorules.com/GUID-87E4BBE7-3CE1-4BDC-92F6-CDFDC8850810.html))&#34;
    )
    md.append(&#34;&#34;)
    md.append(&#34;### 운영 적용 룰(혼선 방지 3줄)&#34;)
    md.append(&#34;&#34;)
    md.append(
        f&#34;1. 문서/리포트에는 반드시 **AFT_MIN {CAPTAIN_AFT_MIN_DRAFT_M:.2f}** 과 **FWD_MAX {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}**를 **각각 별도 Gate로 표기**&#34;
    )
    md.append(
        &#34;2. Stage별로 어떤 Gate가 “강제(applicable)”인지 명시(특히 Mammoet는 “critical RoRo stages”만 적용)&#34;
    )
    md.append(
        &#34;3. “Chart Datum” 관련 계산은 **동일 datum 기반의 charted depth + 공식 tide** 조합으로만 수행(승인 대응)&#34;
    )
    md.append(&#34;&#34;)
    md.append(&#34;### Gate 위반 요약 (Counts)&#34;)
    md.append(&#34;&#34;)
    md.append(f&#34;- FWD_Max: **{cnt_fwd}** stage(s)&#34;)
    md.append(f&#34;- AFT_Min: **{cnt_aft}** stage(s)&#34;)
    md.append(f&#34;- Freeboard: **{cnt_fb}** stage(s)&#34;)
    md.append(f&#34;- UKC: **{cnt_ukc}** stage(s) (N/A={na_ukc})&#34;)
    md.append(f&#34;- GateA_AFT_MIN_2p70 (Captain): **{cnt_gate_a}** stage(s)&#34;)
    md.append(f&#34;- GateB_FWD_MAX_2p70_CD (Mammoet): **{cnt_gate_b}** stage(s)&#34;)
    if hydro_gate_col:
        md.append(
            f&#34;- {hydro_gate_col}: **{cnt_hydro}** stage(s) (Hydro Table range out-of-domain 감지)&#34;
        )
    if hardstop_col:
        md.append(
            f&#34;- {hardstop_col}: **{cnt_hardstop}** stage(s) (HardStop=TRUE → 즉시 데이터/정의역 점검 권고)&#34;
        )
    md.append(&#34;&#34;)

    # HardStop Summary Section (if exists)
    if hardstop_col and cnt_hardstop &gt; 0:
        md.append(&#34;## 2) HardStop Summary&#34;)
        md.append(&#34;&#34;)
        md.append(f&#34;- HardStop Count: **{cnt_hardstop} / {len(qa)}**&#34;)
        if hardstop_stages:
            stages_display = hardstop_stages[:50]
            if len(hardstop_stages) &gt; 50:
                md.append(f&#34;- HardStop Stages: {&#39;, &#39;.join(stages_display)} ...&#34;)
            else:
                md.append(f&#34;- HardStop Stages: {&#39;, &#39;.join(stages_display)}&#34;)
        md.append(&#34;&#34;)

        if hardstop_reason_to_stages:
            md.append(&#34;### 2.1 HardStop Reason Breakdown&#34;)
            md.append(&#34;&#34;)
            md.append(&#34;| Reason | Count | Stages (up to 20) |&#34;)
            md.append(&#34;|---|---:|---|&#34;)
            for reason, stgs in sorted(
                hardstop_reason_to_stages.items(), key=lambda x: (-len(x[1]), x[0])
            ):
                stages_list = stgs[:20]
                stages_str = &#34;, &#34;.join(stages_list)
                if len(stgs) &gt; 20:
                    stages_str += &#34; ...&#34;
                md.append(f&#34;| {reason} | {len(stgs)} | {stages_str} |&#34;)
        md.append(&#34;&#34;)

    section_num_ssot = &#34;2&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;3&#34;
    md.append(f&#34;## {section_num_ssot}) SSOT(Current_t) 상태 요약&#34;)
    md.append(&#34;&#34;)
    md.append(f&#34;- Tanks: {tanks_total}&#34;)
    md.append(f&#34;- Current_t == 0.0: {tanks_zero}/{tanks_total}&#34;)
    md.append(f&#34;- Total Current_t (sum): {tanks_sum:.3f} t&#34;)
    if sensor_stats:
        sensor_section = &#34;2.1&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;3.1&#34;
        md.append(&#34;&#34;)
        md.append(f&#34;### {sensor_section} Sensor Sync 적용 결과&#34;)
        md.append(&#34;&#34;)
        md.append(f&#34;- Sensor CSV: {sensor_stats.get(&#39;sensor_csv&#39;)}&#34;)
        md.append(f&#34;- Strategy: {sensor_stats.get(&#39;strategy&#39;)}&#34;)
        md.append(f&#34;- Value mode: {sensor_stats.get(&#39;value_mode&#39;)}&#34;)
        md.append(f&#34;- Updated (exact): {sensor_stats.get(&#39;updated_exact&#39;)}&#34;)
        md.append(f&#34;- Updated (group): {sensor_stats.get(&#39;updated_group&#39;)}&#34;)
        if sensor_stats.get(&#34;bad_value_rows&#34;):
            md.append(
                f&#34;- Bad value rows (sample): {sensor_stats.get(&#39;bad_value_rows&#39;)}&#34;
            )

    if ukc_inputs:
        ukc_section = &#34;3&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;4&#34;
        md.append(&#34;&#34;)
        md.append(f&#34;## {ukc_section}) UKC 입력 상태&#34;)
        md.append(&#34;&#34;)
        for k, v in ukc_inputs.items():
            md.append(f&#34;- {k}: {v}&#34;)

    stage_section = &#34;4&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;5&#34;
    md.append(&#34;&#34;)
    md.append(f&#34;## {stage_section}) Stage별 Gate 상태&#34;)
    md.append(&#34;&#34;)
    md.append(_to_md_table(qa_view))

    heuristics_section = &#34;5&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;6&#34;
    md.append(&#34;&#34;)
    md.append(f&#34;## {heuristics_section}) 자동 원인 추정(Heuristics)&#34;)
    md.append(&#34;&#34;)
    md.extend(heuristics)

    action_section = &#34;6&#34; if not (hardstop_col and cnt_hardstop &gt; 0) else &#34;7&#34;
    md.append(&#34;&#34;)
    md.append(f&#34;## {action_section}) 권고 조치&#34;)
    md.append(&#34;&#34;)
    md.append(
        &#34;- **Current_t 실측/센서 값 주입 후 재실행** (Stage QA → Solver → Optimizer 순)&#34;
    )
    md.append(
        &#34;- **Site Profile(AGI/DAS)로 Gate/펌프율/UKC 입력값을 분리 관리** (코드 수정 없이 JSON 수정)&#34;
    )
    md.append(
        &#34;- **Freeboard 음수 발생 시 Stage 하중/수조선 입력 재검증** (Draft &gt; Molded Depth 케이스)&#34;
    )
    if hardstop_col and cnt_hardstop &gt; 0:
        md.append(
            &#34;- **HardStop 발생 Stage: 입력 데이터/정의역(Hydro Table 범위 등) 재검증 후 재실행** (Solver/Optimizer 실행 전 필수)&#34;
        )
    md.append(&#34;&#34;)

    out_md.write_text(&#34;\n&#34;.join(md), encoding=&#34;utf-8&#34;)
    return out_md</code></pre>
</details>
<div class="desc"><p>Generate a Markdown report summarizing gate failures and likely root causes.</p>
<p>PATCH: Extended with Gate_Hydro_Range and HardStop summary support.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_stage_QA_csv"><code class="name flex">
<span>def <span class="ident">generate_stage_QA_csv</span></span>(<span>stage_table_csv: Path,<br>out_qa_csv: Path,<br>fwd_max_m: float = None,<br>aft_min_m: float = None,<br>d_vessel_m: float = None,<br>forecast_tide_m: Optional[float] = None,<br>depth_ref_m: Optional[float] = None,<br>ukc_min_m: Optional[float] = None,<br>critical_only: bool = False,<br>critical_stage_list: Optional[List[str]] = None,<br>critical_regex: str = '(preballast.*critical|6a.*critical|stage\\s*5.*preballast|stage\\s*6a)',<br>gateb_fwd_max_m_cd: Optional[float] = None,<br>squat_m: float = 0.0,<br>safety_allow_m: float = 0.0,<br>hydro_json_path: Optional[Path] = None,<br>strict_hardstop: bool = True,<br>tol_m: float = 1e-06,<br>hydro_disp_tol_t: float = 0.001,<br>solver_summary_csv: Optional[Path] = None,<br>hmax_wave_m: Optional[float] = None,<br>four_corner_monitoring: bool = False,<br>tide_tol_m: float = 0.1,<br>trim_abs_limit_m: float = 0.5,<br>trim_limit_enforced: bool = True,<br>freeboard_min_m: float = 0.0,<br>freeboard_min_enforced: bool = True) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_stage_QA_csv(
    stage_table_csv: Path,
    out_qa_csv: Path,
    fwd_max_m: float = None,  # Legacy param, read from CSV instead
    aft_min_m: float = None,  # Legacy param, read from CSV instead
    d_vessel_m: float = None,  # Legacy param, read from CSV instead
    forecast_tide_m: Optional[float] = None,  # Legacy param, read from CSV instead
    depth_ref_m: Optional[float] = None,  # Legacy param, read from CSV instead
    ukc_min_m: Optional[float] = None,  # Legacy param, read from CSV instead
    critical_only: bool = False,
    critical_stage_list: Optional[List[str]] = None,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    gateb_fwd_max_m_cd: Optional[float] = None,
    squat_m: float = 0.0,
    safety_allow_m: float = 0.0,
    hydro_json_path: Optional[Path] = None,
    strict_hardstop: bool = True,
    tol_m: float = 1e-6,
    hydro_disp_tol_t: float = 1e-3,
    solver_summary_csv: Optional[Path] = None,
    hmax_wave_m: Optional[float] = None,
    four_corner_monitoring: bool = False,
    tide_tol_m: float = DEFAULT_TIDE_TOL_M,
    trim_abs_limit_m: float = 0.50,
    trim_limit_enforced: bool = True,
    freeboard_min_m: float = 0.0,
    freeboard_min_enforced: bool = True,
) -&gt; Path:
    &#34;&#34;&#34;
    stage_table_unified.csv -&gt; pipeline_stage_QA.csv

    PATCH:
      - Gate_Hydro_Range 추가 (Disp_t in hydro disp range)
      - HardStop_Any(Y/N) + HardStop_Reason 합성
      - raw vs post-solve draft 분리 (solver_summary_csv 적용 시)
      - Gate_Freeboard_ND (GL Noble Denton 0013/ND effective freeboard)
      - strict_hardstop=True이면 QA CSV 생성 후 ValueError로 중단
    &#34;&#34;&#34;
    import math

    import numpy as np

    stage_table_csv = Path(stage_table_csv)
    out_qa_csv = Path(out_qa_csv)

    df = pd.read_csv(stage_table_csv)

    required = [
        &#34;Stage&#34;,
        &#34;Current_FWD_m&#34;,
        &#34;Current_AFT_m&#34;,
        &#34;FWD_MAX_m&#34;,
        &#34;AFT_MIN_m&#34;,
        &#34;D_vessel_m&#34;,
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(
            f&#34;[HARDSTOP] stage_table_unified.csv missing required columns: {missing}&#34;
        )

    # Drafts (raw)
    df[&#34;Draft_FWD_m_raw&#34;] = pd.to_numeric(df[&#34;Current_FWD_m&#34;], errors=&#34;coerce&#34;)
    df[&#34;Draft_AFT_m_raw&#34;] = pd.to_numeric(df[&#34;Current_AFT_m&#34;], errors=&#34;coerce&#34;)
    df[&#34;Draft_FWD_m&#34;] = df[&#34;Draft_FWD_m_raw&#34;].copy()
    df[&#34;Draft_AFT_m&#34;] = df[&#34;Draft_AFT_m_raw&#34;].copy()
    df[&#34;Draft_Source&#34;] = &#34;raw&#34;

    # Apply solver results (post-solve drafts) if provided
    if solver_summary_csv is not None and Path(solver_summary_csv).exists():
        try:
            solver_df = pd.read_csv(solver_summary_csv, encoding=&#34;utf-8-sig&#34;)
            col_fwd = (
                &#34;New_FWD_m&#34;
                if &#34;New_FWD_m&#34; in solver_df.columns
                else (&#34;new_fwd_m&#34; if &#34;new_fwd_m&#34; in solver_df.columns else None)
            )
            col_aft = (
                &#34;New_AFT_m&#34;
                if &#34;New_AFT_m&#34; in solver_df.columns
                else (&#34;new_aft_m&#34; if &#34;new_aft_m&#34; in solver_df.columns else None)
            )
            if not solver_df.empty and col_fwd and col_aft:
                if &#34;Stage&#34; in solver_df.columns:
                    solver_df = solver_df.copy()
                    solver_df[&#34;_stage_norm&#34;] = (
                        solver_df[&#34;Stage&#34;].astype(str).str.strip().str.lower()
                    )
                    for idx, row in df.iterrows():
                        stage_name = str(row.get(&#34;Stage&#34;, &#34;&#34;)).strip().lower()
                        match = solver_df[solver_df[&#34;_stage_norm&#34;] == stage_name]
                        if match.empty:
                            continue
                        solver_row = match.iloc[0]
                        if pd.notna(solver_row.get(col_fwd)):
                            df.at[idx, &#34;Draft_FWD_m&#34;] = float(solver_row[col_fwd])
                            df.at[idx, &#34;Draft_Source&#34;] = &#34;solver&#34;
                        if pd.notna(solver_row.get(col_aft)):
                            df.at[idx, &#34;Draft_AFT_m&#34;] = float(solver_row[col_aft])
                            df.at[idx, &#34;Draft_Source&#34;] = &#34;solver&#34;
                else:
                    solver_row = solver_df.iloc[0]
                    if pd.notna(solver_row.get(col_fwd)) and pd.notna(
                        solver_row.get(col_aft)
                    ):
                        df[&#34;Draft_FWD_m&#34;] = float(solver_row[col_fwd])
                        df[&#34;Draft_AFT_m&#34;] = float(solver_row[col_aft])
                        df[&#34;Draft_Source&#34;] = &#34;solver&#34;
                print(
                    f&#34;[OK] Solver results applied to QA: {Path(solver_summary_csv).name}&#34;
                )
        except Exception as e:
            print(
                f&#34;[WARN] Failed to apply solver results to QA ({type(e).__name__}: {e})&#34;
            )

    # Draft clipping flags (raw vs solver)
    df[&#34;Draft_Max_raw_m&#34;] = df[[&#34;Draft_FWD_m_raw&#34;, &#34;Draft_AFT_m_raw&#34;]].max(axis=1)
    df[&#34;Draft_Max_solver_m&#34;] = df[[&#34;Draft_FWD_m&#34;, &#34;Draft_AFT_m&#34;]].max(axis=1)
    dv = pd.to_numeric(df[&#34;D_vessel_m&#34;], errors=&#34;coerce&#34;)
    df[&#34;D_vessel_m&#34;] = dv
    df[&#34;Draft_Clipped_raw&#34;] = df[&#34;Draft_Max_raw_m&#34;] &gt; (dv + tol_m)
    df[&#34;Draft_Clipped_solver&#34;] = df[&#34;Draft_Max_solver_m&#34;] &gt; (dv + tol_m)

    # Freeboard
    df[&#34;Freeboard_FWD_m&#34;] = (
        pd.to_numeric(df[&#34;D_vessel_m&#34;], errors=&#34;coerce&#34;) - df[&#34;Draft_FWD_m&#34;]
    )
    df[&#34;Freeboard_AFT_m&#34;] = (
        pd.to_numeric(df[&#34;D_vessel_m&#34;], errors=&#34;coerce&#34;) - df[&#34;Draft_AFT_m&#34;]
    )
    df[&#34;Freeboard_Min_m&#34;] = df[[&#34;Freeboard_FWD_m&#34;, &#34;Freeboard_AFT_m&#34;]].min(axis=1)
    # Explicit label: Bow/Stern min freeboard (avoid linkspan freeboard confusion)
    df[&#34;Freeboard_Min_BowStern_m&#34;] = df[&#34;Freeboard_Min_m&#34;]

    # Trim (cm) for gate evaluation
    # Always use solver/plan drafts to avoid Current_* vs Draft_* mismatch.
    trim_cm_from_draft = (df[&#34;Draft_AFT_m&#34;] - df[&#34;Draft_FWD_m&#34;]) * 100.0
    df[&#34;Trim_cm&#34;] = trim_cm_from_draft
    if &#34;Input_Trim_cm&#34; in df.columns:
        input_trim_cm = pd.to_numeric(df[&#34;Input_Trim_cm&#34;], errors=&#34;coerce&#34;)
        df[&#34;Input_Trim_cm&#34;] = input_trim_cm
        df[&#34;Trim_cm_diff_plan_vs_input&#34;] = trim_cm_from_draft - input_trim_cm
    else:
        df[&#34;Trim_cm_diff_plan_vs_input&#34;] = math.nan

    # Gate-FB (GL Noble Denton 0013/ND) - Effective freeboard requirement
    if hmax_wave_m is not None and hmax_wave_m &gt; 0:
        if four_corner_monitoring:
            df[&#34;Freeboard_Req_ND_m&#34;] = 0.50 + 0.50 * float(hmax_wave_m)
            df[&#34;Freeboard_ND_Monitoring&#34;] = &#34;4-corner&#34;
        else:
            df[&#34;Freeboard_Req_ND_m&#34;] = 0.80 + 0.50 * float(hmax_wave_m)
            df[&#34;Freeboard_ND_Monitoring&#34;] = &#34;None&#34;
        df[&#34;Gate_Freeboard_ND&#34;] = np.where(
            df[&#34;Freeboard_Min_m&#34;] &gt;= df[&#34;Freeboard_Req_ND_m&#34;] - tol_m, &#34;OK&#34;, &#34;NG&#34;
        )
        df[&#34;Freeboard_ND_Margin_m&#34;] = df[&#34;Freeboard_Min_m&#34;] - df[&#34;Freeboard_Req_ND_m&#34;]
    else:
        df[&#34;Freeboard_Req_ND_m&#34;] = math.nan
        df[&#34;Gate_Freeboard_ND&#34;] = &#34;N/A&#34;
        df[&#34;Freeboard_ND_Margin_m&#34;] = math.nan
        df[&#34;Freeboard_ND_Monitoring&#34;] = &#34;N/A&#34;

    # Option B: Gate-Trim (hard constraint if enabled)
    trim_limit_cm = float(trim_abs_limit_m) * 100.0
    trim_tol_cm = float(tol_m) * 100.0
    if trim_limit_enforced:
        df[&#34;Gate_Trim&#34;] = np.where(
            df[&#34;Trim_cm&#34;].abs() &lt;= (trim_limit_cm + trim_tol_cm), &#34;OK&#34;, &#34;NG&#34;
        )
        df[&#34;Trim_Margin_cm&#34;] = trim_limit_cm - df[&#34;Trim_cm&#34;].abs()
    else:
        df[&#34;Gate_Trim&#34;] = &#34;N/A&#34;
        df[&#34;Trim_Margin_cm&#34;] = math.nan

    # Option B: Gate-Freeboard minimum (hard constraint if enabled)
    if freeboard_min_enforced:
        df[&#34;Gate_Freeboard_MIN&#34;] = np.where(
            df[&#34;Freeboard_Min_m&#34;] &gt;= float(freeboard_min_m) - tol_m, &#34;OK&#34;, &#34;NG&#34;
        )
        df[&#34;Freeboard_MIN_Margin_m&#34;] = df[&#34;Freeboard_Min_m&#34;] - float(freeboard_min_m)
    else:
        df[&#34;Gate_Freeboard_MIN&#34;] = &#34;N/A&#34;
        df[&#34;Freeboard_MIN_Margin_m&#34;] = math.nan

    # Gates (기존)
    fwd_lim = pd.to_numeric(df[&#34;FWD_MAX_m&#34;], errors=&#34;coerce&#34;)
    # Gate-B uses Chart Datum. Convert drafts to CD if tide is available.
    fwd_gate = df[&#34;Draft_FWD_m&#34;]
    if &#34;Forecast_Tide_m&#34; in df.columns:
        df[&#34;Draft_FWD_m_CD&#34;] = pd.to_numeric(
            df[&#34;Draft_FWD_m&#34;], errors=&#34;coerce&#34;
        ) - pd.to_numeric(df[&#34;Forecast_Tide_m&#34;], errors=&#34;coerce&#34;)
        fwd_gate = df[&#34;Draft_FWD_m_CD&#34;]
    if &#34;FWD_MAX_applicable&#34; in df.columns:
        mask_app = df[&#34;FWD_MAX_applicable&#34;] == True
    else:
        mask_app = fwd_lim.notna()

    df[&#34;Gate_FWD_Max&#34;] = &#34;OK&#34;
    mask_ng = mask_app &amp; (fwd_gate &gt; fwd_lim + tol_m)
    df.loc[mask_ng, &#34;Gate_FWD_Max&#34;] = &#34;NG&#34;
    df[&#34;Gate_AFT_Min&#34;] = np.where(
        df[&#34;Draft_AFT_m&#34;] &gt;= pd.to_numeric(df[&#34;AFT_MIN_m&#34;], errors=&#34;coerce&#34;) - tol_m,
        &#34;OK&#34;,
        &#34;NG&#34;,
    )
    df[&#34;Gate_Freeboard&#34;] = np.where(df[&#34;Freeboard_Min_m&#34;] &gt;= -tol_m, &#34;OK&#34;, &#34;NG&#34;)

    # Margins
    df[&#34;FWD_Margin_m&#34;] = np.nan
    df.loc[mask_app, &#34;FWD_Margin_m&#34;] = fwd_lim[mask_app] - fwd_gate[mask_app]
    df[&#34;AFT_Margin_m&#34;] = df[&#34;Draft_AFT_m&#34;] - pd.to_numeric(
        df[&#34;AFT_MIN_m&#34;], errors=&#34;coerce&#34;
    )

    # ----------------------------------------------------------
    # Gate-A / Gate-B SSOT labels (avoid ambiguous &#34;2.70m&#34;)
    # ----------------------------------------------------------
    df[&#34;AFT_MIN_2p70_m&#34;] = float(GATE_A_VALUE_M)
    df[&#34;AFT_Margin_2p70_m&#34;] = pd.to_numeric(df[&#34;Draft_AFT_m&#34;], errors=&#34;coerce&#34;) - float(
        GATE_A_VALUE_M
    )
    df[&#34;Gate_AFT_MIN_2p70&#34;] = np.where(df[&#34;AFT_Margin_2p70_m&#34;] &gt;= -tol_m, &#34;OK&#34;, &#34;NG&#34;)
    df[&#34;Gate_AFT_MIN_2p70_PASS&#34;] = df[&#34;Gate_AFT_MIN_2p70&#34;] == &#34;OK&#34;
    if &#34;GateA_AFT_MIN_2p70_PASS&#34; not in df.columns:
        df[&#34;GateA_AFT_MIN_2p70_PASS&#34;] = df[&#34;Gate_AFT_MIN_2p70_PASS&#34;]

    # Gate-B scope (critical-only) — ALWAYS compute by critical matcher
    stage_is_critical = (
        df[&#34;Stage&#34;]
        .astype(str)
        .apply(
            lambda s: _is_critical_stage(
                s, patterns=critical_stage_list, regex=critical_regex
            )
        )
        .fillna(False)
    )
    df[&#34;Gate_B_Applies&#34;] = stage_is_critical

    df[&#34;FWD_MAX_2p70_m&#34;] = float(GATE_B_VALUE_M)
    df[&#34;FWD_Margin_2p70_m&#34;] = float(GATE_B_VALUE_M) - pd.to_numeric(
        fwd_gate, errors=&#34;coerce&#34;
    )
    df[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] = np.where(
        df[&#34;Gate_B_Applies&#34;],
        np.where(df[&#34;FWD_Margin_2p70_m&#34;] &gt;= -tol_m, &#34;OK&#34;, &#34;NG&#34;),
        &#34;N/A&#34;,
    )
    df.loc[~df[&#34;Gate_B_Applies&#34;], &#34;Gate_FWD_MAX_2p70_critical_only&#34;] = &#34;N/A&#34;
    # Extra safety: ensure all non-critical are truly &#34;N/A&#34; string (not nan)
    df[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] = df[
        &#34;Gate_FWD_MAX_2p70_critical_only&#34;
    ].fillna(&#34;N/A&#34;)
    df[&#34;Gate_FWD_MAX_2p70_critical_only_PASS&#34;] = (
        df[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] == &#34;OK&#34;
    )

    # -----------------------------
    # UKC (Definition-split)
    # -----------------------------
    has_ukc_inputs = (
        (&#34;Forecast_Tide_m&#34; in df.columns)
        and (&#34;DepthRef_m&#34; in df.columns)
        and (&#34;UKC_Min_m&#34; in df.columns)
    )
    if has_ukc_inputs:
        df[&#34;UKC_Min_Required_m&#34;] = pd.to_numeric(df[&#34;UKC_Min_m&#34;], errors=&#34;coerce&#34;)

        available_depth = pd.to_numeric(
            df[&#34;DepthRef_m&#34;], errors=&#34;coerce&#34;
        ) + pd.to_numeric(df[&#34;Forecast_Tide_m&#34;], errors=&#34;coerce&#34;)
        df[&#34;UKC_FWD_m&#34;] = available_depth - (
            df[&#34;Draft_FWD_m&#34;] + float(squat_m) + float(safety_allow_m)
        )
        df[&#34;UKC_AFT_m&#34;] = available_depth - (
            df[&#34;Draft_AFT_m&#34;] + float(squat_m) + float(safety_allow_m)
        )
        df[&#34;UKC_Min_m&#34;] = df[[&#34;UKC_FWD_m&#34;, &#34;UKC_AFT_m&#34;]].min(axis=1)

        df[&#34;Gate_UKC&#34;] = np.where(
            df[&#34;UKC_Min_m&#34;] &gt;= df[&#34;UKC_Min_Required_m&#34;] - tol_m, &#34;OK&#34;, &#34;NG&#34;
        )
        df[&#34;UKC_Margin_m&#34;] = df[&#34;UKC_Min_m&#34;] - df[&#34;UKC_Min_Required_m&#34;]

        df[&#34;Required_WL_for_UKC_m&#34;] = (
            df[[&#34;Draft_FWD_m&#34;, &#34;Draft_AFT_m&#34;]].max(axis=1)
            + float(squat_m)
            + float(safety_allow_m)
            + df[&#34;UKC_Min_Required_m&#34;]
            - pd.to_numeric(df[&#34;DepthRef_m&#34;], errors=&#34;coerce&#34;)
        )
    else:
        df[&#34;UKC_Min_Required_m&#34;] = math.nan
        df[&#34;UKC_FWD_m&#34;] = math.nan
        df[&#34;UKC_AFT_m&#34;] = math.nan
        df[&#34;UKC_Min_m&#34;] = math.nan
        df[&#34;Gate_UKC&#34;] = &#34;N/A&#34;
        df[&#34;UKC_Margin_m&#34;] = math.nan
        df[&#34;Required_WL_for_UKC_m&#34;] = math.nan

    # -----------------------------
    # NEW Gate: Hydro range
    # -----------------------------
    df[&#34;Gate_Hydro_Range&#34;] = &#34;N/A&#34;
    df[&#34;HydroDisp_Margin_t&#34;] = math.nan
    hmin = None
    hmax = None
    disp = None

    def _auto_find_hydro_json(start: Path):
        candidates = [
            start.parent / &#34;bplus_inputs&#34; / &#34;Hydro_Table_Engineering.json&#34;,
            start.parent.parent / &#34;bplus_inputs&#34; / &#34;Hydro_Table_Engineering.json&#34;,
            start.parent / &#34;Hydro_Table_Engineering.json&#34;,
        ]
        for p in candidates:
            if p.exists():
                return p
        return None

    if &#34;Input_Disp_t&#34; in df.columns:
        disp = pd.to_numeric(df[&#34;Input_Disp_t&#34;], errors=&#34;coerce&#34;)

        if (
            (&#34;Hydro_Disp_Min_t&#34; in df.columns)
            and (&#34;Hydro_Disp_Max_t&#34; in df.columns)
            and (not df[&#34;Hydro_Disp_Min_t&#34;].isna().all())
        ):
            hmin = float(
                pd.to_numeric(df[&#34;Hydro_Disp_Min_t&#34;], errors=&#34;coerce&#34;).dropna().iloc[0]
            )
            hmax = float(
                pd.to_numeric(df[&#34;Hydro_Disp_Max_t&#34;], errors=&#34;coerce&#34;).dropna().iloc[0]
            )
    else:
        hydro_path = (
            Path(hydro_json_path)
            if hydro_json_path
            else _auto_find_hydro_json(stage_table_csv)
        )
        if hydro_path is not None and hydro_path.exists():
            try:
                with open(hydro_path, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
                    hydro_obj = json.load(f)

                rows = None
                if isinstance(hydro_obj, list):
                    rows = hydro_obj
                elif isinstance(hydro_obj, dict):
                    for k in (&#34;rows&#34;, &#34;table&#34;, &#34;data&#34;, &#34;HydroTable&#34;):
                        if k in hydro_obj and isinstance(hydro_obj[k], list):
                            rows = hydro_obj[k]
                            break
                    if rows is None and all(
                        isinstance(v, dict) for v in hydro_obj.values()
                    ):
                        rows = list(hydro_obj.values())

                disp_vals = []
                if isinstance(rows, list):
                    for r in rows:
                        if not isinstance(r, dict):
                            continue
                        for k, v in r.items():
                            if str(k).strip().lower() in (
                                &#34;disp_t&#34;,
                                &#34;disp&#34;,
                                &#34;displacement_t&#34;,
                                &#34;displacement&#34;,
                            ):
                                try:
                                    disp_vals.append(float(v))
                                except Exception:
                                    pass
                                break

                if disp_vals:
                    hmin = min(disp_vals)
                    hmax = max(disp_vals)
                    df[&#34;Hydro_Disp_Min_t&#34;] = hmin
                    df[&#34;Hydro_Disp_Max_t&#34;] = hmax
            except Exception:
                pass

    if hmin is not None and hmax is not None and disp is not None:
        ok = (disp &gt;= hmin - hydro_disp_tol_t) &amp; (disp &lt;= hmax + hydro_disp_tol_t)
        df[&#34;Gate_Hydro_Range&#34;] = np.where(ok, &#34;OK&#34;, &#34;NG&#34;)

        lower_margin = disp - hmin
        upper_margin = hmax - disp
        df[&#34;HydroDisp_Margin_t&#34;] = np.where(
            ok,
            np.minimum(lower_margin, upper_margin),
            -np.minimum(abs(lower_margin), abs(upper_margin)),
        )

    # -----------------------------
    # HARDSTOP synthesis
    # -----------------------------
    if &#34;HS_Any&#34; in df.columns:
        hardstop_bool = df[&#34;HS_Any&#34;] == True
    else:
        hardstop_bool = (
            (df[&#34;Gate_Freeboard&#34;] == &#34;NG&#34;)
            | (df[&#34;Gate_Hydro_Range&#34;] == &#34;NG&#34;)
            | df[&#34;Draft_FWD_m&#34;].isna()
            | df[&#34;Draft_AFT_m&#34;].isna()
        )
    if trim_limit_enforced:
        hardstop_bool = hardstop_bool | (df[&#34;Gate_Trim&#34;] == &#34;NG&#34;)
    if freeboard_min_enforced:
        hardstop_bool = hardstop_bool | (df[&#34;Gate_Freeboard_MIN&#34;] == &#34;NG&#34;)

    df[&#34;HardStop_Any&#34;] = np.where(hardstop_bool, &#34;Y&#34;, &#34;N&#34;)

    reasons = []
    for _, row in df.iterrows():
        r = []
        if pd.isna(row[&#34;Draft_FWD_m&#34;]) or pd.isna(row[&#34;Draft_AFT_m&#34;]):
            r.append(&#34;DraftNaN&#34;)
        if row.get(&#34;Gate_Freeboard&#34;) == &#34;NG&#34;:
            r.append(&#34;OverDepth/DeckWet&#34;)
        if trim_limit_enforced and row.get(&#34;Gate_Trim&#34;) == &#34;NG&#34;:
            r.append(&#34;TrimLimit&#34;)
        if freeboard_min_enforced and row.get(&#34;Gate_Freeboard_MIN&#34;) == &#34;NG&#34;:
            r.append(&#34;FreeboardMin&#34;)
        if row.get(&#34;Gate_Hydro_Range&#34;) == &#34;NG&#34;:
            r.append(&#34;HydroOutOfRange&#34;)
        reasons.append(&#34;|&#34;.join(r) if r else &#34;&#34;)
    df[&#34;HardStop_Reason&#34;] = reasons

    # Add Tmean_m column for compatibility
    if &#34;Tmean_m&#34; not in df.columns:
        df[&#34;Tmean_m&#34;] = (df[&#34;Draft_FWD_m&#34;] + df[&#34;Draft_AFT_m&#34;]) / 2.0

    # ----------------------------------------------------------
    # PATCH: split 2.70m gates (Captain vs Mammoet) into QA CSV
    #   - GateA_AFT_MIN_2p70_PASS (Captain)
    #   - GateB_FWD_MAX_2p70_CD_PASS (Mammoet; profile-driven critical stages)
    # ----------------------------------------------------------
    try:
        gateb_fwd_max = (
            float(gateb_fwd_max_m_cd)
            if gateb_fwd_max_m_cd is not None
            else MAMMOET_FWD_MAX_DRAFT_M_CD
        )
        df = add_split_270_gates(
            df,
            aft_min_m=CAPTAIN_AFT_MIN_DRAFT_M,
            fwd_max_m_cd=gateb_fwd_max,
            critical_only=critical_only,
            critical_regex=critical_regex,
            critical_stage_list=critical_stage_list,
            tol_m=float(tol_m),
        )
    except Exception as e:
        # Do not hard-stop QA generation if the split gate columns cannot be added
        print(f&#34;[WARN] add_split_270_gates skipped in generate_stage_QA_csv: {e}&#34;)

    # ------------------------------------------------------------------
    # Tide/UKC user-facing columns (AGI 기준 통합 출력)
    # ------------------------------------------------------------------
    try:
        # Required tide (clamped to &gt;=0), margin, and aliases requested by operations
        df[&#34;Tide_required_m&#34;] = pd.to_numeric(
            df.get(&#34;Required_WL_for_UKC_m&#34;), errors=&#34;coerce&#34;
        ).clip(lower=0.0)
        df[&#34;Forecast_tide_m&#34;] = pd.to_numeric(
            df.get(&#34;Forecast_Tide_m&#34;), errors=&#34;coerce&#34;
        )
        df[&#34;Tide_margin_m&#34;] = df[&#34;Forecast_tide_m&#34;] - df[&#34;Tide_required_m&#34;]

        # UKC requirement vs actual ends
        df[&#34;UKC_min_m&#34;] = pd.to_numeric(df.get(&#34;UKC_Min_Required_m&#34;), errors=&#34;coerce&#34;)
        df[&#34;UKC_fwd_m&#34;] = pd.to_numeric(df.get(&#34;UKC_FWD_m&#34;), errors=&#34;coerce&#34;)
        df[&#34;UKC_aft_m&#34;] = pd.to_numeric(df.get(&#34;UKC_AFT_m&#34;), errors=&#34;coerce&#34;)
        df[&#34;UKC_min_actual_m&#34;] = pd.to_numeric(df.get(&#34;UKC_Min_m&#34;), errors=&#34;coerce&#34;)

        # Tide verification (OK/LIMIT/FAIL/VERIFY) based on margin
        tol = float(tide_tol_m) if tide_tol_m is not None else float(DEFAULT_TIDE_TOL_M)
        df[&#34;Tide_verification&#34;] = np.where(
            df[&#34;Forecast_tide_m&#34;].isna(),
            &#34;VERIFY&#34;,
            np.where(
                df[&#34;Tide_margin_m&#34;] &lt; 0.0,
                &#34;FAIL&#34;,
                np.where(df[&#34;Tide_margin_m&#34;] &lt; tol, &#34;LIMIT&#34;, &#34;OK&#34;),
            ),
        )
    except Exception as e:
        print(f&#34;[WARN] Tide alias columns skipped in QA CSV: {e}&#34;)

    df.to_csv(out_qa_csv, index=False, encoding=&#34;utf-8-sig&#34;)

    if strict_hardstop and (df[&#34;HardStop_Any&#34;] == &#34;Y&#34;).any():
        failing = df.loc[
            df[&#34;HardStop_Any&#34;] == &#34;Y&#34;,
            [
                &#34;Stage&#34;,
                &#34;HardStop_Reason&#34;,
                &#34;Draft_FWD_m&#34;,
                &#34;Draft_AFT_m&#34;,
                &#34;Gate_Hydro_Range&#34;,
            ],
        ].copy()
        msg = [
            &#34;[HARDSTOP] One or more stages failed HardStop gates (QA).&#34;,
            f&#34;  - stage_table_csv: {stage_table_csv}&#34;,
            f&#34;  - out_qa_csv      : {out_qa_csv}&#34;,
            &#34;  - failing rows (Stage, Reason, FWD, AFT, HydroGate):&#34;,
        ]
        for _, r in failing.iterrows():
            msg.append(
                f&#34;    * {r[&#39;Stage&#39;]}: {r[&#39;HardStop_Reason&#39;]} | FWD={r[&#39;Draft_FWD_m&#39;]}, AFT={r[&#39;Draft_AFT_m&#39;]}, Hydro={r[&#39;Gate_Hydro_Range&#39;]}&#34;
            )
        raise ValueError(&#34;\n&#34;.join(msg))

    return out_qa_csv</code></pre>
</details>
<div class="desc"><p>stage_table_unified.csv -&gt; pipeline_stage_QA.csv</p>
<h2 id="patch">Patch</h2>
<ul>
<li>Gate_Hydro_Range 추가 (Disp_t in hydro disp range)</li>
<li>HardStop_Any(Y/N) + HardStop_Reason 합성</li>
<li>raw vs post-solve draft 분리 (solver_summary_csv 적용 시)</li>
<li>Gate_Freeboard_ND (GL Noble Denton 0013/ND effective freeboard)</li>
<li>strict_hardstop=True이면 QA CSV 생성 후 ValueError로 중단</li>
</ul></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_tug_operational_sop_md"><code class="name flex">
<span>def <span class="ident">generate_tug_operational_sop_md</span></span>(<span>out_md: Path, stage_qa_csv: Path, site: str = 'AGI') ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_tug_operational_sop_md(
    out_md: Path,
    stage_qa_csv: Path,
    site: str = &#34;AGI&#34;,
) -&gt; Path:
    &#34;&#34;&#34;Generate TUG-assisted operational SOP document (DNV-ST-N001 aligned).&#34;&#34;&#34;
    try:
        import numpy as np

        qa = pd.read_csv(stage_qa_csv, encoding=&#34;utf-8-sig&#34;)

        gate_a_col = None
        if &#34;GateA_AFT_MIN_2p70_PASS&#34; in qa.columns:
            gate_a_col = &#34;GateA_AFT_MIN_2p70_PASS&#34;
        elif &#34;Gate_AFT_Min&#34; in qa.columns:
            gate_a_col = &#34;Gate_AFT_Min&#34;
        if gate_a_col is None:
            return out_md

        def _is_fail(val) -&gt; bool:
            if isinstance(val, (bool, np.bool_)):
                return not bool(val)
            s = str(val).strip().lower()
            return s in (&#34;ng&#34;, &#34;fail&#34;, &#34;false&#34;, &#34;0&#34;)

        failing_stages = qa[qa[gate_a_col].apply(_is_fail)]
        if failing_stages.empty:
            return out_md

        lines = []
        lines.append(&#34;# TUG-Assisted Operational SOP (DNV-ST-N001 aligned)\n&#34;)
        lines.append(f&#34;Site: {site}&#34;)
        lines.append(f&#34;Generated: {pd.Timestamp.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}\n&#34;)
        lines.append(&#34;## Executive Summary&#34;)
        lines.append(
            &#34;This SOP defines tug-assisted procedures for stages where AFT draft is below the propulsion gate.&#34;
        )
        lines.append(
            &#34;Operational philosophy: no reliance on main propulsion during incomplete immersion.&#34;
        )
        lines.append(&#34;Control authority: tug has operational control.\n&#34;)

        lines.append(&#34;## 1) Pre-RoRo Phase (T-3h to T-1h)&#34;)
        lines.append(&#34;1. Tug deployment: minimum 2 tugs (bow + stern), 1 standby&#34;)
        lines.append(
            &#34;2. Main propulsion restriction: minimum RPM/power, limited steering angle&#34;
        )
        lines.append(
            &#34;3. Abort briefing: AFT draft below 2.50 m, bearing temp alarm, loss of thrust&#34;
        )
        lines.append(&#34;&#34;)

        lines.append(&#34;## 2) RoRo / Ramp / Roll-on Phase (T0 to T+X)&#34;)
        lines.append(
            &#34;1. Tug holds heading and position; LCT propulsion is emergency backup only&#34;
        )
        lines.append(&#34;2. Mooring + tug for hang-up/trim change response&#34;)
        lines.append(
            &#34;3. Real-time monitoring: 4-corner draft, freeboard, weather, bearing temp&#34;
        )
        lines.append(&#34;&#34;)

        lines.append(&#34;## 3) Post-RoRo Phase (T+X to T+X+2h)&#34;)
        lines.append(&#34;1. Recover AFT draft &gt;= 2.70 m if possible (deballast/transfer)&#34;)
        lines.append(
            &#34;2. Restore propulsion only after safe draft or maintain tug escort&#34;
        )
        lines.append(&#34;&#34;)

        lines.append(&#34;## 4) Critical Stages Requiring Tug Assistance\n&#34;)
        for _, row in failing_stages.iterrows():
            stage_name = str(row.get(&#34;Stage&#34;, &#34;Unknown&#34;))
            aft_draft = row.get(&#34;Draft_AFT_m&#34;, &#34;N/A&#34;)
            aft_min = row.get(&#34;AFT_MIN_m&#34;, 2.70)
            freeboard_min = row.get(&#34;Freeboard_Min_m&#34;, &#34;N/A&#34;)
            lines.append(
                f&#34;- {stage_name}: AFT={aft_draft} m (req {aft_min} m), &#34;
                f&#34;Freeboard(min)={freeboard_min} m&#34;
            )

        lines.append(&#34;\n## 5) DNV-ST-N001 Compliance Checklist&#34;)
        lines.append(&#34;- Design calculations (ballast plan, stability, trim)&#34;)
        lines.append(&#34;- Operational procedures (this SOP)&#34;)
        lines.append(
            &#34;- Monitoring requirements (draft, freeboard, weather, bearing temp)&#34;
        )
        lines.append(&#34;- Load cases (critical stages documented)&#34;)
        lines.append(
            &#34;- Risk assessment and mitigation (RPM limit, steering limit, tug standby)&#34;
        )

        out_md.write_text(&#34;\n&#34;.join(lines), encoding=&#34;utf-8&#34;)
        return out_md
    except Exception as e:
        print(f&#34;[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}&#34;)
        return out_md</code></pre>
</details>
<div class="desc"><p>Generate TUG-assisted operational SOP document (DNV-ST-N001 aligned).</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.inject_current_t_from_sensor_csv"><code class="name flex">
<span>def <span class="ident">inject_current_t_from_sensor_csv</span></span>(<span>tank_ssot_csv: Path,<br>sensor_csv: Path,<br>strategy: str = 'override',<br>out_csv: Optional[Path] = None) ‑> Dict[str, object]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inject_current_t_from_sensor_csv(
    tank_ssot_csv: Path,
    sensor_csv: Path,
    strategy: str = &#34;override&#34;,
    out_csv: Optional[Path] = None,
) -&gt; Dict[str, object]:
    &#34;&#34;&#34;
    Inject Current_t values from a sensor/PLC/IoT CSV into tank_ssot_for_solver.csv.

    Supported sensor columns (case-insensitive):
    - Tank identifier: Tank | tank_id | id | tag | name
    - Value (priority order):
        1) Current_t | tons | ton | amount_t | value_t
        2) level_pct | level_percent  (uses Capacity_t)
        3) volume_m3 | m3 + optional spgr (defaults 1.0)
    &#34;&#34;&#34;
    out_csv = out_csv or tank_ssot_csv

    tank_df = pd.read_csv(tank_ssot_csv, encoding=&#34;utf-8-sig&#34;)
    sensor_df = _try_read_csv_flexible(sensor_csv)

    # Normalize sensor columns
    s_cols = {str(c).strip().lower(): c for c in sensor_df.columns}
    t_col = None
    for k in (&#34;tank&#34;, &#34;tank_id&#34;, &#34;tankid&#34;, &#34;id&#34;, &#34;tag&#34;, &#34;name&#34;):
        if k in s_cols:
            t_col = s_cols[k]
            break
    if t_col is None:
        raise ValueError(
            f&#34;Sensor CSV missing tank id column. Columns={list(sensor_df.columns)}&#34;
        )

    # Value columns
    v_col = None
    for k in (
        &#34;current_t&#34;,
        &#34;current_ton&#34;,
        &#34;current_tons&#34;,
        &#34;tons&#34;,
        &#34;ton&#34;,
        &#34;amount_t&#34;,
        &#34;value_t&#34;,
        &#34;value&#34;,
    ):
        if k in s_cols:
            v_col = s_cols[k]
            break

    level_col = None
    for k in (&#34;level_pct&#34;, &#34;level_percent&#34;, &#34;level_%&#34;):
        if k in s_cols:
            level_col = s_cols[k]
            break

    vol_col = None
    for k in (&#34;volume_m3&#34;, &#34;vol_m3&#34;, &#34;m3&#34;):
        if k in s_cols:
            vol_col = s_cols[k]
            break

    spgr_col = None
    for k in (&#34;spgr&#34;, &#34;sg&#34;, &#34;specific_gravity&#34;):
        if k in s_cols:
            spgr_col = s_cols[k]
            break

    # Build exact lookup (case-insensitive)
    sensor_df = sensor_df.copy()
    sensor_df[&#34;_tank_key&#34;] = sensor_df[t_col].astype(str).str.strip().str.upper()

    exact_map: Dict[str, float] = {}
    bad_value_rows: List[str] = []

    def _to_float(x) -&gt; Optional[float]:
        try:
            if pd.isna(x):
                return None
            return float(x)
        except Exception:
            return None

    if v_col is not None:
        for _, r in sensor_df.iterrows():
            key = str(r.get(&#34;_tank_key&#34;, &#34;&#34;)).strip()
            val = _to_float(r.get(v_col))
            if not key:
                continue
            if val is None:
                bad_value_rows.append(key)
                continue
            exact_map[key] = float(val)
        value_mode = &#34;Current_t&#34;
    elif level_col is not None:
        # We&#39;ll convert later using Capacity_t
        value_mode = &#34;level_pct&#34;
    elif vol_col is not None:
        value_mode = &#34;volume_m3&#34;
    else:
        raise ValueError(
            f&#34;Sensor CSV missing value column. Columns={list(sensor_df.columns)}&#34;
        )

    # Prepare group matching (e.g., &#39;FWB1&#39; -&gt; &#39;FWB1.P&#39;/&#39;FWB1.S&#39;)
    tank_df = tank_df.copy()
    tank_df[&#34;_tank_key&#34;] = tank_df[&#34;Tank&#34;].astype(str).str.strip().str.upper()
    tank_df[&#34;_tank_base&#34;] = (
        tank_df[&#34;Tank&#34;].astype(str).str.strip().str.upper().str.split(&#34;.&#34;).str[0]
    )

    # Group values from sensor where exact tank id doesn&#39;t exist but base exists
    group_map: Dict[str, float] = {}
    if value_mode == &#34;Current_t&#34;:
        existing_tanks = set(tank_df[&#34;_tank_key&#34;].tolist())
        for key, val in list(exact_map.items()):
            if key in existing_tanks:
                continue
            # Treat as base key if any tank base matches
            if key in set(tank_df[&#34;_tank_base&#34;].tolist()):
                group_map[key] = val
                # Keep exact_map entry as-is; apply to bases in the update loop

    # Update tank_df
    updated_exact = 0
    updated_group = 0

    # Diff-audit (sensor injection) — write diff_audit.csv next to out_csv
    audit_rows: List[Dict[str, object]] = []
    audit_csv = Path(out_csv).resolve().parent / &#34;diff_audit.csv&#34;
    audit_ts = datetime.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;)

    for i, row in tank_df.iterrows():
        tank_key = row[&#34;_tank_key&#34;]
        tank_base = row[&#34;_tank_base&#34;]

        current_old = float(row.get(&#34;Current_t&#34;, 0.0) or 0.0)

        # Audit fields
        match_type = &#34;none&#34;  # exact | group | base | none
        sensor_key_used = &#34;&#34;  # exact tank key or base key used for lookup
        sensor_raw_val = None  # raw value read from sensor csv (t / % / m3)
        sensor_raw_unit = &#34;&#34;  # &#34;t&#34; | &#34;%&#34; | &#34;m3&#34;
        sensor_spgr = None  # specific gravity for volume_m3 mode
        group_n = None  # number of members for group distribution
        computed_new_t = None  # derived tons before clamp (distributed/converted)

        if value_mode == &#34;Current_t&#34;:
            new_val = None
            if tank_key in exact_map:
                new_val = exact_map[tank_key]
                match_type = &#34;exact&#34;
                sensor_key_used = tank_key
                sensor_raw_val = float(exact_map[tank_key])
                sensor_raw_unit = &#34;t&#34;
            elif tank_base in group_map:
                # distribute equally across group members
                members = tank_df[tank_df[&#34;_tank_base&#34;] == tank_base]
                group_n = max(int(len(members)), 1)
                new_val = float(group_map[tank_base]) / float(group_n)
                match_type = &#34;group&#34;
                sensor_key_used = tank_base
                sensor_raw_val = float(group_map[tank_base])  # total on base key
                sensor_raw_unit = &#34;t&#34;
        elif value_mode == &#34;level_pct&#34;:
            # Find row in sensor_df for exact or base match
            new_val = None
            match = sensor_df[sensor_df[&#34;_tank_key&#34;] == tank_key]
            match_key_used = tank_key
            if match.empty:
                match = sensor_df[sensor_df[&#34;_tank_key&#34;] == tank_base]
                match_key_used = tank_base
            if not match.empty:
                lvl = _to_float(match.iloc[0].get(level_col))
                if lvl is not None:
                    cap = float(row.get(&#34;Capacity_t&#34;, 0.0) or 0.0)
                    new_val = cap * float(lvl) / 100.0
                    match_type = &#34;exact&#34; if match_key_used == tank_key else &#34;base&#34;
                    sensor_key_used = match_key_used
                    sensor_raw_val = float(lvl)
                    sensor_raw_unit = &#34;%&#34;
        elif value_mode == &#34;volume_m3&#34;:
            new_val = None
            match = sensor_df[sensor_df[&#34;_tank_key&#34;] == tank_key]
            match_key_used = tank_key
            if match.empty:
                match = sensor_df[sensor_df[&#34;_tank_key&#34;] == tank_base]
                match_key_used = tank_base
            if not match.empty:
                vol = _to_float(match.iloc[0].get(vol_col))
                if vol is not None:
                    sg = _to_float(match.iloc[0].get(spgr_col)) if spgr_col else 1.0
                    sg = 1.0 if sg is None else float(sg)
                    new_val = float(vol) * sg
                    match_type = &#34;exact&#34; if match_key_used == tank_key else &#34;base&#34;
                    sensor_key_used = match_key_used
                    sensor_raw_val = float(vol)
                    sensor_raw_unit = &#34;m3&#34;
                    sensor_spgr = float(sg)

        if new_val is None:
            # No match from sensor CSV for this tank (or base)
            audit_rows.append(
                {
                    &#34;TS&#34;: audit_ts,
                    &#34;SensorCSV&#34;: str(sensor_csv),
                    &#34;Strategy&#34;: strategy,
                    &#34;ValueMode&#34;: value_mode,
                    &#34;Tank&#34;: str(row.get(&#34;Tank&#34;, &#34;&#34;)),
                    &#34;TankKey&#34;: tank_key,
                    &#34;TankBase&#34;: tank_base,
                    &#34;MatchType&#34;: match_type,
                    &#34;SensorKey&#34;: sensor_key_used,
                    &#34;SensorRaw&#34;: sensor_raw_val,
                    &#34;SensorRawUnit&#34;: sensor_raw_unit,
                    &#34;SensorSpGr&#34;: sensor_spgr,
                    &#34;GroupN&#34;: group_n,
                    &#34;CurrentOld_t&#34;: round(current_old, 6),
                    &#34;ComputedNew_t&#34;: None,
                    &#34;Min_t&#34;: None,
                    &#34;Max_t&#34;: None,
                    &#34;Clamped_t&#34;: None,
                    &#34;Rounded_t&#34;: None,
                    &#34;Delta_t&#34;: None,
                    &#34;ClampedFlag&#34;: &#34;N&#34;,
                    &#34;Updated&#34;: &#34;N&#34;,
                    &#34;SkipReason&#34;: &#34;NO_MATCH&#34;,
                }
            )
            continue

        computed_new_t = float(new_val)

        # Strategy application
        if strategy == &#34;fill_missing&#34; and abs(current_old) &gt; 1e-9:
            # Keep existing value; do not overwrite
            audit_rows.append(
                {
                    &#34;TS&#34;: audit_ts,
                    &#34;SensorCSV&#34;: str(sensor_csv),
                    &#34;Strategy&#34;: strategy,
                    &#34;ValueMode&#34;: value_mode,
                    &#34;Tank&#34;: str(row.get(&#34;Tank&#34;, &#34;&#34;)),
                    &#34;TankKey&#34;: tank_key,
                    &#34;TankBase&#34;: tank_base,
                    &#34;MatchType&#34;: match_type,
                    &#34;SensorKey&#34;: sensor_key_used,
                    &#34;SensorRaw&#34;: sensor_raw_val,
                    &#34;SensorRawUnit&#34;: sensor_raw_unit,
                    &#34;SensorSpGr&#34;: sensor_spgr,
                    &#34;GroupN&#34;: group_n,
                    &#34;CurrentOld_t&#34;: round(current_old, 6),
                    &#34;ComputedNew_t&#34;: round(computed_new_t, 6),
                    &#34;Min_t&#34;: None,
                    &#34;Max_t&#34;: None,
                    &#34;Clamped_t&#34;: None,
                    &#34;Rounded_t&#34;: None,
                    &#34;Delta_t&#34;: 0.0,
                    &#34;ClampedFlag&#34;: &#34;N&#34;,
                    &#34;Updated&#34;: &#34;N&#34;,
                    &#34;SkipReason&#34;: &#34;FILL_MISSING_KEEP&#34;,
                }
            )
            continue

        # Clamp to bounds if present
        min_t = float(row.get(&#34;Min_t&#34;, 0.0) or 0.0)
        max_t = float(row.get(&#34;Max_t&#34;, row.get(&#34;Capacity_t&#34;, 0.0)) or 0.0)
        clamped = max(min_t, min(float(computed_new_t), max_t))
        rounded_val = round(clamped, 6)
        tank_df.at[i, &#34;Current_t&#34;] = rounded_val

        clamp_flag = abs(float(computed_new_t) - float(clamped)) &gt; 1e-9
        delta_t = float(rounded_val) - float(current_old)

        if tank_key in exact_map:
            updated_exact += 1
        elif tank_base in group_map:
            updated_group += 1

        audit_rows.append(
            {
                &#34;TS&#34;: audit_ts,
                &#34;SensorCSV&#34;: str(sensor_csv),
                &#34;Strategy&#34;: strategy,
                &#34;ValueMode&#34;: value_mode,
                &#34;Tank&#34;: str(row.get(&#34;Tank&#34;, &#34;&#34;)),
                &#34;TankKey&#34;: tank_key,
                &#34;TankBase&#34;: tank_base,
                &#34;MatchType&#34;: match_type,
                &#34;SensorKey&#34;: sensor_key_used,
                &#34;SensorRaw&#34;: sensor_raw_val,
                &#34;SensorRawUnit&#34;: sensor_raw_unit,
                &#34;SensorSpGr&#34;: sensor_spgr,
                &#34;GroupN&#34;: group_n,
                &#34;CurrentOld_t&#34;: round(current_old, 6),
                &#34;ComputedNew_t&#34;: round(computed_new_t, 6),
                &#34;Min_t&#34;: round(min_t, 6),
                &#34;Max_t&#34;: round(max_t, 6),
                &#34;Clamped_t&#34;: round(clamped, 6),
                &#34;Rounded_t&#34;: round(rounded_val, 6),
                &#34;Delta_t&#34;: round(delta_t, 6),
                &#34;ClampedFlag&#34;: &#34;Y&#34; if clamp_flag else &#34;N&#34;,
                &#34;Updated&#34;: &#34;Y&#34;,
                &#34;SkipReason&#34;: &#34;&#34;,
            }
        )

    # Persist
    tank_df.drop(
        columns=[c for c in (&#34;_tank_key&#34;, &#34;_tank_base&#34;) if c in tank_df.columns],
        inplace=True,
    )
    tank_df.to_csv(out_csv, index=False, encoding=&#34;utf-8-sig&#34;)

    # Write diff_audit.csv (before/after + clamp flag)
    audit_error: Optional[str] = None
    try:
        audit_df = pd.DataFrame(audit_rows)
        # Helpful ordering: updated rows first, then by abs(delta)
        if not audit_df.empty and &#34;Delta_t&#34; in audit_df.columns:
            try:
                audit_df[&#34;_abs_delta&#34;] = audit_df[&#34;Delta_t&#34;].abs()
                audit_df.sort_values(
                    by=[&#34;Updated&#34;, &#34;_abs_delta&#34;, &#34;TankKey&#34;],
                    ascending=[False, False, True],
                    inplace=True,
                )
                audit_df.drop(columns=[&#34;_abs_delta&#34;], inplace=True)
            except Exception:
                pass
        audit_df.to_csv(audit_csv, index=False, encoding=&#34;utf-8-sig&#34;)
    except Exception as e:
        audit_error = f&#34;{type(e).__name__}: {e}&#34;

    return {
        &#34;sensor_csv&#34;: str(sensor_csv),
        &#34;strategy&#34;: strategy,
        &#34;value_mode&#34;: value_mode,
        &#34;updated_exact&#34;: updated_exact,
        &#34;updated_group&#34;: updated_group,
        &#34;bad_value_rows&#34;: bad_value_rows[:50],
        &#34;diff_audit_csv&#34;: str(audit_csv),
        &#34;diff_audit_error&#34;: audit_error,
        &#34;diff_audit_rows&#34;: int(len(audit_rows)),
        &#34;tank_rows&#34;: int(len(tank_df)),
    }</code></pre>
</details>
<div class="desc"><p>Inject Current_t values from a sensor/PLC/IoT CSV into tank_ssot_for_solver.csv.</p>
<p>Supported sensor columns (case-insensitive):
- Tank identifier: Tank | tank_id | id | tag | name
- Value (priority order):
1) Current_t | tons | ton | amount_t | value_t
2) level_pct | level_percent
(uses Capacity_t)
3) volume_m3 | m3 + optional spgr (defaults 1.0)</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.load_site_profile_json"><code class="name flex">
<span>def <span class="ident">load_site_profile_json</span></span>(<span>profile_path: Path) ‑> Dict[str, object]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_site_profile_json(profile_path: Path) -&gt; Dict[str, object]:
    &#34;&#34;&#34;Load a site profile JSON file.&#34;&#34;&#34;
    obj = json.loads(profile_path.read_text(encoding=&#34;utf-8&#34;))
    if not isinstance(obj, dict):
        raise ValueError(f&#34;Profile JSON must be an object: {profile_path}&#34;)
    return obj</code></pre>
</details>
<div class="desc"><p>Load a site profile JSON file.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main() -&gt; int:
    ap = argparse.ArgumentParser(
        description=&#34;Integrated Pipeline (Definition-split aware)&#34;
    )

    ap.add_argument(
        &#34;--site&#34;,
        choices=[&#34;AGI&#34;, &#34;DAS&#34;],
        default=&#34;AGI&#34;,
        help=&#34;Site label for logging (does not change math automatically).&#34;,
    )

    ap.add_argument(
        &#34;--profile_json&#34;,
        default=&#34;&#34;,
        help=&#34;Site profile JSON (AGI/DAS). Overrides defaults unless the CLI flag is explicitly provided.&#34;,
    )
    ap.add_argument(
        &#34;--current_t_csv&#34;,
        default=&#34;&#34;,
        help=&#34;Sensor/PLC/IoT CSV for Current_t injection into Tank SSOT (optional).&#34;,
    )
    ap.add_argument(
        &#34;--current_t_strategy&#34;,
        choices=[&#34;override&#34;, &#34;fill_missing&#34;],
        default=&#34;override&#34;,
        help=&#34;How to apply sensor values to Current_t: override | fill_missing.&#34;,
    )
    ap.add_argument(
        &#34;--no_gate_report&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Disable Gate FAIL auto report generation.&#34;,
    )
    ap.add_argument(
        &#34;--debug_report&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Generate debug feasibility report after QA CSV generation.&#34;,
    )
    ap.add_argument(
        &#34;--auto_debug_report&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Automatically generate debug report (default: False).&#34;,
    )
    ap.add_argument(
        &#34;--base_dir&#34;,
        default=&#34;&#34;,
        help=&#34;Folder containing the 4 scripts (default: this script folder).&#34;,
    )
    ap.add_argument(
        &#34;--inputs_dir&#34;,
        default=&#34;&#34;,
        help=&#34;Folder containing shared JSON inputs (default: base_dir).&#34;,
    )
    ap.add_argument(
        &#34;--out_dir&#34;,
        default=&#34;&#34;,
        help=&#34;Output folder (default: base_dir/pipeline_out_&lt;timestamp&gt;).&#34;,
    )

    # Step control
    ap.add_argument(&#34;--from_step&#34;, type=int, default=1, help=&#34;Start step (0..5).&#34;)
    ap.add_argument(&#34;--to_step&#34;, type=int, default=5, help=&#34;End step (0..5).&#34;)
    ap.add_argument(
        &#34;--enable-sequence&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Enable Step 4b: Ballast Sequence &amp; Checklist (standalone modules).&#34;,
    )
    ap.add_argument(
        &#34;--enable-valve-lineup&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Enable Step 4c: Valve Lineup (requires valve_lineup_generator.py).&#34;,
    )

    # Script paths (override if needed)
    ap.add_argument(&#34;--tr_script&#34;, default=&#34;agi_tr_patched_v6_6_defsplit_v1.py&#34;)
    ap.add_argument(
        &#34;--ops_script&#34;,
        default=&#34;ops_final_r3_integrated_defs_split_v4_patched_TIDE_v1.py&#34;,
    )
    ap.add_argument(&#34;--solver_script&#34;, default=&#34;ballast_gate_solver_v4_TIDE_v1.py&#34;)
    ap.add_argument(&#34;--optimizer_script&#34;, default=&#34;Untitled-2_patched_defsplit_v1_1.py&#34;)
    ap.add_argument(
        &#34;--spmt_script&#34;,
        default=&#34;spmt v1/agi_spmt_unified.py&#34;,
        help=&#34;AGI SPMT unified script (optional Step 0).&#34;,
    )
    ap.add_argument(
        &#34;--bryan_template_script&#34;,
        default=&#34;tide/bryan_template_unified_TIDE_v1.py&#34;,
        help=&#34;Bryan template unified script (Step 5).&#34;,
    )
    ap.add_argument(
        &#34;--headers-registry&#34;,
        default=&#34;&#34;,
        help=&#34;Path to headers_registry.json for SSOT header management (optional). Auto-compiles from HEADERS_MASTER.xlsx if needed.&#34;,
    )
    ap.add_argument(
        &#34;--enable-headers-ssot&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Enable headers SSOT for all outputs (requires --headers-registry).&#34;,
    )
    ap.add_argument(
        &#34;--head-registry&#34;,
        default=&#34;&#34;,
        help=&#34;Path to HEAD_REGISTRY_AGI_v3.0.yaml for header validation (optional).&#34;,
    )
    ap.add_argument(
        &#34;--auto-head-guard&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Automatically run Head Guard validation after pipeline execution (requires --head-registry).&#34;,
    )

    # Input files (override if needed)
    ap.add_argument(&#34;--tank_catalog&#34;, default=&#34;tank_catalog_from_tankmd.json&#34;)
    ap.add_argument(
        &#34;--hydro&#34;,
        default=&#34;&#34;,
        help=&#34;Hydro table json (default: inputs_dir/bplus_inputs/Hydro_Table_Engineering.json)&#34;,
    )
    ap.add_argument(
        &#34;--stage_results&#34;,
        default=&#34;&#34;,
        help=&#34;stage_results.csv (default: generated by TR script &#39;csv&#39; mode)&#34;,
    )
    ap.add_argument(
        &#34;--spmt_config&#34;,
        default=&#34;spmt v1/spmt_shuttle_example_config_AGI_FR_M.json&#34;,
        help=&#34;SPMT config JSON for agi_spmt_unified.py (optional Step 0).&#34;,
    )

    # Gates / Limits (definition-split aligned)
    ap.add_argument(
        &#34;--fwd_max&#34;, type=float, default=2.70, help=&#34;FWD maximum draft gate (m).&#34;
    )
    ap.add_argument(
        &#34;--aft_min&#34;,
        type=float,
        default=2.70,
        help=&#34;AFT minimum draft gate (m) (Captain / prop immersion).&#34;,
    )
    ap.add_argument(
        &#34;--aft_max&#34;,
        type=float,
        default=3.50,
        help=&#34;AFT maximum draft limit (m) for optimizer (upper limit).&#34;,
    )
    ap.add_argument(
        &#34;--trim_abs_limit&#34;,
        type=float,
        default=0.50,
        help=&#34;Absolute trim limit (m) for optimizer stage mode.&#34;,
    )
    ap.add_argument(
        &#34;--trim-limit-enforced&#34;,
        dest=&#34;trim_limit_enforced&#34;,
        action=&#34;store_true&#34;,
        default=True,
        help=&#34;Enforce trim limit as hard constraint (default: enabled).&#34;,
    )
    ap.add_argument(
        &#34;--no-trim-limit-enforced&#34;,
        dest=&#34;trim_limit_enforced&#34;,
        action=&#34;store_false&#34;,
        help=&#34;Disable hard enforcement of trim limit.&#34;,
    )
    ap.add_argument(
        &#34;--freeboard-min-m&#34;,
        type=float,
        default=0.0,
        help=&#34;Minimum freeboard requirement (m).&#34;,
    )
    ap.add_argument(
        &#34;--freeboard-min-enforced&#34;,
        dest=&#34;freeboard_min_enforced&#34;,
        action=&#34;store_true&#34;,
        default=True,
        help=&#34;Enforce minimum freeboard as hard constraint (default: enabled).&#34;,
    )
    ap.add_argument(
        &#34;--no-freeboard-min-enforced&#34;,
        dest=&#34;freeboard_min_enforced&#34;,
        action=&#34;store_false&#34;,
        help=&#34;Disable hard enforcement of minimum freeboard.&#34;,
    )
    ap.add_argument(
        &#34;--stateful_solver&#34;,
        &#34;--stateful&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Enable stateful solver (carry-forward Current_t across stages).&#34;,
    )
    ap.add_argument(
        &#34;--reset_tank_state&#34;,
        default=&#34;&#34;,
        help=&#34;Comma list or regex (prefix &#39;re:&#39;) to reset Current_t from SSOT.&#34;,
    )
    ap.add_argument(
        &#34;--state_trace_csv&#34;,
        default=&#34;&#34;,
        help=&#34;Optional CSV path for stateful solver tank snapshots.&#34;,
    )
    ap.add_argument(
        &#34;--tank_operability_json&#34;,
        default=&#34;&#34;,
        help=&#34;Tank operability/profile JSON for PRE_BALLAST_ONLY enforcement.&#34;,
    )
    ap.add_argument(
        &#34;--operational_stage_regex&#34;,
        default=r&#34;(6a|critical|ramp|roll|loadout)&#34;,
        help=&#34;Regex to decide operational stages for PRE_BALLAST_ONLY enforcement.&#34;,
    )
    ap.add_argument(
        &#34;--disable_preballast_only_on_operational_stages&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Disable PRE_BALLAST_ONLY enforcement on operational stages.&#34;,
    )

    # Tide/Depth/UKC optional inputs (for solver UKC split)
    ap.add_argument(
        &#34;--forecast_tide&#34;, type=float, default=None, help=&#34;Forecast tide (m, CD).&#34;
    )
    ap.add_argument(
        &#34;--depth_ref&#34;,
        type=float,
        default=None,
        help=&#34;Reference depth (m, CD) for UKC calculation.&#34;,
    )
    ap.add_argument(
        &#34;--ukc_min&#34;, type=float, default=None, help=&#34;Minimum UKC requirement (m).&#34;
    )
    ap.add_argument(
        &#34;--squat&#34;, type=float, default=0.0, help=&#34;Squat allowance (m) for solver UKC.&#34;
    )
    ap.add_argument(
        &#34;--safety_allow&#34;,
        type=float,
        default=0.0,
        help=&#34;Additional safety allowance (m) for solver UKC.&#34;,
    )

    # Tide table simulation (optional)
    ap.add_argument(
        &#34;--tide_table&#34;,
        default=&#34;&#34;,
        help=&#34;Optional official tide table file (csv/xlsx/json). Used with --stage_schedule to assign per-stage Forecast_Tide_m by interpolation.&#34;,
    )
    ap.add_argument(
        &#34;--stage_schedule&#34;,
        default=&#34;&#34;,
        help=&#34;Optional stage schedule file (csv/xlsx) containing Stage + Timestamp/Datetime. Used with --tide_table.&#34;,
    )
    ap.add_argument(
        &#34;--stage_tide_csv&#34;,
        default=&#34;&#34;,
        help=&#34;Pre-computed stage tide CSV (Stage/StageKey + Forecast_tide_m columns). Highest priority for tide assignment. Generated by tide_stage_mapper.py.&#34;,
    )
    ap.add_argument(
        &#34;--tide_strategy&#34;,
        choices=[&#34;keep_csv&#34;, &#34;override_from_table&#34;],
        default=&#34;keep_csv&#34;,
        help=&#34;If Forecast_Tide_m already exists in stage_table, keep it or override it with tide_table interpolation.&#34;,
    )
    ap.add_argument(
        &#34;--tide_tol&#34;,
        type=float,
        default=DEFAULT_TIDE_TOL_M,
        help=&#34;Tide margin tolerance (m) for LIMIT vs OK (default 0.10m).&#34;,
    )
    ap.add_argument(
        &#34;--hmax_wave_m&#34;,
        type=float,
        default=None,
        help=&#34;Maximum wave height (m) for GL Noble Denton 0013/ND freeboard check. &#34;
        &#34;Freeboard_Req = 0.50 + 0.50*Hmax (with 4-corner monitoring) &#34;
        &#34;or 0.80 + 0.50*Hmax (without).&#34;,
    )
    ap.add_argument(
        &#34;--four_corner_monitoring&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Enable 4-corner freeboard monitoring (reduces ND freeboard requirement).&#34;,
    )
    ap.add_argument(
        &#34;--gate_guard_band_cm&#34;,
        type=float,
        default=2.0,
        help=&#34;Gate guard-band tolerance in cm (default 2.0cm). &#34;
        &#34;Applied to Gate-A (AFT_MIN_2p70) and Gate-B (FWD_MAX_2p70_critical_only). &#34;
        &#34;Production: 2.0cm, Development: 1.0cm, Strict: 0.0cm.&#34;,
    )

    # Data conversion options
    ap.add_argument(
        &#34;--pump_rate&#34;,
        type=float,
        default=100.0,
        help=&#34;Default pump rate (t/h) used when generating solver tank CSV.&#34;,
    )
    ap.add_argument(
        &#34;--tank_keywords&#34;,
        default=&#34;BALLAST,VOID,FWB,FW,DB&#34;,
        help=&#34;Comma keywords to mark use_flag=Y in tank conversion.&#34;,
    )
    ap.add_argument(
        &#34;--exclude_fwd_tanks&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Exclude FWD tanks (x_from_mid_m &lt; 0) from solver by setting use_flag=N after profile overrides.&#34;,
    )
    ap.add_argument(
        &#34;--exclude_fwd_tanks_aftmin_only&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;For AFT-min stages, set FWD tanks to DISCHARGE_ONLY (fill prohibited, discharge allowed). &#34;
        &#34;Uses stage flag Ban_FWD_Tanks for per-stage handling.&#34;,
    )

    # Execution
    ap.add_argument(
        &#34;--dry_run&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Print resolved paths and planned actions, then exit.&#34;,
    )
    args = ap.parse_args()

    script_dir = Path(__file__).parent.resolve()
    if args.base_dir:
        base_dir = Path(args.base_dir).resolve()
    else:
        base_dir = script_dir.parent if script_dir.name == &#34;tide&#34; else script_dir
    inputs_dir = Path(args.inputs_dir).resolve() if args.inputs_dir else base_dir

    # Headers SSOT support
    registry_path = None
    if args.enable_headers_ssot:
        if args.headers_registry:
            registry_path = Path(args.headers_registry)
            if not registry_path.is_absolute():
                registry_path = base_dir / registry_path
        else:
            # Auto-compile from HEADERS_MASTER.xlsx if available
            excel_master = base_dir / &#34;HEADERS_MASTER.xlsx&#34;
            json_registry = base_dir / &#34;headers_registry.json&#34;
            if excel_master.exists():
                try:
                    # Add base_dir to path for import
                    import sys

                    if str(base_dir) not in sys.path:
                        sys.path.insert(0, str(base_dir))
                    from compile_headers_registry import compile_from_excel

                    compile_needed = (
                        not json_registry.exists()
                        or excel_master.stat().st_mtime &gt; json_registry.stat().st_mtime
                    )
                    if compile_needed:
                        compile_from_excel(excel_master, json_registry)
                        print(f&#34;[OK] Headers registry compiled: {json_registry.name}&#34;)
                    registry_path = json_registry
                except Exception as e:
                    print(f&#34;[WARN] Headers registry compilation failed: {e}&#34;)
            elif json_registry.exists():
                registry_path = json_registry

    if registry_path and registry_path.exists():
        print(f&#34;[OK] Headers SSOT enabled: {registry_path.name}&#34;)
    else:
        print(f&#34;[WARN] Headers registry not found, continuing without SSOT&#34;)
        args.enable_headers_ssot = False

    # Head Guard registry support
    head_registry_path = None
    if args.auto_head_guard:
        if args.head_registry:
            head_registry_path = Path(args.head_registry)
            if not head_registry_path.is_absolute():
                head_registry_path = base_dir / head_registry_path
        else:
            default_registry = base_dir / &#34;HEAD_REGISTRY_AGI_v3.0.yaml&#34;
            if default_registry.exists():
                head_registry_path = default_registry

        if head_registry_path and head_registry_path.exists():
            print(f&#34;[OK] Head Guard registry: {head_registry_path.name}&#34;)
        else:
            print(&#34;[WARN] Head registry not found, skipping auto validation&#34;)
            args.auto_head_guard = False

    if not args.base_dir and not args.dry_run:
        migrate_tide_outputs_to_parent(script_dir, base_dir)

    if not args.dry_run:
        ensure_bplus_inputs_data_dir(base_dir)

    # Check dependencies before proceeding
    if not check_dependencies(base_dir):
        print(&#34;[ERROR] Dependency check failed. Please install required packages.&#34;)
        return 1

    # Ensure sys is available (fix for UnboundLocalError)
    import sys

    argv = sys.argv[1:]

    # ---------------------------------------------------------------------
    # Site profile (AGI/DAS): parameter-set separation (measured values)
    # - Precedence: explicit CLI flags &gt; profile JSON &gt; argparse defaults
    # ---------------------------------------------------------------------
    resolved_profile_path: Optional[Path] = resolve_site_profile_path(
        args.profile_json, base_dir=base_dir, inputs_dir=inputs_dir, site=args.site
    )
    profile_obj: Optional[Dict[str, object]] = None
    if resolved_profile_path is not None:
        try:
            profile_obj = load_site_profile_json(resolved_profile_path)
            apply_site_profile_overrides(args, profile_obj, argv)
            apply_tide_ukc_profile_fallbacks(args, profile_obj, argv)
            print(f&#34;[OK] Site profile loaded: {resolved_profile_path}&#34;)
        except Exception as e:
            print(
                f&#34;[WARN] Site profile load failed: {resolved_profile_path} ({type(e).__name__}: {e})&#34;
            )
            resolved_profile_path = None
            profile_obj = None

    # Gate-B critical-only selectors (profile-driven, CLI fwd_max overrides profile fwd max)
    cli_has_fwd_max = _argv_has_flag(&#34;--fwd_max&#34;, argv)
    gateb_fwd_max_val = _profile_last_value(profile_obj, PROFILE_GATEB_FWD_MAX_KEYS)
    gateb_fwd_max_cd = _coerce_float(gateb_fwd_max_val, None)
    if cli_has_fwd_max or gateb_fwd_max_cd is None:
        gateb_fwd_max_cd = float(args.fwd_max)

    gateb_critical_only = _profile_truthy_any(
        profile_obj, PROFILE_GATEB_CRITICAL_ONLY_KEYS
    )
    gateb_crit_list = _normalize_stage_patterns(
        _profile_last_value(profile_obj, PROFILE_CRITICAL_STAGE_KEYS)
    )
    gateb_crit_regex_val = _profile_last_value(profile_obj, PROFILE_CRITICAL_REGEX_KEYS)
    gateb_crit_regex = (
        gateb_crit_regex_val
        if isinstance(gateb_crit_regex_val, str) and gateb_crit_regex_val.strip()
        else DEFAULT_CRITICAL_STAGE_REGEX
    )

    out_dir = (
        Path(args.out_dir).resolve()
        if args.out_dir
        else (base_dir / f&#34;pipeline_out_{now_tag()}&#34;)
    )
    ensure_dir(out_dir)

    # Resolve scripts
    tr_script = resolve_script_path(base_dir, args.tr_script, label=&#34;TR&#34;)
    tr_base_dir = tr_script.parent if tr_script.exists() else base_dir
    ops_script = resolve_script_path(base_dir, args.ops_script, label=&#34;OPS&#34;)
    solver_script = resolve_script_path(base_dir, args.solver_script, label=&#34;SOLVER&#34;)
    optimizer_script = resolve_script_path(
        base_dir, args.optimizer_script, label=&#34;OPTIMIZER&#34;
    )
    spmt_script = resolve_script_path(base_dir, args.spmt_script, label=&#34;SPMT&#34;)
    bryan_template_script = resolve_script_path(
        base_dir, args.bryan_template_script, label=&#34;BRYAN_TPL&#34;
    )
    spmt_config = resolve_script_path(base_dir, args.spmt_config, label=&#34;SPMT_CFG&#34;)

    # Resolve inputs
    tank_catalog = (inputs_dir / args.tank_catalog).resolve()

    hydro_default = inputs_dir / &#34;bplus_inputs&#34; / &#34;Hydro_Table_Engineering.json&#34;
    hydro_path = Path(args.hydro).resolve() if args.hydro else hydro_default.resolve()

    # Stage results: may be generated by TR script (csv mode)
    stage_results_path = (
        Path(args.stage_results).resolve()
        if args.stage_results
        else (tr_base_dir / &#34;stage_results.csv&#34;).resolve()
    )

    # Derived SSOT CSVs for solver/optimizer
    ssot_dir = ensure_dir(out_dir / &#34;ssot&#34;)
    tank_ssot_csv = ssot_dir / &#34;tank_ssot_for_solver.csv&#34;
    hydro_ssot_csv = ssot_dir / &#34;hydro_table_for_solver.csv&#34;
    stage_table_csv = ssot_dir / &#34;stage_table_unified.csv&#34;

    if args.dry_run:
        print(&#34;=&#34; * 80)
        print(&#34;DRY RUN - PATH RESOLUTION&#34;)
        print(&#34;=&#34; * 80)
        print(f&#34;Site:        {args.site}&#34;)
        print(f&#34;Base dir:    {base_dir}&#34;)
        print(f&#34;Inputs dir:  {inputs_dir}&#34;)
        print(f&#34;Out dir:     {out_dir}&#34;)
        print(&#34;\n[SCRIPTS]&#34;)
        print(
            f&#34;TR:          {tr_script}  ({&#39;OK&#39; if tr_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;OPS:         {ops_script}  ({&#39;OK&#39; if ops_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;SOLVER:      {solver_script}  ({&#39;OK&#39; if solver_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;OPTIMIZER:   {optimizer_script}  ({&#39;OK&#39; if optimizer_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;SPMT:        {spmt_script}  ({&#39;OK&#39; if spmt_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;BRYAN_TPL:   {bryan_template_script}  ({&#39;OK&#39; if bryan_template_script.exists() else &#39;MISSING&#39;})&#34;
        )
        print(&#34;\n[INPUTS]&#34;)
        print(
            f&#34;Tank catalog:{tank_catalog}  ({&#39;OK&#39; if tank_catalog.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;Hydro:       {hydro_path}  ({&#39;OK&#39; if hydro_path.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;StageRes:    {stage_results_path}  ({&#39;OK&#39; if stage_results_path.exists() else &#39;MISSING&#39;})&#34;
        )
        print(
            f&#34;SPMT config:{spmt_config}  ({&#39;OK&#39; if spmt_config.exists() else &#39;MISSING&#39;})&#34;
        )
        print(&#34;\n[OUTPUT SSOT]&#34;)
        print(f&#34;Tank SSOT:   {tank_ssot_csv}&#34;)
        print(f&#34;Hydro SSOT:  {hydro_ssot_csv}&#34;)
        print(f&#34;Stage table: {stage_table_csv}&#34;)
        return 0

    # -------------------------------------------------------------------------
    # Step 0: AGI SPMT Unified (optional)
    # -------------------------------------------------------------------------
    spmt_out_dir = out_dir / &#34;spmt_output&#34;
    spmt_out_xlsx = out_dir / &#34;AGI_SPMT_Shuttle_Output.xlsx&#34;
    if args.from_step &lt;= 0 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 0] AGI SPMT Unified (optional)&#34;)
        print(&#34;=&#34; * 80)
        if not spmt_script.exists():
            print(f&#34;[WARN] SPMT script not found: {spmt_script}&#34;)
        elif not spmt_config.exists():
            print(f&#34;[WARN] SPMT config not found: {spmt_config}&#34;)
        else:
            ensure_dir(spmt_out_dir)
            spmt_args = [
                &#34;--config&#34;,
                str(spmt_config),
                &#34;--out_xlsx&#34;,
                str(spmt_out_xlsx),
                &#34;--out_dir&#34;,
                str(spmt_out_dir),
            ]
            r0 = step_run_script(
                0,
                &#34;SPMT_UNIFIED&#34;,
                spmt_script,
                args=spmt_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r0.ok:
                print(f&#34;[WARN] Step-0 failed (rc={r0.returncode}). Log: {r0.log}&#34;)
            else:
                print(f&#34;[OK] Step-0 completed. Log: {r0.log}&#34;)
                print(f&#34;     SPMT:   {spmt_out_xlsx}&#34;)

    # -------------------------------------------------------------------------
    # Step 1: TR Excel generation (optional but kept to maintain original flow)
    # -------------------------------------------------------------------------
    if args.from_step &lt;= 1 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 1] TR Excel Generation&#34;)
        print(&#34;=&#34; * 80)
        r1 = step_run_script(
            1,
            &#34;TR_EXCEL&#34;,
            tr_script,
            args=[],
            cwd=tr_base_dir,
            out_dir=out_dir,
        )
        if not r1.ok:
            print(f&#34;[WARN] Step-1 failed (rc={r1.returncode}). Log: {r1.log}&#34;)
            print(
                &#34;       You can still proceed if you already have stage_results.csv or do not need the Excel.&#34;
            )
        else:
            print(f&#34;[OK] Step-1 completed. Log: {r1.log}&#34;)

    # -------------------------------------------------------------------------
    # Step 1b: Generate stage_results.csv from TR script (csv mode)
    # -------------------------------------------------------------------------
    if args.from_step &lt;= 1 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 1b] Generate stage_results.csv (TR script csv mode)&#34;)
        print(&#34;=&#34; * 80)

        if not stage_results_path.exists():
            r1b = step_run_script(
                2,
                &#34;TR_STAGE_CSV&#34;,
                tr_script,
                args=[&#34;csv&#34;],
                cwd=tr_base_dir,
                out_dir=out_dir,
            )
            if not r1b.ok:
                print(
                    f&#34;[ERROR] Step-1b failed and stage_results.csv not found. rc={r1b.returncode}&#34;
                )
                print(f&#34;        Log: {r1b.log}&#34;)
                return 1
            else:
                print(f&#34;[OK] stage_results.csv generated. Log: {r1b.log}&#34;)

        if not stage_results_path.exists():
            print(f&#34;[ERROR] stage_results.csv still missing at: {stage_results_path}&#34;)
            return 1

    # -------------------------------------------------------------------------
    # Build SSOT CSVs needed for solver/optimizer
    # -------------------------------------------------------------------------
    print(&#34;\n&#34; + &#34;=&#34; * 80)
    print(&#34;[PREP] Build SSOT CSVs (Tank/Hydro/Stage)&#34;)
    print(&#34;=&#34; * 80)

    # Tank SSOT
    if not tank_catalog.exists():
        print(f&#34;[ERROR] Missing tank catalog: {tank_catalog}&#34;)
        return 1
    tank_keywords = [k.strip() for k in str(args.tank_keywords).split(&#34;,&#34;) if k.strip()]
    try:
        convert_tank_catalog_json_to_solver_csv(
            tank_catalog_json=tank_catalog,
            out_csv=tank_ssot_csv,
            pump_rate_tph=float(args.pump_rate),
            include_keywords=tank_keywords,
        )
        print(f&#34;[OK] Tank SSOT: {tank_ssot_csv}&#34;)
    except Exception as e:
        print(f&#34;[ERROR] Failed to convert tank catalog: {type(e).__name__}: {e}&#34;)
        import traceback

        traceback.print_exc()
        return 1

    # Current_t sensor sync (PLC/IoT) -&gt; inject into Tank SSOT
    sensor_stats: Optional[Dict[str, object]] = None
    resolved_sensor_csv = resolve_current_t_sensor_csv(
        args.current_t_csv, base_dir=base_dir, inputs_dir=inputs_dir
    )
    if resolved_sensor_csv is not None:
        try:
            sensor_stats = inject_current_t_from_sensor_csv(
                tank_ssot_csv=tank_ssot_csv,
                sensor_csv=resolved_sensor_csv,
                strategy=str(args.current_t_strategy),
                out_csv=tank_ssot_csv,
            )
            print(
                f&#34;[OK] Current_t injected: {resolved_sensor_csv} &#34;
                f&#34;(exact={sensor_stats.get(&#39;updated_exact&#39;)}, group={sensor_stats.get(&#39;updated_group&#39;)})&#34;
            )
        except Exception as e:
            print(
                f&#34;[WARN] Current_t injection failed: {resolved_sensor_csv} ({type(e).__name__}: {e})&#34;
            )
            sensor_stats = None
    else:
        print(
            &#34;[WARN] No Current_t sensor CSV found. Tank SSOT Current_t remains default (0.0).&#34;
        )

    # Tank overrides from SITE profile (use_flag/mode/pump_rate/etc.)
    tank_override_stats: Optional[Dict[str, object]] = None
    try:
        tank_override_stats = apply_tank_overrides_from_profile(
            tank_ssot_csv=tank_ssot_csv,
            profile=profile_obj,
            out_csv=tank_ssot_csv,
        )
        if int(tank_override_stats.get(&#34;overrides_applied&#34;, 0) or 0) &gt; 0:
            print(
                f&#34;[OK] Tank overrides applied: rules={tank_override_stats.get(&#39;overrides_applied&#39;)}, &#34;
                f&#34;touched={tank_override_stats.get(&#39;tanks_touched&#39;)}, &#34;
                f&#34;missing={len(tank_override_stats.get(&#39;missing_keys&#39;) or [])}&#34;
            )
            if int(tank_override_stats.get(&#34;base_skipped&#34;, 0) or 0) &gt; 0:
                print(
                    f&#34;[WARN] Base-match overrides skipped (explicit allow required): &#34;
                    f&#34;{tank_override_stats.get(&#39;base_skipped_keys&#39;)}&#34;
                )
            if tank_override_stats.get(&#34;missing_keys&#34;):
                print(
                    f&#34;[WARN] Tank override keys not found in SSOT: {tank_override_stats.get(&#39;missing_keys&#39;)}&#34;
                )
    except Exception as e:
        print(f&#34;[WARN] Tank override application failed ({type(e).__name__}: {e})&#34;)
        tank_override_stats = None

    # Exclude FWD tanks globally (after profile overrides)
    if args.exclude_fwd_tanks:
        try:
            df_tank = pd.read_csv(tank_ssot_csv, encoding=&#34;utf-8-sig&#34;)
            if &#34;x_from_mid_m&#34; in df_tank.columns and &#34;use_flag&#34; in df_tank.columns:
                x_col = pd.to_numeric(df_tank[&#34;x_from_mid_m&#34;], errors=&#34;coerce&#34;)
                fwd_mask = x_col &lt; 0
                fwd_count = int(fwd_mask.sum())
                if fwd_count &gt; 0:
                    before_use = (
                        df_tank.loc[fwd_mask, &#34;use_flag&#34;].value_counts().to_dict()
                    )
                    df_tank.loc[fwd_mask, &#34;use_flag&#34;] = &#34;N&#34;
                    df_tank.to_csv(tank_ssot_csv, index=False, encoding=&#34;utf-8-sig&#34;)
                    print(
                        f&#34;[OK] Excluded {fwd_count} FWD tank(s) globally (x_from_mid_m &lt; 0)&#34;
                    )
                    print(f&#34;     Before: {before_use}, After: all use_flag=N&#34;)
                else:
                    print(&#34;[INFO] No FWD tanks found (x_from_mid_m &lt; 0).&#34;)
            else:
                print(
                    &#34;[WARN] --exclude_fwd_tanks: required columns missing in Tank SSOT&#34;
                )
        except Exception as e:
            print(f&#34;[WARN] FWD tank exclusion failed ({type(e).__name__}: {e})&#34;)

    # Hydro SSOT
    if not hydro_path.exists():
        print(f&#34;[ERROR] Missing hydro input: {hydro_path}&#34;)
        print(
            &#34;       Provide --hydro or place it at inputs_dir/bplus_inputs/Hydro_Table_Engineering.json&#34;
        )
        return 1
    try:
        convert_hydro_engineering_json_to_solver_csv(
            hydro_json=hydro_path,
            out_csv=hydro_ssot_csv,
            lbp_m=LPP_M,
        )
        print(f&#34;[OK] Hydro SSOT: {hydro_ssot_csv}&#34;)
    except Exception as e:
        print(f&#34;[ERROR] Failed to convert hydro table: {type(e).__name__}: {e}&#34;)
        import traceback

        traceback.print_exc()
        return 1

    # Stage table
    if not stage_results_path.exists():
        print(f&#34;[ERROR] Missing stage_results.csv: {stage_results_path}&#34;)
        return 1

    try:
        build_stage_table_from_stage_results(
            stage_results_csv=stage_results_path,
            out_csv=stage_table_csv,
            fwd_max_m=float(gateb_fwd_max_cd),
            aft_min_m=float(args.aft_min),
            aft_max_m_for_optimizer=float(args.aft_max),
            trim_abs_limit_m=float(args.trim_abs_limit),
            critical_only=gateb_critical_only,
            critical_stage_list=gateb_crit_list,
            critical_regex=gateb_crit_regex,
            forecast_tide_m=args.forecast_tide,
            depth_ref_m=args.depth_ref,
            ukc_min_m=args.ukc_min,
            hydro_json_path=hydro_path,
        )
    except Exception as e:
        print(f&#34;[ERROR] Failed to build stage table: {type(e).__name__}: {e}&#34;)
        import traceback

        traceback.print_exc()
        return 1
    # Tide/UKC enrichment (optional, AGI)
    try:
        tide_table_path = (
            Path(args.tide_table).resolve() if str(args.tide_table).strip() else None
        )
        stage_schedule_path = (
            Path(args.stage_schedule).resolve()
            if str(args.stage_schedule).strip()
            else None
        )
        stage_tide_csv_path = resolve_stage_tide_csv(
            str(args.stage_tide_csv),
            base_dir=base_dir,
            inputs_dir=inputs_dir,
            out_dir=out_dir,
            site=args.site,
        )
        if stage_tide_csv_path:
            print(f&#34;[OK] stage_tide_csv resolved: {stage_tide_csv_path}&#34;)
        # Apply profile fallbacks if not set via CLI
        depth_ref_for_enrich = args.depth_ref
        ukc_min_for_enrich = args.ukc_min
        # Debug: Check if profile_obj is available
        if profile_obj is None:
            print(f&#34;[WARN] profile_obj is None at enrich_stage_table call site&#34;)
        else:
            print(
                f&#34;[DEBUG] profile_obj available, keys: {list(profile_obj.keys()) if isinstance(profile_obj, dict) else &#39;not a dict&#39;}&#34;
            )
        if depth_ref_for_enrich is None and profile_obj:
            tide_ukc = profile_obj.get(&#34;tide_ukc&#34;, {})
            if isinstance(tide_ukc, dict):
                depth_ref_for_enrich = tide_ukc.get(&#34;depth_ref_m&#34;)
                if depth_ref_for_enrich is not None:
                    print(
                        f&#34;[OK] Using depth_ref_m from profile: {depth_ref_for_enrich}&#34;
                    )
                else:
                    print(f&#34;[WARN] tide_ukc.depth_ref_m is None in profile&#34;)
            else:
                print(f&#34;[WARN] profile_obj.tide_ukc is not a dict: {type(tide_ukc)}&#34;)
        elif depth_ref_for_enrich is None:
            print(
                f&#34;[WARN] depth_ref_for_enrich is None (profile_obj={profile_obj is not None})&#34;
            )
        if ukc_min_for_enrich is None and profile_obj:
            tide_ukc = profile_obj.get(&#34;tide_ukc&#34;, {})
            if isinstance(tide_ukc, dict):
                ukc_min_for_enrich = tide_ukc.get(&#34;ukc_min_m&#34;)
                if ukc_min_for_enrich is not None:
                    print(f&#34;[OK] Using ukc_min_m from profile: {ukc_min_for_enrich}&#34;)
                else:
                    print(f&#34;[WARN] tide_ukc.ukc_min_m is None in profile&#34;)
            else:
                print(f&#34;[WARN] profile_obj.tide_ukc is not a dict: {type(tide_ukc)}&#34;)
        elif ukc_min_for_enrich is None:
            print(
                f&#34;[WARN] ukc_min_for_enrich is None (profile_obj={profile_obj is not None})&#34;
            )

        enrich_stage_table_with_tide_ukc(
            stage_table_csv=stage_table_csv,
            forecast_tide_m=args.forecast_tide,
            depth_ref_m=depth_ref_for_enrich,
            ukc_min_m=ukc_min_for_enrich,
            squat_m=float(args.squat),
            safety_allow_m=float(args.safety_allow),
            tide_tol_m=(
                float(args.tide_tol)
                if args.tide_tol is not None
                else DEFAULT_TIDE_TOL_M
            ),
            stage_tide_csv_path=stage_tide_csv_path,
            tide_table_path=tide_table_path,
            stage_schedule_path=stage_schedule_path,
            tide_strategy=str(args.tide_strategy),
        )
        if tide_table_path and stage_schedule_path:
            print(
                f&#34;[OK] Tide table applied to stages: {tide_table_path.name} (schedule={stage_schedule_path.name})&#34;
            )
    except Exception as e:
        print(
            f&#34;[WARN] Stage table Tide/UKC enrichment skipped: {type(e).__name__}: {e}&#34;
        )

    print(f&#34;[OK] Stage table: {stage_table_csv}&#34;)

    # AFT-min-only FWD tank handling (stage-level SSOT selection)
    if args.exclude_fwd_tanks_aftmin_only:
        try:
            df_stage = pd.read_csv(stage_table_csv, encoding=&#34;utf-8-sig&#34;)
            if &#34;Current_AFT_m&#34; in df_stage.columns and &#34;AFT_MIN_m&#34; in df_stage.columns:
                tol = 1e-6
                df_stage[&#34;Ban_FWD_Tanks&#34;] = (
                    pd.to_numeric(df_stage[&#34;Current_AFT_m&#34;], errors=&#34;coerce&#34;)
                    &lt; pd.to_numeric(df_stage[&#34;AFT_MIN_m&#34;], errors=&#34;coerce&#34;) - tol
                )
                ban_count = int(df_stage[&#34;Ban_FWD_Tanks&#34;].fillna(False).sum())

                # Default: all stages use global SSOT
                df_stage[&#34;Tank_SSOT_CSV&#34;] = str(tank_ssot_csv.name)

                if ban_count &gt; 0:
                    try:
                        df_tank = pd.read_csv(tank_ssot_csv, encoding=&#34;utf-8-sig&#34;)
                        if (
                            &#34;x_from_mid_m&#34; in df_tank.columns
                            and &#34;mode&#34; in df_tank.columns
                        ):
                            x_col = pd.to_numeric(
                                df_tank[&#34;x_from_mid_m&#34;], errors=&#34;coerce&#34;
                            )
                            fwd_mask = x_col &lt; 0
                            fwd_count = int(fwd_mask.sum())
                            if fwd_count &gt; 0:
                                df_tank_aftmin = df_tank.copy()
                                df_tank_aftmin.loc[fwd_mask, &#34;use_flag&#34;] = &#34;Y&#34;
                                df_tank_aftmin.loc[fwd_mask, &#34;mode&#34;] = &#34;DISCHARGE_ONLY&#34;
                                if (
                                    &#34;Current_t&#34; in df_tank_aftmin.columns
                                    and &#34;Max_t&#34; in df_tank_aftmin.columns
                                ):
                                    current_t_col = pd.to_numeric(
                                        df_tank_aftmin.loc[fwd_mask, &#34;Current_t&#34;],
                                        errors=&#34;coerce&#34;,
                                    )
                                    df_tank_aftmin.loc[fwd_mask, &#34;Max_t&#34;] = (
                                        current_t_col
                                    )
                                if &#34;Min_t&#34; in df_tank_aftmin.columns:
                                    df_tank_aftmin.loc[fwd_mask, &#34;Min_t&#34;] = 0.0

                                tank_ssot_csv_aftmin = (
                                    ssot_dir / &#34;tank_ssot_for_solver__aftmin.csv&#34;
                                )
                                df_tank_aftmin.to_csv(
                                    tank_ssot_csv_aftmin,
                                    index=False,
                                    encoding=&#34;utf-8-sig&#34;,
                                )
                                print(
                                    f&#34;[OK] AFT-min stage SSOT created: {tank_ssot_csv_aftmin.name}&#34;
                                )
                                print(
                                    f&#34;     {fwd_count} FWD tank(s) set to DISCHARGE_ONLY (use_flag=Y, Min_t=0.0, Max_t=Current_t)&#34;
                                )
                                print(
                                    f&#34;     Global SSOT unchanged: {tank_ssot_csv.name}&#34;
                                )

                                df_stage.loc[
                                    df_stage[&#34;Ban_FWD_Tanks&#34;] == True,
                                    &#34;Tank_SSOT_CSV&#34;,
                                ] = str(tank_ssot_csv_aftmin.name)
                                print(
                                    f&#34;[OK] Stage-level Tank SSOT selector: {ban_count} AFT-min stage(s) -&gt; {tank_ssot_csv_aftmin.name}&#34;
                                )
                            else:
                                print(&#34;[INFO] No FWD tanks found (x_from_mid_m &lt; 0).&#34;)
                        else:
                            print(
                                &#34;[WARN] AFT-min handling skipped (x_from_mid_m/mode missing in Tank SSOT)&#34;
                            )
                    except Exception as e:
                        print(
                            f&#34;[WARN] AFT-min FWD handling failed: {type(e).__name__}: {e}&#34;
                        )
                df_stage.to_csv(stage_table_csv, index=False, encoding=&#34;utf-8-sig&#34;)
                print(
                    f&#34;[OK] AFT-min stage filter enabled: Ban_FWD_Tanks={ban_count} stage(s)&#34;
                )
            else:
                print(&#34;[WARN] AFT-min filter skipped (Current_AFT_m/AFT_MIN_m missing)&#34;)
        except Exception as e:
            print(f&#34;[WARN] AFT-min filter update failed: {type(e).__name__}: {e}&#34;)

    # Generate QA CSV (definition-split enforcement)
    stage_qa_csv = ssot_dir / &#34;pipeline_stage_QA.csv&#34;
    solver_summary_for_qa = None
    if args.from_step &lt;= 3 &lt;= args.to_step:
        candidate = out_dir / &#34;solver_ballast_summary.csv&#34;
        if candidate.exists():
            solver_summary_for_qa = candidate
    # Gate guard-band (P0-3): convert cm to m
    gate_guard_band_m = float(args.gate_guard_band_cm) / 100.0
    generate_stage_QA_csv(
        stage_table_csv=stage_table_csv,
        out_qa_csv=stage_qa_csv,
        fwd_max_m=float(gateb_fwd_max_cd),
        aft_min_m=float(args.aft_min),
        d_vessel_m=float(D_VESSEL_M),
        forecast_tide_m=args.forecast_tide,
        depth_ref_m=args.depth_ref,
        ukc_min_m=args.ukc_min,
        critical_only=gateb_critical_only,
        critical_stage_list=gateb_crit_list,
        critical_regex=gateb_crit_regex,
        gateb_fwd_max_m_cd=float(gateb_fwd_max_cd),
        squat_m=float(args.squat),
        safety_allow_m=float(args.safety_allow),
        hydro_json_path=hydro_path,
        strict_hardstop=False,  # Generate QA CSV but don&#39;t stop on HardStop violations
        solver_summary_csv=solver_summary_for_qa,
        hmax_wave_m=args.hmax_wave_m,
        four_corner_monitoring=args.four_corner_monitoring,
        tol_m=gate_guard_band_m,
        tide_tol_m=(
            float(args.tide_tol) if args.tide_tol is not None else DEFAULT_TIDE_TOL_M
        ),
        trim_abs_limit_m=float(args.trim_abs_limit),
        trim_limit_enforced=bool(args.trim_limit_enforced),
        freeboard_min_m=float(args.freeboard_min_m),
        freeboard_min_enforced=bool(args.freeboard_min_enforced),
    )
    print(f&#34;[OK] Stage QA CSV (definition-split): {stage_qa_csv}&#34;)

    # -------------------------------------------------------------------------
    # Tidying: pipeline_stage_QA.csv
    # -------------------------------------------------------------------------
    if stage_qa_csv.exists():
        registry_path_tidying = (
            registry_path if args.enable_headers_ssot and registry_path else None
        )
        run_tidying_for_csv(
            file_path=stage_qa_csv,
            base_dir=base_dir,
            registry_path=registry_path_tidying,
            deliverable_id=&#34;PIPELINE_STAGE_QA_CSV&#34;,
            verbose=True,
        )

    # Gate FAIL root-cause report (auto)
    gate_report_md = out_dir / &#34;gate_fail_report.md&#34;
    if not args.no_gate_report:
        try:
            ukc_inputs = {
                &#34;forecast_tide_m&#34;: args.forecast_tide,
                &#34;depth_ref_m&#34;: args.depth_ref,
                &#34;ukc_min_m&#34;: args.ukc_min,
                &#34;squat_m&#34;: float(args.squat),
                &#34;safety_allow_m&#34;: float(args.safety_allow),
            }
            generate_gate_fail_report_md(
                out_md=gate_report_md,
                site=str(args.site),
                profile_path=resolved_profile_path,
                stage_qa_csv=stage_qa_csv,
                tank_ssot_csv=tank_ssot_csv,
                sensor_stats=sensor_stats,
                ukc_inputs=ukc_inputs,
            )
            print(f&#34;[OK] Gate FAIL report: {gate_report_md}&#34;)
            try:
                append_dnv_mitigation_section_to_gate_report(
                    report_md=gate_report_md,
                    stage_qa_csv=stage_qa_csv,
                    propeller_diameter_m=1.38,
                )
                print(&#34;[OK] DNV mitigation section appended to gate report&#34;)
            except Exception as e:
                print(
                    f&#34;[WARN] Failed to append DNV mitigation: {type(e).__name__}: {e}&#34;
                )
            try:
                tug_sop_md = out_dir / &#34;TUG_Operational_SOP_DNV_ST_N001.md&#34;
                generate_tug_operational_sop_md(
                    out_md=tug_sop_md,
                    stage_qa_csv=stage_qa_csv,
                    site=str(args.site),
                )
                print(f&#34;[OK] TUG Operational SOP: {tug_sop_md}&#34;)
            except Exception as e:
                print(f&#34;[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}&#34;)
        except Exception as e:
            print(f&#34;[WARN] Gate FAIL report generation failed: {type(e).__name__}: {e}&#34;)

    if args.debug_report or args.auto_debug_report:
        debug_report_step(
            out_dir=out_dir,
            stage_qa_csv=stage_qa_csv,
            hydro_json_path=hydro_path,
        )

    # -------------------------------------------------------------------------
    # Step 2: OPS integrated report (Excel + MD)
    # -------------------------------------------------------------------------
    if args.from_step &lt;= 2 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 2] OPS-FINAL-R3 Integrated (Excel + Markdown)&#34;)
        print(&#34;=&#34; * 80)
        r2 = step_run_script(
            3,
            &#34;OPS_INTEGRATED&#34;,
            ops_script,
            args=[
                &#34;--fwd_max&#34;,
                str(gateb_fwd_max_cd),
                &#34;--aft_min&#34;,
                str(args.aft_min),
            ],
            cwd=base_dir,
            out_dir=out_dir,
        )
        if not r2.ok:
            print(f&#34;[ERROR] Step-2 failed (rc={r2.returncode}). Log: {r2.log}&#34;)
            return 1
        else:
            print(f&#34;[OK] Step-2 completed. Log: {r2.log}&#34;)

        # Collect outputs if present (script writes in CWD)
        expected_ops = [
            base_dir / &#34;OPS_FINAL_R3_AGI_Ballast_Integrated.xlsx&#34;,
            base_dir / &#34;OPS_FINAL_R3_Report_Integrated.md&#34;,
        ]
        for p in expected_ops:
            if p.exists():
                shutil.copy2(p, out_dir / p.name)
                print(f&#34;[OK] Collected output -&gt; {out_dir / p.name}&#34;)

    # -------------------------------------------------------------------------
    # Step 3: Gate solver (LP) - unified gates (FWD max + AFT min + FB + UKC)
    # -------------------------------------------------------------------------
    solver_out_plan = out_dir / &#34;solver_ballast_plan.csv&#34;
    solver_out_sum = out_dir / &#34;solver_ballast_summary.csv&#34;
    solver_out_stage_plan = out_dir / &#34;solver_ballast_stage_plan.csv&#34;

    opt_out_plan = out_dir / &#34;optimizer_plan.csv&#34;
    opt_out_sum = out_dir / &#34;optimizer_summary.csv&#34;
    opt_out_bwrb = out_dir / &#34;optimizer_bwrb_log.csv&#34;
    opt_out_tanklog = out_dir / &#34;optimizer_tank_log.csv&#34;
    opt_excel = out_dir / &#34;optimizer_ballast_plan.xlsx&#34;
    opt_out_stage_plan = out_dir / &#34;ballast_stage_plan.csv&#34;

    if args.from_step &lt;= 3 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 3] Ballast Gate Solver (LP, definition-split + unified gates)&#34;)
        print(&#34;=&#34; * 80)

        # 파일 존재 확인 (필수 입력 파일)
        missing_files = []
        if not stage_table_csv.exists():
            missing_files.append(f&#34;stage_table_csv: {stage_table_csv}&#34;)
        if not tank_ssot_csv.exists():
            missing_files.append(f&#34;tank_ssot_csv: {tank_ssot_csv}&#34;)
        if not hydro_ssot_csv.exists():
            missing_files.append(f&#34;hydro_ssot_csv: {hydro_ssot_csv}&#34;)

        if missing_files:
            print(f&#34;[ERROR] Step-3 cannot proceed: Missing required files:&#34;)
            for f in missing_files:
                print(f&#34;  - {f}&#34;)
            return 1

        try:
            fwd_max_arg = &#34;nan&#34; if gateb_critical_only else str(gateb_fwd_max_cd)
            solver_args = [
                &#34;--tank&#34;,
                str(tank_ssot_csv),
                &#34;--hydro&#34;,
                str(hydro_ssot_csv),
                &#34;--mode&#34;,
                &#34;limit&#34;,
                &#34;--stage&#34;,
                str(stage_table_csv),
                &#34;--iterate_hydro&#34;,
                &#34;2&#34;,
                &#34;--out_plan&#34;,
                str(solver_out_plan),
                &#34;--out_summary&#34;,
                str(solver_out_sum),
                &#34;--out_stage_plan&#34;,
                str(solver_out_stage_plan),
                &#34;--fwd_max&#34;,
                fwd_max_arg,
                &#34;--aft_min&#34;,
                str(args.aft_min),
                &#34;--d_vessel&#34;,
                str(D_VESSEL_M),
                &#34;--fb_min&#34;,
                str(args.freeboard_min_m),
                &#34;--squat&#34;,
                str(args.squat),
                &#34;--safety_allow&#34;,
                str(args.safety_allow),
                &#34;--trim_abs_limit&#34;,
                str(args.trim_abs_limit),
                &#34;--freeboard_min_m&#34;,
                str(args.freeboard_min_m),
            ]
            if args.trim_limit_enforced:
                solver_args.append(&#34;--trim_limit_enforced&#34;)
            else:
                solver_args.append(&#34;--no-trim-limit-enforced&#34;)
            if args.freeboard_min_enforced:
                solver_args.append(&#34;--freeboard_min_enforced&#34;)
            else:
                solver_args.append(&#34;--no-freeboard-min-enforced&#34;)
            if args.forecast_tide is not None:
                solver_args += [&#34;--forecast_tide&#34;, str(args.forecast_tide)]
            if args.depth_ref is not None:
                solver_args += [&#34;--depth_ref&#34;, str(args.depth_ref)]
            if args.ukc_min is not None:
                solver_args += [&#34;--ukc_min&#34;, str(args.ukc_min)]
            if args.stateful_solver:
                solver_args.append(&#34;--stateful&#34;)
            if str(args.reset_tank_state).strip():
                solver_args += [
                    &#34;--reset_tank_state&#34;,
                    str(args.reset_tank_state).strip(),
                ]
            oper_json = str(args.tank_operability_json).strip()
            if (
                not oper_json
                and resolved_profile_path
                and Path(resolved_profile_path).exists()
            ):
                oper_json = str(resolved_profile_path)
            if oper_json:
                solver_args += [&#34;--tank_operability_json&#34;, oper_json]
            if str(args.operational_stage_regex).strip():
                solver_args += [
                    &#34;--operational_stage_regex&#34;,
                    str(args.operational_stage_regex).strip(),
                ]
            if args.disable_preballast_only_on_operational_stages:
                solver_args.append(&#34;--disable_preballast_only_on_operational_stages&#34;)
            if args.stateful_solver:
                trace_path = str(args.state_trace_csv).strip()
                if not trace_path:
                    trace_path = str(out_dir / &#34;solver_state_trace.csv&#34;)
                solver_args += [&#34;--state_trace_csv&#34;, trace_path]

            r3 = step_run_script(
                4,
                &#34;SOLVER_LP&#34;,
                solver_script,
                args=solver_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r3.ok:
                print(f&#34;[ERROR] Step-3 failed (rc={r3.returncode}). Log: {r3.log}&#34;)
                return 1
            else:
                print(f&#34;[OK] Step-3 completed. Log: {r3.log}&#34;)
                print(f&#34;     Plan:    {solver_out_plan}&#34;)
                print(f&#34;     Summary: {solver_out_sum}&#34;)

                # -------------------------------------------------------------------------
                # Tidying: Step 3 Solver Outputs
                # -------------------------------------------------------------------------
                registry_path_tidying = (
                    registry_path
                    if args.enable_headers_ssot and registry_path
                    else None
                )
                if solver_out_plan.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_plan,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id=&#34;SOLVER_BALLAST_PLAN_CSV&#34;,
                        verbose=True,
                    )
                if solver_out_sum.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_sum,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id=&#34;SOLVER_BALLAST_SUMMARY_CSV&#34;,
                        verbose=True,
                    )
                if solver_out_stage_plan.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_stage_plan,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id=&#34;SOLVER_BALLAST_STAGE_PLAN_CSV&#34;,
                        verbose=True,
                    )

                # Append solver outputs to Gate FAIL report (if enabled)
                if (not args.no_gate_report) and &#34;gate_report_md&#34; in locals():
                    try:
                        append_solver_section_to_gate_report(
                            report_md=gate_report_md,
                            solver_out_summary=solver_out_sum,
                            solver_out_plan=solver_out_plan,
                            solver_out_stage_plan=solver_out_stage_plan,
                        )
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to append solver section to gate report: {type(e).__name__}: {e}&#34;
                        )
                # Regenerate QA CSV with solver post-solve drafts
                try:
                    # Gate guard-band (P0-3): convert cm to m
                    gate_guard_band_m = float(args.gate_guard_band_cm) / 100.0
                    generate_stage_QA_csv(
                        stage_table_csv=stage_table_csv,
                        out_qa_csv=stage_qa_csv,
                        fwd_max_m=float(gateb_fwd_max_cd),
                        aft_min_m=float(args.aft_min),
                        d_vessel_m=float(D_VESSEL_M),
                        forecast_tide_m=args.forecast_tide,
                        depth_ref_m=args.depth_ref,
                        ukc_min_m=args.ukc_min,
                        critical_only=gateb_critical_only,
                        critical_stage_list=gateb_crit_list,
                        critical_regex=gateb_crit_regex,
                        gateb_fwd_max_m_cd=float(gateb_fwd_max_cd),
                        squat_m=float(args.squat),
                        safety_allow_m=float(args.safety_allow),
                        hydro_json_path=hydro_path,
                        strict_hardstop=False,
                        solver_summary_csv=solver_out_sum,
                        hmax_wave_m=args.hmax_wave_m,
                        four_corner_monitoring=args.four_corner_monitoring,
                        tol_m=gate_guard_band_m,
                        trim_abs_limit_m=float(args.trim_abs_limit),
                        trim_limit_enforced=bool(args.trim_limit_enforced),
                        freeboard_min_m=float(args.freeboard_min_m),
                        freeboard_min_enforced=bool(args.freeboard_min_enforced),
                    )
                    print(
                        f&#34;[OK] Stage QA CSV updated with solver results: {stage_qa_csv}&#34;
                    )
                except Exception as e:
                    print(
                        f&#34;[WARN] Failed to update QA with solver results: {type(e).__name__}: {e}&#34;
                    )
        except Exception as e:
            print(f&#34;[ERROR] Step-3 exception: {type(e).__name__}: {e}&#34;)
            import traceback

            traceback.print_exc()
            return 1

    # -------------------------------------------------------------------------
    # Step 4: Optimizer (optional) - note: AFT_MIN is NOT a native constraint here
    # -------------------------------------------------------------------------
    if args.from_step &lt;= 4 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 4] Ballast Optimizer Integrated (optional)&#34;)
        print(&#34;=&#34; * 80)
        print(&#34;[NOTE] This optimizer uses AFT_Limit_m as an upper limit (MAX).&#34;)
        print(
            &#34;       If you need Captain gate AFT &gt;= 2.70m, enforce it via Step-3 solver (AFT_MIN).&#34;
        )

        opt_args = [
            &#34;--batch&#34;,
            &#34;--tank&#34;,
            str(tank_ssot_csv),
            &#34;--hydro&#34;,
            str(hydro_ssot_csv),
            &#34;--stage&#34;,
            str(stage_table_csv),
            &#34;--prefer_time&#34;,
            &#34;--iterate_hydro&#34;,
            &#34;2&#34;,
            &#34;--out_plan&#34;,
            str(opt_out_plan),
            &#34;--out_summary&#34;,
            str(opt_out_sum),
            &#34;--out_stage_plan&#34;,
            str(opt_out_stage_plan),
            &#34;--bwrb_out&#34;,
            str(opt_out_bwrb),
            &#34;--tanklog_out&#34;,
            str(opt_out_tanklog),
            &#34;--excel_out&#34;,
            str(opt_excel),
        ]

        r4 = step_run_script(
            5,
            &#34;OPTIMIZER&#34;,
            optimizer_script,
            args=opt_args,
            cwd=base_dir,
            out_dir=out_dir,
        )
        if not r4.ok:
            print(f&#34;[ERROR] Step-4 failed (rc={r4.returncode}). Log: {r4.log}&#34;)
            return 1
        else:
            print(f&#34;[OK] Step-4 completed. Log: {r4.log}&#34;)
            print(f&#34;     Excel: {opt_excel}&#34;)

    # -------------------------------------------------------------------------
    # Step 4b: Ballast Sequence &amp; Checklist Generation (optional)
    # -------------------------------------------------------------------------
    if args.enable_sequence and args.from_step &lt;= 4 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 4b] Ballast Sequence &amp; Checklist Generation&#34;)
        print(&#34;=&#34; * 80)

        try:
            seq_gen_script = resolve_script_path(
                base_dir, &#34;ballast_sequence_generator.py&#34;, label=&#34;SEQUENCE_GEN&#34;
            )
            checklist_gen_script = resolve_script_path(
                base_dir, &#34;checklist_generator.py&#34;, label=&#34;CHECKLIST_GEN&#34;
            )

            if not seq_gen_script.exists():
                print(f&#34;[WARN] Sequence generator not found: {seq_gen_script}&#34;)
                print(&#34;[INFO] Step 4b skipped. Run standalone scripts if needed.&#34;)
            else:
                import pandas as pd

                ballast_plan_path = None
                if opt_out_plan.exists():
                    opt_df = pd.read_csv(opt_out_plan, encoding=&#34;utf-8-sig&#34;)
                    if &#34;Stage&#34; in opt_df.columns:
                        stages_in_opt = set(opt_df[&#34;Stage&#34;].astype(str).unique())
                        required_stages = {
                            &#34;Stage 5_PreBallast&#34;,
                            &#34;Stage 6A_Critical (Opt C)&#34;,
                            &#34;Stage 6B Tide Window&#34;,
                        }
                        if (
                            not required_stages.issubset(stages_in_opt)
                            and solver_out_plan.exists()
                        ):
                            solver_df = pd.read_csv(
                                solver_out_plan, encoding=&#34;utf-8-sig&#34;
                            )
                            missing_stages = required_stages - stages_in_opt
                            for stage in missing_stages:
                                stage_rows = solver_df[
                                    solver_df[&#34;Stage&#34;].astype(str) == stage
                                ]
                                if not stage_rows.empty:
                                    opt_df = pd.concat(
                                        [opt_df, stage_rows], ignore_index=True
                                    )
                            temp_plan = out_dir / &#34;ballast_plan_merged.csv&#34;
                            opt_df.to_csv(temp_plan, index=False, encoding=&#34;utf-8-sig&#34;)
                            ballast_plan_path = temp_plan
                            print(
                                f&#34;[INFO] Using optimizer plan (merged with solver): {temp_plan.name}&#34;
                            )
                        else:
                            ballast_plan_path = opt_out_plan
                            print(f&#34;[INFO] Using optimizer plan: {opt_out_plan.name}&#34;)
                    else:
                        ballast_plan_path = opt_out_plan
                        print(f&#34;[INFO] Using optimizer plan: {opt_out_plan.name}&#34;)
                elif solver_out_plan.exists():
                    ballast_plan_path = solver_out_plan
                    print(f&#34;[INFO] Using solver plan: {solver_out_plan.name}&#34;)
                else:
                    print(
                        &#34;[WARN] No ballast plan found (optimizer_plan.csv or solver_ballast_plan.csv).&#34;
                    )

                if ballast_plan_path is not None:
                    ballast_plan_df = pd.read_csv(
                        ballast_plan_path, encoding=&#34;utf-8-sig&#34;
                    )
                    # Ensure Stage column exists and fill empty values
                    if &#34;Stage&#34; not in ballast_plan_df.columns:
                        print(&#34;[WARN] ballast_plan_df missing Stage column&#34;)
                        if &#34;Stage_ID&#34; in ballast_plan_df.columns:
                            ballast_plan_df[&#34;Stage&#34;] = ballast_plan_df[&#34;Stage_ID&#34;]
                        else:
                            ballast_plan_df[&#34;Stage&#34;] = &#34;&#34;
                    # Fill empty Stage values
                    empty_stage_mask = ballast_plan_df[&#34;Stage&#34;].isna() | (
                        ballast_plan_df[&#34;Stage&#34;].astype(str).str.strip() == &#34;&#34;
                    )
                    if empty_stage_mask.any():
                        # Forward fill from previous non-empty Stage
                        ballast_plan_df[&#34;Stage&#34;] = (
                            ballast_plan_df[&#34;Stage&#34;].fillna(method=&#34;ffill&#34;).fillna(&#34;&#34;)
                        )
                        # Replace remaining empty strings
                        ballast_plan_df.loc[
                            ballast_plan_df[&#34;Stage&#34;].astype(str).str.strip() == &#34;&#34;,
                            &#34;Stage&#34;,
                        ] = &#34;&#34;
                        if empty_stage_mask.sum() &gt; 0:
                            print(
                                f&#34;[WARN] Found {empty_stage_mask.sum()} rows with empty Stage, attempted to fill&#34;
                            )

                    fallback_plan_df = None
                    if solver_out_plan.exists():
                        try:
                            if (
                                Path(ballast_plan_path).resolve()
                                != solver_out_plan.resolve()
                            ):
                                fallback_plan_df = pd.read_csv(
                                    solver_out_plan, encoding=&#34;utf-8-sig&#34;
                                )
                        except Exception as e:
                            print(
                                &#34;[WARN] Failed to load solver fallback plan: &#34;
                                f&#34;{type(e).__name__}: {e}&#34;
                            )

                    stage_drafts = {}
                    if stage_qa_csv.exists():
                        qa_df = pd.read_csv(stage_qa_csv, encoding=&#34;utf-8-sig&#34;)
                        for _, row in qa_df.iterrows():
                            ukc_val = row.get(&#34;UKC_min_actual_m&#34;)
                            if ukc_val is None:
                                ukc_val = row.get(&#34;UKC_min_m&#34;)
                            if ukc_val is None:
                                ukc_val = row.get(&#34;UKC_m&#34;)
                            stage_name = str(row.get(&#34;Stage&#34;, &#34;&#34;)).strip()
                            if not stage_name:
                                continue
                            stage_drafts[stage_name] = {
                                &#34;fwd&#34;: row.get(&#34;Draft_FWD_m&#34;, 0.0),
                                &#34;aft&#34;: row.get(&#34;Draft_AFT_m&#34;, 0.0),
                                &#34;trim&#34;: (
                                    (
                                        row.get(&#34;Input_Trim_cm&#34;, 0.0)
                                        or row.get(&#34;Trim_cm&#34;, 0.0)
                                    )
                                    / 100.0
                                ),
                                &#34;ukc&#34;: ukc_val if ukc_val is not None else 0.0,
                            }

                    tank_catalog_df = None
                    if tank_catalog.exists():
                        import json

                        tank_data = json.loads(tank_catalog.read_text(encoding=&#34;utf-8&#34;))
                        if isinstance(tank_data, dict) and &#34;tanks&#34; in tank_data:
                            tank_catalog_df = pd.DataFrame(tank_data[&#34;tanks&#34;])

                    # 수정: Tank SSOT의 초기 Current_t를 tank_catalog_df에 병합
                    # 이렇게 하면 Sequence generator가 Solver와 동일한 초기 상태를 사용함
                    if tank_ssot_csv.exists() and tank_catalog_df is not None:
                        try:
                            tank_ssot_df = pd.read_csv(
                                tank_ssot_csv, encoding=&#34;utf-8-sig&#34;
                            )
                            if (
                                &#34;Tank&#34; in tank_ssot_df.columns
                                and &#34;Current_t&#34; in tank_ssot_df.columns
                            ):
                                # tank_catalog_df에 Current_t 컬럼이 없으면 추가
                                if &#34;Current_t&#34; not in tank_catalog_df.columns:
                                    tank_catalog_df[&#34;Current_t&#34;] = 0.0

                                # Tank SSOT의 Current_t로 업데이트 (Tank 이름 매칭)
                                # tank_catalog_df의 &#34;id&#34; 컬럼과 tank_ssot_df의 &#34;Tank&#34; 컬럼 매칭
                                if &#34;id&#34; in tank_catalog_df.columns:
                                    for _, ssot_row in tank_ssot_df.iterrows():
                                        tank_name = str(ssot_row[&#34;Tank&#34;]).strip()
                                        current_t = float(
                                            ssot_row.get(&#34;Current_t&#34;, 0.0)
                                        )

                                        # 정확한 매칭 또는 base 매칭 (예: &#34;FWB2.S&#34; 또는 &#34;FWB2&#34;)
                                        mask = (
                                            tank_catalog_df[&#34;id&#34;].str.strip()
                                            == tank_name
                                        )
                                        if not mask.any():
                                            # Base 매칭 시도 (예: &#34;FWB2.S&#34; -&gt; &#34;FWB2&#34;)
                                            base_name = (
                                                tank_name.split(&#34;.&#34;, 1)[0]
                                                if &#34;.&#34; in tank_name
                                                else tank_name
                                            )
                                            mask = (
                                                tank_catalog_df[&#34;id&#34;]
                                                .str.strip()
                                                .str.startswith(base_name)
                                            )

                                        if mask.any():
                                            tank_catalog_df.loc[mask, &#34;Current_t&#34;] = (
                                                current_t
                                            )

                                print(
                                    f&#34;[OK] Tank SSOT Current_t 병합: {tank_ssot_csv.name} → tank_catalog_df&#34;
                                )
                        except Exception as e:
                            print(
                                f&#34;[WARN] Tank SSOT Current_t 병합 실패: {type(e).__name__}: {e}&#34;
                            )

                    import sys as _sys
                    from pathlib import Path as _Path

                    _base_dir = (
                        _Path(__file__).parent.parent
                        if _Path(__file__).parent.name == &#34;tide&#34;
                        else _Path(__file__).parent
                    )
                    if str(_base_dir) not in _sys.path:
                        _sys.path.insert(0, str(_base_dir))

                    profile_for_seq = None
                    try:
                        from ssot.gates_loader import SiteProfile, load_agi_profile

                        if resolved_profile_path and resolved_profile_path.exists():
                            profile_for_seq = SiteProfile(str(resolved_profile_path))
                        else:
                            profile_for_seq = load_agi_profile()
                    except Exception as e:
                        print(
                            f&#34;[WARN] SiteProfile load failed for Step 4b: {type(e).__name__}: {e}&#34;
                        )

                    _sys.path.insert(0, str(seq_gen_script.parent))
                    from ballast_sequence_generator import (
                        export_to_csv,
                        export_to_dataframe,
                        export_exec_to_csv,
                        export_option_to_csv,
                        export_to_exec_dataframe,
                        export_to_option_dataframe,
                        generate_option_plan,
                        generate_sequence_with_carryforward,
                    )

                    option_plan = generate_option_plan(
                        ballast_plan_df=ballast_plan_df,
                        profile=profile_for_seq,
                        stage_drafts=stage_drafts,
                        tank_catalog_df=tank_catalog_df,
                        fallback_plan_df=fallback_plan_df,
                    )

                    option_csv = out_dir / &#34;BALLAST_OPTION.csv&#34;
                    export_option_to_csv(
                        option_plan,
                        str(option_csv),
                        registry_path=(
                            registry_path if args.enable_headers_ssot else None
                        ),
                    )
                    print(
                        f&#34;[OK] Ballast option plan: {option_csv.name} ({len(option_plan)} options)&#34;
                    )

                    # -------------------------------------------------------------------------
                    # Tidying: BALLAST_OPTION.csv
                    # -------------------------------------------------------------------------
                    if option_csv.exists():
                        validator_registry_path = (
                            registry_path
                            if args.enable_headers_ssot and registry_path
                            else base_dir / &#34;headers_registry.json&#34;
                        )
                        if validator_registry_path.exists():
                            try:
                                from ssot.data_quality_validator import (
                                    DataQualityValidator,
                                )

                                validator = DataQualityValidator(
                                    registry_path=validator_registry_path
                                )
                                tank_catalog_path = (
                                    ssot_dir / &#34;tank_ssot_for_solver.csv&#34;
                                    if ssot_dir.exists()
                                    else None
                                )
                                tidying_results = (
                                    validator.tidy_and_validate_ballast_sequence(
                                        file_path=option_csv,
                                        tank_catalog_path=(
                                            tank_catalog_path
                                            if tank_catalog_path
                                            and tank_catalog_path.exists()
                                            else None
                                        ),
                                        deliverable_id=&#34;BALLAST_OPTION_CSV&#34;,
                                    )
                                )
                                if tidying_results[&#34;llm_context&#34;]:
                                    print(
                                        f&#34;[OK] Option CSV tidying: {tidying_results[&#39;validated_count&#39;]}/{tidying_results[&#39;total_rows&#39;]} rows validated&#34;
                                    )
                                validation_report_path = (
                                    out_dir / &#34;tidying_BALLAST_OPTION_report.json&#34;
                                )
                                import json

                                with open(
                                    validation_report_path, &#34;w&#34;, encoding=&#34;utf-8&#34;
                                ) as f:
                                    json.dump(
                                        tidying_results,
                                        f,
                                        indent=2,
                                        ensure_ascii=False,
                                        default=str,
                                    )
                            except Exception as e:
                                print(
                                    f&#34;[WARN] Option CSV tidying failed: {type(e).__name__}: {e}&#34;
                                )

                    sequence = generate_sequence_with_carryforward(
                        ballast_plan_df=ballast_plan_df,
                        profile=profile_for_seq,
                        stage_drafts=stage_drafts,
                        tank_catalog_df=tank_catalog_df,
                        exclude_optional_stages=True,
                        fallback_plan_df=fallback_plan_df,
                    )

                    # Find optimizer Excel for Delta_t supplementation
                    optimizer_excel = out_dir / &#34;optimizer_ballast_plan.xlsx&#34;
                    if not optimizer_excel.exists():
                        optimizer_excel = None

                    exec_csv = out_dir / &#34;BALLAST_EXEC.csv&#34;
                    export_exec_to_csv(
                        sequence,
                        str(exec_csv),
                        optimizer_excel_path=optimizer_excel,
                        registry_path=(
                            registry_path if args.enable_headers_ssot else None
                        ),
                    )
                    print(
                        f&#34;[OK] Ballast execution sequence: {exec_csv.name} ({len(sequence)} steps)&#34;
                    )

                    # -------------------------------------------------------------------------
                    # Step 4b-1: Data Tidying &amp; Validation (Tidying First Implementation)
                    # -------------------------------------------------------------------------
                    if exec_csv.exists():
                        try:
                            # Try to import DataQualityValidator with base_dir in path
                            import sys as _sys

                            if str(base_dir) not in _sys.path:
                                _sys.path.insert(0, str(base_dir))

                            try:
                                from ssot.data_quality_validator import (
                                    DataQualityValidator,
                                )
                            except ImportError:
                                DataQualityValidator = None

                            if DataQualityValidator is None:
                                print(
                                    &#34;[WARN] DataQualityValidator not available (Pydantic not installed?), skipping tidying&#34;
                                )
                            else:
                                print(&#34;\n&#34; + &#34;-&#34; * 80)
                                print(&#34;[STEP 4b-1] Data Tidying &amp; Validation&#34;)
                                print(&#34;-&#34; * 80)

                                # Initialize validator
                                validator_registry_path = (
                                    registry_path
                                    if args.enable_headers_ssot and registry_path
                                    else base_dir / &#34;headers_registry.json&#34;
                                )

                                if not validator_registry_path.exists():
                                    print(
                                        f&#34;[WARN] Registry not found: {validator_registry_path}, skipping tidying&#34;
                                    )
                                else:
                                    validator = DataQualityValidator(
                                        registry_path=validator_registry_path
                                    )

                                # Tank catalog path
                                tank_catalog_path = (
                                    ssot_dir / &#34;tank_ssot_for_solver.csv&#34;
                                    if ssot_dir.exists()
                                    else None
                                )

                                # Run tidying and validation
                                tidying_results = (
                                    validator.tidy_and_validate_ballast_sequence(
                                        file_path=exec_csv,
                                        tank_catalog_path=(
                                            tank_catalog_path
                                            if tank_catalog_path
                                            and tank_catalog_path.exists()
                                            else None
                                        ),
                                        deliverable_id=&#34;BALLAST_EXEC_CSV&#34;,
                                    )
                                )

                                # Report results
                                if tidying_results[&#34;tidying&#34;][&#34;errors&#34;]:
                                    print(
                                        f&#34;[ERROR] Tidying failed: {len(tidying_results[&#39;tidying&#39;][&#39;errors&#39;])} errors&#34;
                                    )
                                    for err in tidying_results[&#34;tidying&#34;][&#34;errors&#34;][:5]:
                                        print(f&#34;  - {err}&#34;)

                                if tidying_results[&#34;validation&#34;][&#34;errors&#34;]:
                                    print(
                                        f&#34;[ERROR] Validation failed: {len(tidying_results[&#39;validation&#39;][&#39;errors&#39;])} errors&#34;
                                    )
                                    for err in tidying_results[&#34;validation&#34;][&#34;errors&#34;][
                                        :10
                                    ]:
                                        print(f&#34;  - {err}&#34;)

                                if tidying_results[&#34;cross_validation&#34;][&#34;errors&#34;]:
                                    print(
                                        f&#34;[ERROR] Cross-validation failed: {len(tidying_results[&#39;cross_validation&#39;][&#39;errors&#39;])} errors&#34;
                                    )
                                    for err in tidying_results[&#34;cross_validation&#34;][
                                        &#34;errors&#34;
                                    ][:10]:
                                        print(f&#34;  - {err}&#34;)

                                # Save validation report
                                validation_report_path = (
                                    out_dir / &#34;tidying_validation_report.json&#34;
                                )

                                with open(
                                    validation_report_path, &#34;w&#34;, encoding=&#34;utf-8&#34;
                                ) as f:
                                    json.dump(
                                        tidying_results,
                                        f,
                                        indent=2,
                                        ensure_ascii=False,
                                        default=str,
                                    )

                                # Final status
                                if tidying_results[&#34;llm_context&#34;]:
                                    print(
                                        f&#34;[OK] Tidying complete: {tidying_results[&#39;validated_count&#39;]}/{tidying_results[&#39;total_rows&#39;]} rows validated&#34;
                                    )
                                    print(
                                        f&#34;[OK] LLM context generated ({len(tidying_results[&#39;llm_context&#39;])} chars)&#34;
                                    )
                                    print(
                                        f&#34;[OK] Validation report: {validation_report_path.name}&#34;
                                    )
                                    # LLM context is available in tidying_results[&#34;llm_context&#34;]
                                    # Can be used for downstream LLM/AI analysis
                                else:
                                    print(
                                        f&#34;[WARN] Validation failed, LLM input blocked (fail-fast)&#34;
                                    )
                                    print(
                                        f&#34;[INFO] See {validation_report_path.name} for details&#34;
                                    )
                        except Exception as e:
                            print(
                                f&#34;[WARN] Tidying pipeline failed: {type(e).__name__}: {e}&#34;
                            )
                            traceback.print_exc()
                    elif exec_csv.exists() and DataQualityValidator is None:
                        print(
                            &#34;[WARN] DataQualityValidator not available (Pydantic not installed?), skipping tidying&#34;
                        )

                    seq_csv = out_dir / &#34;BALLAST_SEQUENCE.csv&#34;
                    export_to_csv(sequence, str(seq_csv))
                    print(
                        f&#34;[OK] Ballast sequence: {seq_csv.name} ({len(sequence)} steps)&#34;
                    )

                    try:
                        seq_xlsx = out_dir / &#34;BALLAST_SEQUENCE.xlsx&#34;
                        with pd.ExcelWriter(seq_xlsx, engine=&#34;openpyxl&#34;) as writer:
                            df_option = export_to_option_dataframe(option_plan)
                            df_exec = export_to_exec_dataframe(
                                sequence, optimizer_excel_path=optimizer_excel
                            )
                            df_legacy = export_to_dataframe(sequence)
                            df_option.to_excel(
                                writer, index=False, sheet_name=&#34;Ballast_Option&#34;
                            )
                            df_exec.to_excel(
                                writer, index=False, sheet_name=&#34;Ballast_Exec&#34;
                            )
                            df_legacy.to_excel(
                                writer, index=False, sheet_name=&#34;Ballast_Sequence&#34;
                            )
                        print(
                            f&#34;[OK] Ballast sequence Excel: {seq_xlsx.name} (Option + Exec + Legacy)&#34;
                        )
                    except Exception as e:
                        print(f&#34;[WARN] Sequence Excel export failed: {e}&#34;)

                    if checklist_gen_script.exists():
                        _sys.path.insert(0, str(checklist_gen_script.parent))
                        from checklist_generator import generate_checklist

                        checklist_md = out_dir / &#34;BALLAST_OPERATIONS_CHECKLIST.md&#34;
                        generate_checklist(
                            sequence=sequence,
                            profile=profile_for_seq,
                            output_path=str(checklist_md),
                        )
                        print(f&#34;[OK] Operations checklist: {checklist_md.name}&#34;)
                    else:
                        print(
                            f&#34;[WARN] Checklist generator not found: {checklist_gen_script}&#34;
                        )

                    hold_points = [s for s in sequence if s.hold_point]
                    if hold_points:
                        hp_summary = out_dir / &#34;HOLD_POINT_SUMMARY.csv&#34;
                        hp_data = []
                        for hp in hold_points:
                            hp_data.append(
                                {
                                    &#34;Step&#34;: hp.step,
                                    &#34;Stage&#34;: hp.stage,
                                    &#34;Tank&#34;: hp.tank,
                                    &#34;Action&#34;: hp.action,
                                    &#34;Delta_t&#34;: hp.delta_t,
                                    &#34;Draft_FWD_m&#34;: hp.draft_fwd,
                                    &#34;Draft_AFT_m&#34;: hp.draft_aft,
                                    &#34;Tolerance_cm&#34;: 2.0,
                                    &#34;Critical_Tolerance_cm&#34;: 4.0,
                                }
                            )
                        pd.DataFrame(hp_data).to_csv(
                            hp_summary, index=False, encoding=&#34;utf-8-sig&#34;
                        )
                        print(f&#34;[OK] Hold point summary: {hp_summary.name}&#34;)

        except Exception as e:
            print(f&#34;[ERROR] Step 4b failed: {type(e).__name__}: {e}&#34;)
            traceback.print_exc()

    # -------------------------------------------------------------------------
    # Step 4c: Valve Lineup Generation (optional)
    # -------------------------------------------------------------------------
    if (
        args.enable_valve_lineup
        and args.enable_sequence
        and args.from_step &lt;= 4 &lt;= args.to_step
    ):
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 4c] Valve Lineup Generation&#34;)
        print(&#34;=&#34; * 80)

        try:
            seq_csv = out_dir / &#34;BALLAST_SEQUENCE.csv&#34;
            if not seq_csv.exists():
                print(&#34;[WARN] BALLAST_SEQUENCE.csv not found. Run Step 4b first.&#34;)
            else:
                valve_gen_script = resolve_script_path(
                    base_dir, &#34;valve_lineup_generator.py&#34;, label=&#34;VALVE_GEN&#34;
                )
                valve_map_path = base_dir / &#34;valve_map.json&#34;

                if not valve_map_path.exists():
                    print(f&#34;[WARN] valve_map.json not found: {valve_map_path}&#34;)
                elif not valve_gen_script.exists():
                    print(
                        f&#34;[WARN] valve_lineup_generator.py not found: {valve_gen_script}&#34;
                    )
                else:
                    import sys as _sys

                    _sys.path.insert(0, str(valve_gen_script.parent))
                    from valve_lineup_generator import ValveLineupGenerator

                    vlg = ValveLineupGenerator(str(valve_map_path))
                    valve_output_md = out_dir / &#34;BALLAST_SEQUENCE_WITH_VALVES.md&#34;
                    vlg.enhance_ballast_sequence_with_valves(
                        sequence_csv=str(seq_csv),
                        output_path=str(valve_output_md),
                    )
                    print(f&#34;[OK] Valve lineup: {valve_output_md.name}&#34;)

                    seq_xlsx = out_dir / &#34;BALLAST_SEQUENCE.xlsx&#34;
                    if seq_xlsx.exists():
                        try:
                            import pandas as pd
                            from openpyxl import load_workbook

                            wb = load_workbook(seq_xlsx)
                            ws = wb[&#34;Ballast_Sequence&#34;]

                            header_row = 1
                            max_col = ws.max_column
                            valve_seq_col = None
                            valve_notes_col = None
                            for c in range(1, max_col + 1):
                                cell_val = ws.cell(header_row, c).value
                                if cell_val == &#34;Valve_Sequence&#34;:
                                    valve_seq_col = c
                                if cell_val == &#34;Valve_Notes&#34;:
                                    valve_notes_col = c

                            if valve_seq_col is None:
                                valve_seq_col = max_col + 1
                                ws.cell(header_row, valve_seq_col).value = (
                                    &#34;Valve_Sequence&#34;
                                )
                            if valve_notes_col is None:
                                valve_notes_col = max_col + 2
                                ws.cell(header_row, valve_notes_col).value = (
                                    &#34;Valve_Notes&#34;
                                )

                            sequence_df = pd.read_csv(seq_csv, encoding=&#34;utf-8-sig&#34;)
                            # OPTIMIZED: Build lookup dict first, then batch update
                            step_to_valve = {}
                            for _, row in sequence_df.iterrows():
                                tank_id = str(row.get(&#34;Tank&#34;, &#34;&#34;))
                                action = str(row.get(&#34;Action&#34;, &#34;&#34;))
                                step_num = row.get(&#34;Step&#34;, 0)
                                if action not in [&#34;FILL&#34;, &#34;DISCHARGE&#34;]:
                                    continue
                                valve_info = vlg.get_tank_valves(tank_id, action)
                                if not valve_info.get(&#34;found&#34;):
                                    continue
                                valve_seq = &#34;\n&#34;.join(
                                    f&#34;{idx + 1}. {s}&#34;
                                    for idx, s in enumerate(
                                        valve_info.get(&#34;sequence&#34;, [])
                                    )
                                )
                                valve_notes = valve_info.get(&#34;notes&#34;, &#34;&#34;)
                                step_to_valve[step_num] = (valve_seq, valve_notes)

                            # Batch read step numbers (values_only for performance)
                            step_col_values = []
                            for r in range(2, ws.max_row + 1):
                                step_val = ws.cell(r, 1).value
                                step_col_values.append((r, step_val))

                            # Batch update matching rows
                            updates = []
                            for r, step_val in step_col_values:
                                if step_val in step_to_valve:
                                    valve_seq, valve_notes = step_to_valve[step_val]
                                    updates.append((r, valve_seq_col, valve_seq))
                                    updates.append((r, valve_notes_col, valve_notes))

                            # Apply all updates
                            for r, col, val in updates:
                                ws.cell(r, col).value = val

                            wb.save(seq_xlsx)
                            print(f&#34;[OK] Valve details added to Excel: {seq_xlsx.name}&#34;)
                        except Exception as e:
                            print(f&#34;[WARN] Failed to update sequence Excel: {e}&#34;)

        except Exception as e:
            print(f&#34;[ERROR] Step 4c failed: {type(e).__name__}: {e}&#34;)
            traceback.print_exc()

    # -------------------------------------------------------------------------
    # Step 5: Bryan Template (optional)
    # -------------------------------------------------------------------------
    if args.from_step &lt;= 5 &lt;= args.to_step:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[STEP 5] Bryan Template Integration (optional)&#34;)
        print(&#34;=&#34; * 80)
        if not bryan_template_script.exists():
            print(f&#34;[WARN] Bryan template script not found: {bryan_template_script}&#34;)
        elif not stage_results_path.exists():
            print(
                f&#34;[WARN] stage_results.csv missing for Bryan template: {stage_results_path}&#34;
            )
        else:
            bryan_template_out = out_dir / &#34;Bryan_Submission_Data_Pack_Template.xlsx&#34;
            bryan_populated_out = out_dir / &#34;Bryan_Submission_Data_Pack_Populated.xlsx&#34;
            bryan_args = [
                &#34;one-click&#34;,
                &#34;--stage-csv&#34;,
                str(stage_results_path),
                &#34;--out-template&#34;,
                str(bryan_template_out),
                &#34;--out-populated&#34;,
                str(bryan_populated_out),
                &#34;--mode&#34;,
                &#34;overwrite&#34;,
                &#34;--validate&#34;,
            ]
            if spmt_out_xlsx.exists():
                bryan_args += [&#34;--spmt-xlsx&#34;, str(spmt_out_xlsx)]
            r5 = step_run_script(
                6,
                &#34;BRYAN_TEMPLATE&#34;,
                bryan_template_script,
                args=bryan_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r5.ok:
                print(f&#34;[WARN] Step-5 failed (rc={r5.returncode}). Log: {r5.log}&#34;)
            else:
                print(f&#34;[OK] Step-5 completed. Log: {r5.log}&#34;)
                print(f&#34;     Template: {bryan_template_out.name}&#34;)
                print(f&#34;     Populated: {bryan_populated_out.name}&#34;)

    # -------------------------------------------------------------------------
    # Collect all output files to output directory
    # -------------------------------------------------------------------------
    print(&#34;\n&#34; + &#34;=&#34; * 80)
    print(&#34;COLLECTING ALL OUTPUT FILES&#34;)
    print(&#34;=&#34; * 80)
    collect_all_output_files(
        base_dir=base_dir, out_dir=out_dir, site=args.site, inputs_dir=inputs_dir
    )

    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    print(&#34;\n&#34; + &#34;=&#34; * 80)
    print(&#34;PIPELINE COMPLETE&#34;)
    print(&#34;=&#34; * 80)
    print(f&#34;Out dir: {out_dir}&#34;)
    print(&#34;Key SSOT outputs:&#34;)
    print(f&#34; - {tank_ssot_csv.name}&#34;)
    print(f&#34; - {hydro_ssot_csv.name}&#34;)
    print(f&#34; - {stage_table_csv.name}&#34;)
    print(f&#34; - pipeline_stage_QA.csv (definition-split QA)&#34;)

    # -------------------------------------------------------------------------
    # Merge all Excel files into one consolidated file
    # -------------------------------------------------------------------------
    print(&#34;\n&#34; + &#34;=&#34; * 80)
    print(&#34;MERGING EXCEL FILES&#34;)
    print(&#34;=&#34; * 80)
    merged_excel = merge_excel_files_to_one(
        out_dir=out_dir,
        site=args.site,
        base_dir=base_dir,
        include_sequence=args.enable_sequence,
    )
    if merged_excel:
        print(f&#34;[OK] Consolidated Excel: {merged_excel.name}&#34;)

        # Optional: Excel COM full recalc to cache formula results (requires Excel + pywin32)
        com_flag = os.environ.get(&#34;EXCEL_COM_RECALC&#34;, &#34;&#34;).strip().lower()
        com_enabled = com_flag in (&#34;1&#34;, &#34;true&#34;, &#34;yes&#34;, &#34;y&#34;)
        print(f&#34;  [INFO] EXCEL_COM_RECALC={com_flag!r} -&gt; enabled={com_enabled}&#34;)
        if com_enabled:
            try:
                com_script = Path(__file__).parent / &#34;excel_com_recalc_save.py&#34;
                out_path = os.environ.get(&#34;EXCEL_COM_RECALC_OUT&#34;, &#34;&#34;).strip()
                if not com_script.exists():
                    print(f&#34;  [WARN] Excel COM recalc script not found: {com_script}&#34;)
                else:
                    cmd = [sys.executable, str(com_script), &#34;--in&#34;, str(merged_excel)]
                    if out_path:
                        cmd += [&#34;--out&#34;, out_path]
                    print(f&#34;  [INFO] Running Excel COM recalc: {merged_excel.name}&#34;)
                    r = subprocess.run(cmd, check=False, capture_output=True, text=True)
                    if r.stdout:
                        print(r.stdout.rstrip())
                    if r.stderr:
                        print(r.stderr.rstrip())
                    print(f&#34;  [INFO] Excel COM recalc returncode={r.returncode}&#34;)
                    if r.returncode == 0:
                        print(&#34;  [OK] Excel COM recalc completed&#34;)
                    else:
                        print(&#34;  [WARN] Excel COM recalc failed&#34;)
            except Exception as e:
                print(f&#34;  [WARN] Excel COM recalc skipped: {e}&#34;)

        # Inject solver results after COM recalc (if any)
        try:
            inject_all = bool(
                int(os.environ.get(&#34;EXCEL_INJECT_ALL_SHEETS&#34;, &#34;0&#34;) or &#34;0&#34;)
            )
            inject_debug = bool(int(os.environ.get(&#34;EXCEL_INJECT_DEBUG&#34;, &#34;0&#34;) or &#34;0&#34;))
            update_excel_with_solver_results(
                merged_excel_path=merged_excel,
                qa_csv_path=stage_qa_csv,
                sheet_hint=&#34;RORO&#34;,
                update_all_matching_sheets=inject_all,
                debug=inject_debug,
            )
        except Exception as e:
            print(f&#34;[WARN] Solver result injection skipped: {e}&#34;)

    # -------------------------------------------------------------------------
    # Ensure Gate FAIL report is generated (final check before completion)
    # -------------------------------------------------------------------------
    if not args.no_gate_report:
        gate_report_md = out_dir / &#34;gate_fail_report.md&#34;
        if not gate_report_md.exists():
            print(&#34;\n&#34; + &#34;=&#34; * 80)
            print(&#34;FINAL GATE FAIL REPORT GENERATION&#34;)
            print(&#34;=&#34; * 80)
            print(f&#34;[WARN] Gate FAIL report not found. Regenerating...&#34;)
            try:
                ukc_inputs = {
                    &#34;forecast_tide_m&#34;: args.forecast_tide,
                    &#34;depth_ref_m&#34;: args.depth_ref,
                    &#34;ukc_min_m&#34;: args.ukc_min,
                    &#34;squat_m&#34;: float(args.squat),
                    &#34;safety_allow_m&#34;: float(args.safety_allow),
                }
                generate_gate_fail_report_md(
                    out_md=gate_report_md,
                    site=str(args.site),
                    profile_path=resolved_profile_path,
                    stage_qa_csv=stage_qa_csv,
                    tank_ssot_csv=tank_ssot_csv,
                    sensor_stats=sensor_stats,
                    ukc_inputs=ukc_inputs,
                )
                print(f&#34;[OK] Gate FAIL report regenerated: {gate_report_md}&#34;)

                try:
                    append_dnv_mitigation_section_to_gate_report(
                        report_md=gate_report_md,
                        stage_qa_csv=stage_qa_csv,
                        propeller_diameter_m=1.38,
                    )
                    print(&#34;[OK] DNV mitigation section appended to gate report&#34;)
                except Exception as e:
                    print(
                        f&#34;[WARN] Failed to append DNV mitigation: {type(e).__name__}: {e}&#34;
                    )
                try:
                    tug_sop_md = out_dir / &#34;TUG_Operational_SOP_DNV_ST_N001.md&#34;
                    generate_tug_operational_sop_md(
                        out_md=tug_sop_md,
                        stage_qa_csv=stage_qa_csv,
                        site=str(args.site),
                    )
                    print(f&#34;[OK] TUG Operational SOP: {tug_sop_md}&#34;)
                except Exception as e:
                    print(f&#34;[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}&#34;)

                # Append solver outputs if Step 3 was executed
                solver_out_sum = out_dir / &#34;solver_ballast_summary.csv&#34;
                solver_out_plan = out_dir / &#34;solver_ballast_plan.csv&#34;
                solver_out_stage_plan = out_dir / &#34;solver_ballast_stage_plan.csv&#34;
                if solver_out_sum.exists() and solver_out_plan.exists():
                    try:
                        append_solver_section_to_gate_report(
                            report_md=gate_report_md,
                            solver_out_summary=solver_out_sum,
                            solver_out_plan=solver_out_plan,
                            solver_out_stage_plan=solver_out_stage_plan,
                        )
                        print(f&#34;[OK] Solver results appended to Gate FAIL report&#34;)
                    except Exception as e:
                        print(
                            f&#34;[WARN] Failed to append solver section: {type(e).__name__}: {e}&#34;
                        )
            except Exception as e:
                print(
                    f&#34;[ERROR] Gate FAIL report generation failed: {type(e).__name__}: {e}&#34;
                )
                traceback.print_exc()
        else:
            print(f&#34;[OK] Gate FAIL report exists: {gate_report_md}&#34;)

    # -------------------------------------------------------------------------
    # Final output consolidation + manifest
    # -------------------------------------------------------------------------
    final_dir = None
    try:
        final_dir = create_final_output_folder(
            base_dir=base_dir, out_dir=out_dir, merged_excel=merged_excel
        )
    except Exception as e:
        print(f&#34;[WARN] Final output consolidation failed: {type(e).__name__}: {e}&#34;)
        final_dir = None

    # -------------------------------------------------------------------------
    # Head Guard Validation (optional post-processing)
    # -------------------------------------------------------------------------
    if args.auto_head_guard and head_registry_path and head_registry_path.exists() and final_dir:
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[POST-STEP] Head Guard Validation&#34;)
        print(&#34;=&#34; * 80)

        try:
            head_guard_script = resolve_script_path(
                base_dir, &#34;Head Guard.py&#34;, label=&#34;HEAD_GUARD&#34;
            )

            if head_guard_script.exists():
                head_guard_cmd = [
                    which_python(),
                    str(head_guard_script),
                    &#34;--registry&#34;,
                    str(head_registry_path),
                    &#34;--final-dir&#34;,
                    str(final_dir),
                    &#34;--manifest&#34;,
                    &#34;HEAD_MANIFEST.json&#34;,
                ]

                result = subprocess.run(
                    head_guard_cmd,
                    cwd=base_dir,
                    capture_output=True,
                    text=True,
                    encoding=&#34;utf-8&#34;,
                    timeout=300,
                )

                if result.returncode == 0:
                    print(&#34;[OK] Head Guard validation completed&#34;)
                    if result.stdout:
                        lines = result.stdout.split(&#34;\n&#34;)
                        in_summary = False
                        for line in lines:
                            if &#34;Head Guard Validation Summary&#34; in line:
                                in_summary = True
                            if in_summary and line.strip():
                                if line.startswith(&#34;  &#34;) or &#34;:&#34; in line:
                                    print(line)
                            if in_summary and line.startswith(&#34;=&#34;):
                                break
                else:
                    print(
                        f&#34;[WARN] Head Guard validation failed (non-critical): {result.returncode}&#34;
                    )
                    if result.stderr:
                        print(f&#34;[ERROR] {result.stderr[:500]}&#34;)
            else:
                print(f&#34;[WARN] Head Guard script not found: {head_guard_script}&#34;)
                print(
                    &#34;[INFO] You can run manually: python \&#34;Head Guard.py\&#34; --registry HEAD_REGISTRY_AGI_v3.0.yaml --final-dir &lt;final_output_dir&gt;&#34;
                )
        except subprocess.TimeoutExpired:
            print(&#34;[WARN] Head Guard validation timeout (300s) - skipping&#34;)
        except Exception as e:
            print(
                f&#34;[WARN] Head Guard execution failed (non-critical): {type(e).__name__}: {e}&#34;
            )

    # -------------------------------------------------------------------------
    # Excel formula preservation (COM post-processing)
    # -------------------------------------------------------------------------
    if merged_excel and merged_excel.exists():
        print(&#34;\n&#34; + &#34;=&#34; * 80)
        print(&#34;[FORMULA FINALIZE] Running COM post-processing...&#34;)
        print(&#34;=&#34; * 80)

        import subprocess

        finalize_script = Path(__file__).parent / &#34;ballast_excel_finalize.py&#34;

        if finalize_script.exists():
            try:
                result = subprocess.run(
                    [sys.executable, str(finalize_script), str(merged_excel)],
                    capture_output=True,
                    text=True,
                    timeout=FORMULA_TIMEOUT_SEC,
                )

                if result.returncode == 0:
                    print(&#34;[OK] Formula finalization completed successfully&#34;)
                    for line in result.stdout.split(&#34;\n&#34;):
                        if &#34;[RESULT]&#34; in line or &#34;SUCCESS&#34; in line:
                            print(f&#34;  {line}&#34;)
                else:
                    print(&#34;[WARN] Formula finalization failed (non-critical)&#34;)
                    if result.stderr:
                        print(f&#34;[ERROR] {result.stderr[:500]}&#34;)
            except subprocess.TimeoutExpired:
                print(
                    f&#34;[WARN] Formula finalization timeout ({FORMULA_TIMEOUT_SEC}s) - skipping&#34;
                )
            except Exception as e:
                print(f&#34;[WARN] Formula finalization error: {e}&#34;)
        else:
            print(f&#34;[INFO] Formula finalization script not found: {finalize_script}&#34;)
            print(
                &#34;[INFO] You can run manually: python ballast_excel_finalize.py --auto&#34;
            )

    return 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.merge_excel_files_to_one"><code class="name flex">
<span>def <span class="ident">merge_excel_files_to_one</span></span>(<span>out_dir: Path,<br>site: str,<br>base_dir: Optional[Path] = None,<br>include_sequence: bool = False) ‑> pathlib.Path | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_excel_files_to_one(
    out_dir: Path,
    site: str,
    base_dir: Optional[Path] = None,
    include_sequence: bool = False,
) -&gt; Optional[Path]:
    &#34;&#34;&#34;
    Merge all Excel files generated by pipeline into one consolidated Excel file.
    모든 파이프라인에서 생성된 Excel 파일들을 하나의 통합 Excel 파일로 병합합니다.
    &#34;&#34;&#34;
    try:
        import openpyxl
        from openpyxl import load_workbook, Workbook
    except ImportError:
        print(&#34;[WARN] openpyxl not available. Skipping Excel merge.&#34;)
        return None

    # Excel files to merge (in order)
    excel_files = [
        f&#34;LCT_BUSHRA_{site}_TR_Final_v*.xlsx&#34;,  # TR Excel
        f&#34;OPS_FINAL_R3_{site}_Ballast_Integrated.xlsx&#34;,  # OPS Final
        &#34;optimizer_ballast_plan.xlsx&#34;,  # Optimizer
    ]

    # Search directories: out_dir first, then base_dir and base_dir.parent if provided
    search_dirs = [out_dir]
    if base_dir:
        search_dirs.extend([base_dir, base_dir.parent])

    # Find all Excel files that exist
    found_files = []
    for pattern in excel_files:
        file_found = False
        for search_dir in search_dirs:
            if &#34;*&#34; in pattern:
                matches = list(search_dir.glob(pattern))
                if matches:
                    # Get latest file if multiple matches
                    latest_file = sorted(
                        matches, key=lambda p: p.stat().st_mtime, reverse=True
                    )[0]
                    found_files.append(latest_file)
                    file_found = True
                    print(
                        f&#34;[INFO] Found Excel file: {latest_file.name} (in {search_dir.name})&#34;
                    )
                    break
            else:
                file_path = search_dir / pattern
                if file_path.exists():
                    found_files.append(file_path)
                    file_found = True
                    print(
                        f&#34;[INFO] Found Excel file: {file_path.name} (in {search_dir.name})&#34;
                    )
                    break
        if not file_found:
            print(f&#34;[WARN] Excel file not found: {pattern}&#34;)

    if not found_files:
        print(&#34;[INFO] No Excel files found to merge&#34;)
        return None

    # Create merged workbook
    merged_wb = Workbook()
    merged_wb.remove(merged_wb.active)  # Remove default sheet
    try:
        merged_wb.calculation.calcMode = &#34;auto&#34;
        merged_wb.calculation.fullCalcOnLoad = True
        merged_wb.calculation.calcOnSave = True
    except Exception:
        pass

    sheet_count = 0
    existing_titles = set(ws.title for ws in merged_wb.worksheets)

    def _copy_sheet_values_only(source_sheet, target_sheet) -&gt; None:
        for row in source_sheet.iter_rows(values_only=True):
            target_sheet.append(row)

    for excel_file in found_files:
        try:
            print(f&#34;[INFO] Merging: {excel_file.name}&#34;)
            source_wb = load_workbook(excel_file, data_only=False)

            for sheet_name in source_wb.sheetnames:
                source_sheet = source_wb[sheet_name]

                # Create unique sheet name if duplicate
                target_sheet_name = sheet_name
                if target_sheet_name in existing_titles:
                    # Add prefix from source filename
                    prefix = excel_file.stem[:15]  # Use first 15 chars of filename
                    target_sheet_name = f&#34;{prefix}_{sheet_name}&#34;[
                        :31
                    ]  # Excel limit is 31 chars

                # Copy sheet (values/formulas only, skip styles to avoid StyleProxy errors)
                merged_sheet = merged_wb.create_sheet(title=target_sheet_name)
                existing_titles.add(target_sheet_name)

                # Copy all data (including formulas)
                _copy_sheet_values_only(source_sheet, merged_sheet)

                # Copy column dimensions
                try:
                    for col, dim in source_sheet.column_dimensions.items():
                        merged_sheet.column_dimensions[col].width = dim.width
                except Exception:
                    pass  # Skip if dimension copy fails

                # Copy merged cell ranges (without styles)
                try:
                    for merged in source_sheet.merged_cells.ranges:
                        merged_sheet.merge_cells(str(merged))
                except Exception:
                    pass

                sheet_count += 1
                print(f&#34;  [OK] Copied sheet: {target_sheet_name}&#34;)

            source_wb.close()

        except Exception as e:
            print(f&#34;[WARN] Failed to merge {excel_file.name}: {type(e).__name__}: {e}&#34;)
            continue

    if sheet_count == 0:
        print(&#34;[WARN] No sheets were copied&#34;)
        return None

    if include_sequence:
        seq_xlsx = out_dir / &#34;BALLAST_SEQUENCE.xlsx&#34;
        if seq_xlsx.exists():
            try:
                print(f&#34;[INFO] Adding sequence sheets from {seq_xlsx.name}&#34;)
                seq_wb = load_workbook(seq_xlsx, data_only=False)
                for sheet_name in seq_wb.sheetnames:
                    src_sheet = seq_wb[sheet_name]
                    base_name = f&#34;Sequence_{sheet_name}&#34;[:31]
                    target_name = base_name
                    if target_name in existing_titles:
                        suffix = 1
                        base_trim = base_name[:27]
                        while target_name in existing_titles:
                            target_name = f&#34;{base_trim}_{suffix}&#34;[:31]
                            suffix += 1

                    dst_sheet = merged_wb.create_sheet(title=target_name)
                    existing_titles.add(target_name)

                    _copy_sheet_values_only(src_sheet, dst_sheet)

                    try:
                        for merged in src_sheet.merged_cells.ranges:
                            dst_sheet.merge_cells(str(merged))
                    except Exception:
                        pass

                    sheet_count += 1
                    print(f&#34;  [OK] Copied sequence sheet: {target_name}&#34;)

                seq_wb.close()
            except Exception as e:
                print(
                    f&#34;[WARN] Failed to merge BALLAST_SEQUENCE.xlsx: {type(e).__name__}: {e}&#34;
                )

    # PATCH: Update Summary sheets with Solver results (New_FWD_m/New_AFT_m)
    # OPTIMIZED: Bulk update using values_only + batch assignment instead of cell-by-cell
    solver_summary_csv = out_dir / &#34;solver_ballast_summary.csv&#34;
    if solver_summary_csv.exists():
        try:
            import pandas as pd

            solver_df = pd.read_csv(solver_summary_csv, encoding=&#34;utf-8-sig&#34;)
            if &#34;New_FWD_m&#34; in solver_df.columns and &#34;New_AFT_m&#34; in solver_df.columns:
                print(&#34;[INFO] Updating Summary sheets with Solver results...&#34;)
                # Create lookup dict for faster matching
                solver_lookup = dict(
                    zip(
                        solver_df[&#34;Stage&#34;].astype(str).str.strip(),
                        zip(solver_df[&#34;New_FWD_m&#34;], solver_df[&#34;New_AFT_m&#34;]),
                    )
                )

                for sheet_name in merged_wb.sheetnames:
                    if &#34;Summary&#34; in sheet_name or &#34;Stage&#34; in sheet_name:
                        ws = merged_wb[sheet_name]
                        stage_col = None
                        fwd_col = None
                        aft_col = None
                        for c in range(1, min(ws.max_column + 1, 30)):
                            header = str(ws.cell(1, c).value or &#34;&#34;).strip()
                            if &#34;Stage&#34; in header and stage_col is None:
                                stage_col = c
                            if (
                                &#34;New_FWD&#34; in header or &#34;FWD&#34; in header
                            ) and fwd_col is None:
                                fwd_col = c
                            if (
                                &#34;New_AFT&#34; in header or &#34;AFT&#34; in header
                            ) and aft_col is None:
                                aft_col = c

                        if stage_col and fwd_col and aft_col:
                            # Bulk read rows (values_only for performance)
                            updates = []
                            for r in range(2, ws.max_row + 1):
                                stage_name = str(
                                    ws.cell(r, stage_col).value or &#34;&#34;
                                ).strip()
                                if stage_name and stage_name in solver_lookup:
                                    new_fwd, new_aft = solver_lookup[stage_name]
                                    updates.append((r, fwd_col, new_fwd))
                                    updates.append((r, aft_col, new_aft))

                            # Batch update (faster than individual cell assignments)
                            if updates:
                                for r, col, val in updates:
                                    ws.cell(r, col).value = val
                                print(
                                    f&#34;  [OK] Updated {len(updates) // 2} rows in sheet: {sheet_name}&#34;
                                )
        except Exception as e:
            print(
                f&#34;[WARN] Failed to update Summary with Solver results: {type(e).__name__}: {e}&#34;
            )

    # Save merged workbook
    merged_file = out_dir / f&#34;PIPELINE_CONSOLIDATED_{site}_{now_tag()}.xlsx&#34;
    try:
        merged_wb.save(merged_file)
        print(f&#34;[OK] Merged Excel saved: {merged_file.name} ({sheet_count} sheets)&#34;)
        return merged_file
    except Exception as e:
        print(f&#34;[ERROR] Failed to save merged Excel: {type(e).__name__}: {e}&#34;)
        return None</code></pre>
</details>
<div class="desc"><p>Merge all Excel files generated by pipeline into one consolidated Excel file.
모든 파이프라인에서 생성된 Excel 파일들을 하나의 통합 Excel 파일로 병합합니다.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.migrate_tide_outputs_to_parent"><code class="name flex">
<span>def <span class="ident">migrate_tide_outputs_to_parent</span></span>(<span>script_dir: Path, base_dir: Path) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def migrate_tide_outputs_to_parent(script_dir: Path, base_dir: Path) -&gt; None:
    &#34;&#34;&#34;
    If running from tide/ and outputs were created there previously, move them
    up to the parent base_dir to keep outputs in a single place.
    &#34;&#34;&#34;
    if script_dir == base_dir:
        return
    if script_dir.name != &#34;tide&#34;:
        return
    if base_dir != script_dir.parent:
        return

    patterns = (&#34;pipeline_out_*&#34;, &#34;final_output_*&#34;)
    for pattern in patterns:
        for src in sorted(script_dir.glob(pattern)):
            if not src.is_dir():
                continue
            dest = base_dir / src.name
            if dest.exists():
                print(f&#34;[MIGRATE] Skip {src} -&gt; {dest} (dest exists)&#34;)
                continue
            try:
                shutil.move(str(src), str(dest))
                print(f&#34;[MIGRATE] Moved {src} -&gt; {dest}&#34;)
            except Exception as e:
                print(f&#34;[WARN] Failed to move {src}: {type(e).__name__}: {e}&#34;)

    file_patterns = (&#34;*.csv&#34;, &#34;*.xlsx&#34;, &#34;*.md&#34;, &#34;*.log&#34;, &#34;*.txt&#34;)
    for pattern in file_patterns:
        for src in sorted(script_dir.glob(pattern)):
            if not src.is_file():
                continue
            dest = base_dir / src.name
            if dest.exists():
                print(f&#34;[MIGRATE] Skip {src} -&gt; {dest} (dest exists)&#34;)
                continue
            try:
                shutil.move(str(src), str(dest))
                print(f&#34;[MIGRATE] Moved {src} -&gt; {dest}&#34;)
            except Exception as e:
                print(f&#34;[WARN] Failed to move {src}: {type(e).__name__}: {e}&#34;)</code></pre>
</details>
<div class="desc"><p>If running from tide/ and outputs were created there previously, move them
up to the parent base_dir to keep outputs in a single place.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.now_tag"><code class="name flex">
<span>def <span class="ident">now_tag</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def now_tag() -&gt; str:
    return datetime.now().strftime(&#34;%Y%m%d_%H%M%S&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_current_t_sensor_csv"><code class="name flex">
<span>def <span class="ident">resolve_current_t_sensor_csv</span></span>(<span>current_t_arg: str, base_dir: Path, inputs_dir: Path) ‑> pathlib.Path | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resolve_current_t_sensor_csv(
    current_t_arg: str, base_dir: Path, inputs_dir: Path
) -&gt; Optional[Path]:
    &#34;&#34;&#34;Resolve sensor CSV for Current_t injection.&#34;&#34;&#34;
    if current_t_arg:
        cand = Path(current_t_arg)
        if not cand.is_absolute():
            for p in [(inputs_dir / cand), (base_dir / cand), cand]:
                if p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    candidates = [
        inputs_dir / &#34;current_t_sensor.csv&#34;,
        inputs_dir / &#34;current_t.csv&#34;,
        inputs_dir / &#34;sensors&#34; / &#34;current_t_sensor.csv&#34;,
        inputs_dir / &#34;sensors&#34; / &#34;current_t.csv&#34;,
        inputs_dir / &#34;plc&#34; / &#34;current_t.csv&#34;,
        inputs_dir / &#34;iot&#34; / &#34;current_t.csv&#34;,
    ]
    found = find_first_existing([p.resolve() for p in candidates])
    if found:
        return found

    # Fallback: auto-detect current_t_*.csv in common directories.
    glob_dirs = [
        inputs_dir,
        inputs_dir / &#34;sensors&#34;,
        base_dir,
        base_dir / &#34;sensors&#34;,
    ]
    glob_patterns = (&#34;current_t_*.csv&#34;, &#34;current_t-*.csv&#34;)
    globbed = []
    for d in glob_dirs:
        if not d.exists():
            continue
        for pat in glob_patterns:
            globbed.extend(sorted(d.glob(pat)))
    if globbed:
        unique = []
        seen = set()
        for p in globbed:
            rp = p.resolve()
            if rp in seen:
                continue
            seen.add(rp)
            unique.append(rp)
        unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return unique[0]

    return None</code></pre>
</details>
<div class="desc"><p>Resolve sensor CSV for Current_t injection.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_script_path"><code class="name flex">
<span>def <span class="ident">resolve_script_path</span></span>(<span>base_dir: Path, rel_path: str, label: str = 'SCRIPT') ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resolve_script_path(base_dir: Path, rel_path: str, label: str = &#34;SCRIPT&#34;) -&gt; Path:
    &#34;&#34;&#34;
    Resolve script path with fallback to parent dir and first filename match.
    &#34;&#34;&#34;
    candidate = (base_dir / rel_path).resolve()
    if candidate.exists():
        print(f&#34;[RESOLVE] {label}: {candidate} (base_dir)&#34;)
        return candidate
    parent = base_dir.parent
    alt = (parent / rel_path).resolve()
    if alt.exists():
        print(f&#34;[RESOLVE] {label}: {alt} (parent_dir)&#34;)
        return alt
    name = Path(rel_path).name
    matches = sorted(parent.rglob(name))
    if matches:
        print(
            f&#34;[RESOLVE] {label}: {matches[0].resolve()} (rglob, {len(matches)} matches)&#34;
        )
        return matches[0].resolve()
    print(f&#34;[RESOLVE] {label}: {candidate} (missing)&#34;)
    return candidate</code></pre>
</details>
<div class="desc"><p>Resolve script path with fallback to parent dir and first filename match.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_site_profile_path"><code class="name flex">
<span>def <span class="ident">resolve_site_profile_path</span></span>(<span>profile_arg: str, base_dir: Path, inputs_dir: Path, site: str) ‑> pathlib.Path | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resolve_site_profile_path(
    profile_arg: str, base_dir: Path, inputs_dir: Path, site: str
) -&gt; Optional[Path]:
    &#34;&#34;&#34;Resolve a site profile JSON path. Precedence: explicit arg &gt; known defaults.&#34;&#34;&#34;
    if profile_arg:
        cand = Path(profile_arg)
        if not cand.is_absolute():
            # Prefer inputs_dir-relative, then base_dir-relative.
            for p in [(inputs_dir / cand), (base_dir / cand), cand]:
                if p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    site_upper = site.upper().strip()
    site_lower = site_upper.lower()
    candidates = [
        inputs_dir / &#34;profiles&#34; / f&#34;{site_upper}.json&#34;,
        inputs_dir / &#34;profiles&#34; / f&#34;{site_lower}.json&#34;,
        inputs_dir / &#34;profiles&#34; / f&#34;site_{site_upper}.json&#34;,
        inputs_dir / &#34;profiles&#34; / f&#34;site_{site_lower}.json&#34;,
        inputs_dir / f&#34;site_profile_{site_upper}.json&#34;,
        inputs_dir / f&#34;site_profile_{site_lower}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / &#34;profiles&#34; / f&#34;{site_upper}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / &#34;profiles&#34; / f&#34;{site_lower}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / &#34;profiles&#34; / f&#34;site_{site_upper}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / &#34;profiles&#34; / f&#34;site_{site_lower}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / f&#34;site_profile_{site_upper}.json&#34;,
        inputs_dir / &#34;bplus_inputs&#34; / f&#34;site_profile_{site_lower}.json&#34;,
    ]
    return find_first_existing([p.resolve() for p in candidates])</code></pre>
</details>
<div class="desc"><p>Resolve a site profile JSON path. Precedence: explicit arg &gt; known defaults.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_stage_tide_csv"><code class="name flex">
<span>def <span class="ident">resolve_stage_tide_csv</span></span>(<span>stage_tide_arg: str,<br>base_dir: Path,<br>inputs_dir: Path,<br>out_dir: Optional[Path],<br>site: str) ‑> pathlib.Path | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resolve_stage_tide_csv(
    stage_tide_arg: str,
    base_dir: Path,
    inputs_dir: Path,
    out_dir: Optional[Path],
    site: str,
) -&gt; Optional[Path]:
    &#34;&#34;&#34;Resolve stage_tide CSV path for tide assignment.&#34;&#34;&#34;
    if stage_tide_arg:
        cand = Path(stage_tide_arg)
        if not cand.is_absolute():
            for p in [
                inputs_dir / cand,
                base_dir / cand,
                cand,
                out_dir / cand if out_dir else None,
                out_dir / &#34;ssot&#34; / cand if out_dir else None,
            ]:
                if p and p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    site_upper = str(site).upper().strip()
    candidates = [
        inputs_dir / f&#34;stage_tide_{site_upper}.csv&#34;,
        inputs_dir / &#34;stage_tide_AGI.csv&#34;,
        inputs_dir / &#34;stage_tide.csv&#34;,
        base_dir / f&#34;stage_tide_{site_upper}.csv&#34;,
        base_dir / &#34;stage_tide_AGI.csv&#34;,
        base_dir / &#34;stage_tide.csv&#34;,
    ]
    if out_dir:
        candidates.extend(
            [
                out_dir / f&#34;stage_tide_{site_upper}.csv&#34;,
                out_dir / &#34;stage_tide_AGI.csv&#34;,
                out_dir / &#34;stage_tide.csv&#34;,
                out_dir / &#34;ssot&#34; / f&#34;stage_tide_{site_upper}.csv&#34;,
                out_dir / &#34;ssot&#34; / &#34;stage_tide_AGI.csv&#34;,
                out_dir / &#34;ssot&#34; / &#34;stage_tide.csv&#34;,
            ]
        )

    found = find_first_existing([p.resolve() for p in candidates if p is not None])
    if found:
        return found
    return None</code></pre>
</details>
<div class="desc"><p>Resolve stage_tide CSV path for tide assignment.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_cmd"><code class="name flex">
<span>def <span class="ident">run_cmd</span></span>(<span>cmd: List[str], cwd: Path, log_path: Path, env: Optional[Dict[str, str]] = None) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_cmd(
    cmd: List[str], cwd: Path, log_path: Path, env: Optional[Dict[str, str]] = None
) -&gt; int:
    &#34;&#34;&#34;
    Run a subprocess, tee stdout/stderr into a log file.
    &#34;&#34;&#34;
    # #region agent log
    debug_log(
        &#34;run_cmd:114&#34;,
        &#34;Function entry&#34;,
        {&#34;cmd&#34;: cmd, &#34;cwd&#34;: str(cwd), &#34;log_path&#34;: str(log_path)},
        &#34;F&#34;,
    )
    # #endregion
    log_path.parent.mkdir(parents=True, exist_ok=True)

    with log_path.open(&#34;w&#34;, encoding=&#34;utf-8&#34;, errors=&#34;replace&#34;) as f:
        f.write(f&#34;[CMD] {&#39; &#39;.join(cmd)}\n&#34;)
        f.write(f&#34;[CWD] {cwd}\n\n&#34;)
        f.flush()

        p = subprocess.Popen(
            cmd,
            cwd=str(cwd),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding=&#34;utf-8&#34;,
            errors=&#34;replace&#34;,
            env=env or os.environ.copy(),
        )
        assert p.stdout is not None
        for line in p.stdout:
            sys.stdout.write(line)
            f.write(line)
        return_code = p.wait()
        # #region agent log
        debug_log(
            &#34;run_cmd:141&#34;, &#34;Subprocess completed&#34;, {&#34;return_code&#34;: return_code}, &#34;F&#34;
        )
        # #endregion
        return return_code</code></pre>
</details>
<div class="desc"><p>Run a subprocess, tee stdout/stderr into a log file.</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_tidying_for_csv"><code class="name flex">
<span>def <span class="ident">run_tidying_for_csv</span></span>(<span>file_path: Path,<br>base_dir: Path,<br>registry_path: Optional[Path] = None,<br>deliverable_id: Optional[str] = None,<br>tank_catalog_path: Optional[Path] = None,<br>verbose: bool = True) ‑> Dict[str, Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_tidying_for_csv(
    file_path: Path,
    base_dir: Path,
    registry_path: Optional[Path] = None,
    deliverable_id: Optional[str] = None,
    tank_catalog_path: Optional[Path] = None,
    verbose: bool = True,
) -&gt; Optional[Dict[str, Any]]:
    &#34;&#34;&#34;
    CSV 파일에 tidying 및 검증을 실행하는 공통 헬퍼 함수

    Args:
        file_path: 검증할 CSV 파일 경로
        base_dir: Base directory (sys.path 추가용)
        registry_path: Headers registry 경로
        deliverable_id: Deliverable ID
        tank_catalog_path: Tank catalog 경로 (BALLAST 파일용)
        verbose: 상세 출력 여부

    Returns:
        검증 결과 딕셔너리 또는 None (실패 시)
    &#34;&#34;&#34;
    if not file_path.exists():
        return None

    try:
        import sys

        if str(base_dir) not in sys.path:
            sys.path.insert(0, str(base_dir))

        from ssot.data_quality_validator import DataQualityValidator

        validator_registry = registry_path or (base_dir / &#34;headers_registry.json&#34;)
        if not validator_registry.exists():
            if verbose:
                print(
                    f&#34;[WARN] Registry not found: {validator_registry}, skipping tidying&#34;
                )
            return None

        validator = DataQualityValidator(registry_path=validator_registry)

        # BALLAST 시퀀스 파일은 특별 처리
        if &#34;BALLAST&#34; in file_path.name and (
            &#34;EXEC&#34; in file_path.name or &#34;OPTION&#34; in file_path.name
        ):
            results = validator.tidy_and_validate_ballast_sequence(
                file_path=file_path,
                tank_catalog_path=tank_catalog_path,
                deliverable_id=deliverable_id or &#34;BALLAST_CSV&#34;,
            )
        else:
            # 일반 CSV tidying
            results = validator.tidy_and_validate_csv(
                file_path=file_path,
                deliverable_id=deliverable_id or &#34;GENERIC_CSV&#34;,
            )

        if verbose:
            if results[&#34;llm_context&#34;]:
                print(
                    f&#34;[OK] Tidying: {file_path.name} &#34;
                    f&#34;({results[&#39;validated_count&#39;]}/{results[&#39;total_rows&#39;]} rows validated)&#34;
                )
            else:
                error_count = len(results[&#34;tidying&#34;][&#34;errors&#34;]) + len(
                    results[&#34;validation&#34;][&#34;errors&#34;]
                )
                if error_count &gt; 0:
                    print(
                        f&#34;[WARN] Tidying failed: {file_path.name} ({error_count} errors)&#34;
                    )

        # Save validation report
        report_path = file_path.parent / f&#34;tidying_{file_path.stem}_report.json&#34;
        import json

        with open(report_path, &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        return results

    except Exception as e:
        if verbose:
            print(
                f&#34;[WARN] Tidying failed for {file_path.name}: {type(e).__name__}: {e}&#34;
            )
        return None</code></pre>
</details>
<div class="desc"><p>CSV 파일에 tidying 및 검증을 실행하는 공통 헬퍼 함수</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong></dt>
<dd>검증할 CSV 파일 경로</dd>
<dt><strong><code>base_dir</code></strong></dt>
<dd>Base directory (sys.path 추가용)</dd>
<dt><strong><code>registry_path</code></strong></dt>
<dd>Headers registry 경로</dd>
<dt><strong><code>deliverable_id</code></strong></dt>
<dd>Deliverable ID</dd>
<dt><strong><code>tank_catalog_path</code></strong></dt>
<dd>Tank catalog 경로 (BALLAST 파일용)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>상세 출력 여부</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>검증 결과 딕셔너리 또는 None (실패 시)</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.step_run_script"><code class="name flex">
<span>def <span class="ident">step_run_script</span></span>(<span>step_id: int,<br>name: str,<br>script: Path,<br>args: List[str],<br>cwd: Path,<br>out_dir: Path,<br>env: Optional[Dict[str, str]] = None) ‑> 01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_run_script(
    step_id: int,
    name: str,
    script: Path,
    args: List[str],
    cwd: Path,
    out_dir: Path,
    env: Optional[Dict[str, str]] = None,
) -&gt; StepResult:
    if not script.exists():
        log = out_dir / &#34;logs&#34; / f&#34;{step_id:02d}_{name}_MISSING.log&#34;
        log.write_text(f&#34;Missing script: {script}\n&#34;, encoding=&#34;utf-8&#34;)
        return StepResult(False, 127, log)

    log = out_dir / &#34;logs&#34; / f&#34;{step_id:02d}_{name}.log&#34;
    cmd = [which_python(), str(script)] + args
    rc = run_cmd(cmd, cwd=cwd, log_path=log, env=env)
    return StepResult(rc == 0, rc, log)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.update_excel_with_solver_results"><code class="name flex">
<span>def <span class="ident">update_excel_with_solver_results</span></span>(<span>merged_excel_path: Path,<br>qa_csv_path: Path,<br>sheet_hint: Optional[str] = 'RORO',<br>update_all_matching_sheets: bool = False,<br>max_scan_rows: int = 60,<br>debug: bool = False) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_excel_with_solver_results(
    merged_excel_path: Path,
    qa_csv_path: Path,
    sheet_hint: Optional[str] = &#34;RORO&#34;,
    update_all_matching_sheets: bool = False,
    max_scan_rows: int = 60,
    debug: bool = False,
) -&gt; None:
    &#34;&#34;&#34;
    Inject post-solve results from pipeline_stage_QA.csv into consolidated Excel.
    Adds/updates columns:
      - Draft_FWD_m_solver
      - Draft_AFT_m_solver
      - Draft_Source
      - GateA_AFT_MIN_2p70_PASS
      - Gate_Freeboard_ND / Freeboard_Req_ND_m / Freeboard_ND_Margin_m
    &#34;&#34;&#34;
    if not merged_excel_path.exists() or not qa_csv_path.exists():
        print(&#34;[WARN] Cannot inject solver results: files missing&#34;)
        return

    try:
        import pandas as pd
        from openpyxl import load_workbook
    except Exception as e:
        print(f&#34;[WARN] Cannot inject solver results (missing deps): {e}&#34;)
        return

    try:
        qa = pd.read_csv(qa_csv_path, encoding=&#34;utf-8-sig&#34;)
    except Exception as e:
        print(f&#34;[WARN] QA CSV read failed: {e}&#34;)
        return

    if &#34;Stage&#34; not in qa.columns:
        print(&#34;[WARN] QA CSV missing &#39;Stage&#39; column; cannot map rows.&#34;)
        return

    col_map = {
        &#34;Draft_FWD_m_solver&#34;: &#34;Draft_FWD_m&#34;,
        &#34;Draft_AFT_m_solver&#34;: &#34;Draft_AFT_m&#34;,
        &#34;Draft_Source&#34;: &#34;Draft_Source&#34;,
        # Tide/UKC columns
        &#34;Forecast_tide_m&#34;: &#34;Forecast_tide_m&#34;,
        &#34;DepthRef_m&#34;: &#34;DepthRef_m&#34;,
        &#34;UKC_min_m&#34;: &#34;UKC_min_m&#34;,
        &#34;UKC_fwd_m&#34;: &#34;UKC_fwd_m&#34;,
        &#34;UKC_aft_m&#34;: &#34;UKC_aft_m&#34;,
        &#34;Tide_required_m&#34;: &#34;Tide_required_m&#34;,
        &#34;Tide_margin_m&#34;: &#34;Tide_margin_m&#34;,
        &#34;Tide_verdict&#34;: &#34;Tide_verdict&#34;,
        # Freeboard columns (Option C clarification)
        &#34;Freeboard_Min_m&#34;: &#34;Freeboard_Min_m&#34;,
        &#34;Freeboard_Min_BowStern_m&#34;: &#34;Freeboard_Min_BowStern_m&#34;,
        &#34;Freeboard_FWD_m&#34;: &#34;Freeboard_FWD_m&#34;,
        &#34;Freeboard_AFT_m&#34;: &#34;Freeboard_AFT_m&#34;,
        # Gate-A SSOT (preferred)
        &#34;Gate_AFT_MIN_2p70_PASS&#34;: &#34;Gate_AFT_MIN_2p70_PASS&#34;,
        &#34;AFT_Margin_2p70_m&#34;: &#34;AFT_Margin_2p70_m&#34;,
        # Gate-B SSOT (preferred, critical-only)
        &#34;Gate_B_Applies&#34;: &#34;Gate_B_Applies&#34;,
        &#34;Gate_FWD_MAX_2p70_critical_only&#34;: &#34;Gate_FWD_MAX_2p70_critical_only&#34;,
        &#34;FWD_Margin_2p70_m&#34;: &#34;FWD_Margin_2p70_m&#34;,
        # ND gate
        &#34;Gate_Freeboard_ND&#34;: &#34;Gate_Freeboard_ND&#34;,
        &#34;Freeboard_Req_ND_m&#34;: &#34;Freeboard_Req_ND_m&#34;,
        &#34;Freeboard_ND_Margin_m&#34;: &#34;Freeboard_ND_Margin_m&#34;,
        # Draft clipping flags
        &#34;D_vessel_m&#34;: &#34;D_vessel_m&#34;,
        &#34;Draft_Max_raw_m&#34;: &#34;Draft_Max_raw_m&#34;,
        &#34;Draft_Max_solver_m&#34;: &#34;Draft_Max_solver_m&#34;,
        &#34;Draft_Clipped_raw&#34;: &#34;Draft_Clipped_raw&#34;,
        &#34;Draft_Clipped_solver&#34;: &#34;Draft_Clipped_solver&#34;,
    }

    # Backward compatibility: map legacy columns if SSOT columns missing
    if (
        &#34;Gate_AFT_MIN_2p70_PASS&#34; not in qa.columns
        and &#34;GateA_AFT_MIN_2p70_PASS&#34; in qa.columns
    ):
        qa[&#34;Gate_AFT_MIN_2p70_PASS&#34;] = qa[&#34;GateA_AFT_MIN_2p70_PASS&#34;]
    if (
        &#34;AFT_Margin_2p70_m&#34; not in qa.columns
        and &#34;GateA_AFT_MIN_2p70_Margin_m&#34; in qa.columns
    ):
        qa[&#34;AFT_Margin_2p70_m&#34;] = qa[&#34;GateA_AFT_MIN_2p70_Margin_m&#34;]
    if (
        &#34;Gate_B_Applies&#34; not in qa.columns
        and &#34;GateB_FWD_MAX_2p70_CD_applicable&#34; in qa.columns
    ):
        qa[&#34;Gate_B_Applies&#34;] = qa[&#34;GateB_FWD_MAX_2p70_CD_applicable&#34;]
    if (
        &#34;Gate_FWD_MAX_2p70_critical_only&#34; not in qa.columns
        and &#34;GateB_FWD_MAX_2p70_CD_PASS&#34; in qa.columns
    ):
        qa[&#34;Gate_FWD_MAX_2p70_critical_only&#34;] = qa[&#34;GateB_FWD_MAX_2p70_CD_PASS&#34;].apply(
            lambda v: &#34;OK&#34; if bool(v) else &#34;NG&#34;
        )
    if (
        &#34;FWD_Margin_2p70_m&#34; not in qa.columns
        and &#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34; in qa.columns
    ):
        qa[&#34;FWD_Margin_2p70_m&#34;] = qa[&#34;GateB_FWD_MAX_2p70_CD_Margin_m&#34;]
    if &#34;Forecast_tide_m&#34; not in qa.columns and &#34;Forecast_Tide_m&#34; in qa.columns:
        qa[&#34;Forecast_tide_m&#34;] = qa[&#34;Forecast_Tide_m&#34;]
    if &#34;DepthRef_m&#34; not in qa.columns and &#34;Depth_Ref_m&#34; in qa.columns:
        qa[&#34;DepthRef_m&#34;] = qa[&#34;Depth_Ref_m&#34;]
    if &#34;UKC_min_m&#34; not in qa.columns and &#34;UKC_Min_m&#34; in qa.columns:
        qa[&#34;UKC_min_m&#34;] = qa[&#34;UKC_Min_m&#34;]
    if &#34;UKC_min_m&#34; not in qa.columns and &#34;UKC_Min_Required_m&#34; in qa.columns:
        qa[&#34;UKC_min_m&#34;] = qa[&#34;UKC_Min_Required_m&#34;]
    if &#34;UKC_fwd_m&#34; not in qa.columns and &#34;UKC_FWD_m&#34; in qa.columns:
        qa[&#34;UKC_fwd_m&#34;] = qa[&#34;UKC_FWD_m&#34;]
    if &#34;UKC_aft_m&#34; not in qa.columns and &#34;UKC_AFT_m&#34; in qa.columns:
        qa[&#34;UKC_aft_m&#34;] = qa[&#34;UKC_AFT_m&#34;]
    if &#34;Tide_required_m&#34; not in qa.columns and &#34;Required_WL_for_UKC_m&#34; in qa.columns:
        qa[&#34;Tide_required_m&#34;] = qa[&#34;Required_WL_for_UKC_m&#34;]
    if &#34;Tide_verdict&#34; not in qa.columns and &#34;Tide_verification&#34; in qa.columns:
        qa[&#34;Tide_verdict&#34;] = qa[&#34;Tide_verification&#34;]

    qa[&#34;_stage_norm&#34;] = qa[&#34;Stage&#34;].apply(
        lambda s: _canon_stage_key(_norm_stage_name(s))
    )
    dup = qa[&#34;_stage_norm&#34;].value_counts()
    dup = dup[dup &gt; 1]
    if len(dup) &gt; 0:
        print(f&#34;[WARN] Duplicate stage keys after normalization: {dup.to_dict()}&#34;)
        qa = qa.drop_duplicates(subset=[&#34;_stage_norm&#34;], keep=&#34;first&#34;)
    qa_idx = qa.set_index(&#34;_stage_norm&#34;)
    if &#34;&#34; in qa_idx.index:
        print(&#34;[WARN] Empty stage key found in QA CSV after normalization.&#34;)

    try:
        wb = load_workbook(merged_excel_path)
    except Exception as e:
        print(f&#34;[WARN] Failed to open merged Excel: {e}&#34;)
        return

    # choose target sheet(s)
    candidates = []
    for name in wb.sheetnames:
        ws0 = wb[name]
        hr, sc, score = _find_best_header(ws0, max_scan_rows=max_scan_rows)
        if hr and sc:
            candidates.append((ws0, hr, sc, score))
    if not candidates:
        print(&#34;[WARN] No suitable sheet found to update with solver results.&#34;)
        wb.close()
        return

    if update_all_matching_sheets:
        target_list = candidates
        if debug:
            print(
                f&#34;[INFO] Multi-sheet mode ON: updating {len(target_list)} candidate sheet(s)&#34;
            )
    else:
        hinted = (
            [
                c
                for c in candidates
                if sheet_hint and sheet_hint.lower() in c[0].title.lower()
            ]
            if sheet_hint
            else []
        )
        if hinted:
            target_list = [max(hinted, key=lambda x: x[3])]
        else:
            target_list = [max(candidates, key=lambda x: x[3])]
        if debug:
            ws0, hr0, sc0, score0 = target_list[0]
            print(
                f&#34;[INFO] Single-sheet mode: picked sheet=&#39;{ws0.title}&#39;, &#34;
                f&#34;header_row={hr0}, stage_col={sc0}, score={score0:.2f}&#34;
            )

    total_updated = 0
    for ws, header_row, stage_col, score in target_list:
        out_cols: Dict[str, int] = {}
        for out_name in col_map.keys():
            out_cols[out_name] = _ensure_column(ws, header_row, out_name)
        if debug:
            print(
                f&#34;[DEBUG] Created/found columns on &#39;{ws.title}&#39;: {[(k, v) for k, v in out_cols.items()]}&#34;
            )

        # OPTIMIZED: Batch collect updates, then apply in one pass
        updates = []  # List of (row, col, value) tuples
        updated_rows = set()  # Track unique rows updated
        misses = 0
        for r in range(header_row + 1, ws.max_row + 1):
            stage_val = ws.cell(row=r, column=stage_col).value
            if stage_val is None:
                continue
            key = _canon_stage_key(_norm_stage_name(stage_val))
            if not key or key not in qa_idx.index:
                misses += 1
                if debug and misses &lt;= 20:
                    print(
                        f&#34;[WARN] Stage not found: excel={repr(stage_val)} norm={repr(key)} &#34;
                        f&#34;sheet={ws.title} row={r}&#34;
                    )
                continue
            row = qa_idx.loc[key]
            if not isinstance(row, pd.Series):
                if isinstance(row, pd.DataFrame):
                    row = row.iloc[0]
                else:
                    if debug:
                        print(
                            f&#34;[WARN] Unexpected row type: {type(row)} for key={repr(key)}&#34;
                        )
                    continue
            updated_rows.add(r)  # Track this row as updated
            for out_name, src_name in col_map.items():
                if src_name not in qa.columns:
                    continue
                try:
                    v = (
                        row[src_name]
                        if isinstance(row, pd.Series)
                        else row.get(src_name)
                    )
                except (KeyError, AttributeError):
                    v = None
                # Special handling for Gate_FWD_MAX: inject &#34;N/A&#34; instead of None for non-critical stages
                if out_name == &#34;Gate_FWD_MAX_2p70_critical_only&#34; and pd.isna(v):
                    updates.append((r, out_cols[out_name], &#34;N/A&#34;))
                else:
                    updates.append((r, out_cols[out_name], None if pd.isna(v) else v))

        # Batch apply all updates (faster than individual cell assignments)
        for r, col, val in updates:
            ws.cell(row=r, column=col).value = val

        updated = len(updated_rows)

        total_updated += updated
        if debug:
            print(
                f&#34;[DEBUG] sheet=&#39;{ws.title}&#39; header_row={header_row} stage_col={stage_col} &#34;
                f&#34;score={score:.2f} updated={updated} misses={misses}&#34;
            )
        print(f&#34;[OK] Excel sheet updated: sheet=&#39;{ws.title}&#39;, rows={updated}&#34;)

    # Add/update a Tide summary sheet
    tide_cols = [
        &#34;Stage&#34;,
        &#34;Forecast_tide_m&#34;,
        &#34;DepthRef_m&#34;,
        &#34;UKC_min_m&#34;,
        &#34;Tide_required_m&#34;,
        &#34;Tide_margin_m&#34;,
        &#34;UKC_fwd_m&#34;,
        &#34;UKC_aft_m&#34;,
        &#34;Tide_verdict&#34;,
    ]
    tide_avail = [c for c in tide_cols if c in qa.columns]
    if tide_avail:
        if &#34;TIDE_BY_STAGE&#34; in wb.sheetnames:
            del wb[&#34;TIDE_BY_STAGE&#34;]
        ws_tide = wb.create_sheet(&#34;TIDE_BY_STAGE&#34;)
        for c_idx, name in enumerate(tide_avail, start=1):
            ws_tide.cell(row=1, column=c_idx, value=name)
        for r_idx, (_, row) in enumerate(qa[tide_avail].iterrows(), start=2):
            for c_idx, name in enumerate(tide_avail, start=1):
                v = row.get(name)
                ws_tide.cell(row=r_idx, column=c_idx, value=None if pd.isna(v) else v)
        print(
            f&#34;[OK] TIDE_BY_STAGE sheet updated: cols={len(tide_avail)} rows={len(qa)}&#34;
        )
    else:
        print(&#34;[WARN] TIDE_BY_STAGE not created: no tide/ukc columns found in QA CSV.&#34;)

    try:
        wb.save(merged_excel_path)
        print(
            f&#34;[OK] Excel updated with solver results: total_rows={total_updated}, file={merged_excel_path.name}&#34;
        )
    except Exception as e:
        print(f&#34;[WARN] Failed to save merged Excel after injection: {e}&#34;)
    finally:
        wb.close()</code></pre>
</details>
<div class="desc"><p>Inject post-solve results from pipeline_stage_QA.csv into consolidated Excel.
Adds/updates columns:
- Draft_FWD_m_solver
- Draft_AFT_m_solver
- Draft_Source
- GateA_AFT_MIN_2p70_PASS
- Gate_Freeboard_ND / Freeboard_Req_ND_m / Freeboard_ND_Margin_m</p></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.which_python"><code class="name flex">
<span>def <span class="ident">which_python</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def which_python() -&gt; str:
    return sys.executable</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult"><code class="flex name class">
<span>class <span class="ident">StepResult</span></span>
<span>(</span><span>ok: bool, returncode: int, log: Path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class StepResult:
    ok: bool
    returncode: int
    log: Path</code></pre>
</details>
<div class="desc"><p>StepResult(ok: 'bool', returncode: 'int', log: 'Path')</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.log"><code class="name">var <span class="ident">log</span> : pathlib.Path</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.ok"><code class="name">var <span class="ident">ok</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.returncode"><code class="name">var <span class="ident">returncode</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#usage-examples">Usage examples</a></li>
<li><a href="#run-all-agi-defaults">Run all (AGI defaults)</a></li>
<li><a href="#run-only-steps-13-skip-optimizer-step">Run only steps 1..3 (skip optimizer step)</a></li>
<li><a href="#use-custom-inputs-folder">Use custom inputs folder</a></li>
<li><a href="#override-gates-important-captain-aft-minimum">Override gates (important: Captain AFT minimum)</a></li>
<li><a href="#provide-tidedepth-for-ukc-checks-in-solver">Provide tide/depth for UKC checks in solver</a><ul>
<li><a href="#python-integrated_pipeline_defsplit_v2py-forecast_tide-200-depth_ref-550-ukc_min-050">python integrated_pipeline_defsplit_v2.py &ndash;forecast_tide 2.00 &ndash;depth_ref 5.50 &ndash;ukc_min 0.50</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="01_EXECUTION_FILES.tide" href="index.html">01_EXECUTION_FILES.tide</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.add_split_270_gates" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.add_split_270_gates">add_split_270_gates</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_dnv_mitigation_section_to_gate_report" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_dnv_mitigation_section_to_gate_report">append_dnv_mitigation_section_to_gate_report</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_solver_section_to_gate_report" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.append_solver_section_to_gate_report">append_solver_section_to_gate_report</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_site_profile_overrides" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_site_profile_overrides">apply_site_profile_overrides</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tank_overrides_from_profile" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tank_overrides_from_profile">apply_tank_overrides_from_profile</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tide_ukc_profile_fallbacks" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.apply_tide_ukc_profile_fallbacks">apply_tide_ukc_profile_fallbacks</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.build_stage_table_from_stage_results" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.build_stage_table_from_stage_results">build_stage_table_from_stage_results</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.check_dependencies" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.check_dependencies">check_dependencies</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.collect_all_output_files" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.collect_all_output_files">collect_all_output_files</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_hydro_engineering_json_to_solver_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_hydro_engineering_json_to_solver_csv">convert_hydro_engineering_json_to_solver_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_tank_catalog_json_to_solver_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.convert_tank_catalog_json_to_solver_csv">convert_tank_catalog_json_to_solver_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.create_final_output_folder" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.create_final_output_folder">create_final_output_folder</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_log" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_log">debug_log</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_report_step" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.debug_report_step">debug_report_step</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.enrich_stage_table_with_tide_ukc" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.enrich_stage_table_with_tide_ukc">enrich_stage_table_with_tide_ukc</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_bplus_inputs_data_dir" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_bplus_inputs_data_dir">ensure_bplus_inputs_data_dir</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_dir" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.ensure_dir">ensure_dir</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.find_first_existing" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.find_first_existing">find_first_existing</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_gate_fail_report_md" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_gate_fail_report_md">generate_gate_fail_report_md</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_stage_QA_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_stage_QA_csv">generate_stage_QA_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_tug_operational_sop_md" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.generate_tug_operational_sop_md">generate_tug_operational_sop_md</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.inject_current_t_from_sensor_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.inject_current_t_from_sensor_csv">inject_current_t_from_sensor_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.load_site_profile_json" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.load_site_profile_json">load_site_profile_json</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.main" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.main">main</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.merge_excel_files_to_one" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.merge_excel_files_to_one">merge_excel_files_to_one</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.migrate_tide_outputs_to_parent" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.migrate_tide_outputs_to_parent">migrate_tide_outputs_to_parent</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.now_tag" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.now_tag">now_tag</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_current_t_sensor_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_current_t_sensor_csv">resolve_current_t_sensor_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_script_path" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_script_path">resolve_script_path</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_site_profile_path" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_site_profile_path">resolve_site_profile_path</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_stage_tide_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.resolve_stage_tide_csv">resolve_stage_tide_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_cmd" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_cmd">run_cmd</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_tidying_for_csv" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.run_tidying_for_csv">run_tidying_for_csv</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.step_run_script" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.step_run_script">step_run_script</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.update_excel_with_solver_results" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.update_excel_with_solver_results">update_excel_with_solver_results</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.which_python" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.which_python">which_python</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult">StepResult</a></code></h4>
<ul class="">
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.log" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.log">log</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.ok" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.ok">ok</a></code></li>
<li><code><a title="01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.returncode" href="#01_EXECUTION_FILES.tide.integrated_pipeline_defsplit_v2_gate270_split_v3_auditpatched_autodetect_TIDE_v1.StepResult.returncode">returncode</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
