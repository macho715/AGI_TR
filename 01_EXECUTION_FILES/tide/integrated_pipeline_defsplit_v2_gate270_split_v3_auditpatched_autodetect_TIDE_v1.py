#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
==============================================================================
INTEGRATED PIPELINE (Definition-split aware)
==============================================================================
Purpose
- Orchestrate 4 scripts as one repeatable pipeline.
- Prevent the exact confusion you described: Forecast Tide vs Required WL, Draft vs
  Freeboard/UKC, and mixed gate-sets.

Why the previous "download" failed (root cause)
- The prior response pasted code in-chat but did not create a physical file.
- Also, the earlier pipeline logic had a sequencing bug: it attempted to extract a
  stage table BEFORE running Step-1 (so stage CSV could be missing and Step-3 fails).

This version fixes:
- Produces a real file (this script).
- Correct step order:
    Step-1 (TR Excel) -> Step-1b (stage_results.csv) -> build stage_table.csv
    -> Step-2 (OPS integrated) -> Step-3 (Gate solver) -> Step-4 (Optimizer)
- Corrects x_from_midship sign convention when converting tank_catalog JSON:
    x_from_midship = MIDSHIP_FROM_AP - LCG_from_AP
  (Aligns with ops_final_r3_integrated_defs_split_v4.py logic)

Inputs expected in your working folder (or provide --inputs_dir):
- tank_catalog_from_tankmd.json           (object with {"tanks":[...]} format)
- bplus_inputs/Hydro_Table_Engineering.json   (or specify --hydro)
- (optional) stage_results.csv (will be generated by Step-1b if possible)

Scripts required (defaults assume same folder as this pipeline):
- agi_tr_patched_v6_6_defsplit_v1.py
- ops_final_r3_integrated_defs_split_v4.py
- ballast_gate_solver_v4.py
- Untitled-2_patched_defsplit_v1_1.py

Usage examples
------------------------------------------------------------------------------
# Run all (AGI defaults)
python integrated_pipeline_defsplit_v2.py

# Run only steps 1..3 (skip optimizer step)
python integrated_pipeline_defsplit_v2.py --to_step 3

# Use custom inputs folder
python integrated_pipeline_defsplit_v2.py --inputs_dir ./LCF

# Override gates (important: Captain AFT minimum)
python integrated_pipeline_defsplit_v2.py --fwd_max 2.70 --aft_min 2.70

# Provide tide/depth for UKC checks in solver
python integrated_pipeline_defsplit_v2.py --forecast_tide 2.00 --depth_ref 5.50 --ukc_min 0.50
------------------------------------------------------------------------------
Notes
- This pipeline is site-agnostic. For AGI vs DAS, keep the same definition split,
  but you may set different gates, depth_ref, etc.
==============================================================================
"""

from __future__ import annotations

import argparse
import fnmatch
import hashlib
import json
import os
import re
import shutil
import subprocess
import sys
import unicodedata
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import traceback

# SSOT data quality validator (Tidying First Implementation)
try:
    from ssot.data_quality_validator import DataQualityValidator
except ImportError:
    DataQualityValidator = None

# Tide/UKC SSOT engine (optional)
try:
    from .tide_ukc_engine import (
        load_tide_table_any,
        load_stage_schedule_any,
        apply_forecast_tide_from_table,
        required_tide_m,
        ukc_fwd_aft_min,
        verify_tide,
    )
    from .tide_constants import (
        DEFAULT_TIDE_TOL_M,
        FORMULA_TIMEOUT_SEC,
    )
except (ImportError, ValueError):
    # Fallback for direct execution or when tide is not a package
    try:
        from tide_ukc_engine import (
            load_tide_table_any,
            load_stage_schedule_any,
            apply_forecast_tide_from_table,
            required_tide_m,
            ukc_fwd_aft_min,
            verify_tide,
        )
        from tide_constants import (
            DEFAULT_TIDE_TOL_M,
            FORMULA_TIMEOUT_SEC,
        )
    except Exception:
        from tide_constants import (
            DEFAULT_TIDE_TOL_M,
            FORMULA_TIMEOUT_SEC,
        )

        load_tide_table_any = None
        load_stage_schedule_any = None
        apply_forecast_tide_from_table = None
        required_tide_m = None
        ukc_fwd_aft_min = None
        verify_tide = None


# Debug logging setup
# Path should be: LCF/.cursor/debug.log (two levels up from script location)
_script_file = Path(__file__).resolve()
DEBUG_LOG_PATH = _script_file.parent.parent.parent / ".cursor" / "debug.log"
DEBUG_SESSION_ID = "debug-session"


def debug_log(
    location: str, message: str, data: dict = None, hypothesis_id: str = None
):
    """Write debug log entry in NDJSON format"""
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        import json
        from datetime import datetime

        entry = {
            "sessionId": DEBUG_SESSION_ID,
            "runId": "run1",
            "timestamp": int(datetime.now().timestamp() * 1000),
            "location": location,
            "message": message,
            "data": data or {},
        }
        if hypothesis_id:
            entry["hypothesisId"] = hypothesis_id
        with DEBUG_LOG_PATH.open("a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            f.flush()  # Ensure data is written immediately
    except Exception as e:
        # Log exception to stderr for debugging (don't fail silently during development)
        import sys

        sys.stderr.write(f"DEBUG_LOG_ERROR: {type(e).__name__}: {e}\n")
        sys.stderr.flush()


# -----------------------------------------------------------------------------
# Platform / encoding helpers
# -----------------------------------------------------------------------------
if sys.platform == "win32":
    # Windows console utf-8 safety
    import io

    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")


# -----------------------------------------------------------------------------
# Constants (SSOT alignment)
# -----------------------------------------------------------------------------
LPP_M = 60.302
MIDSHIP_FROM_AP_M = LPP_M / 2.0  # 30.151 m
D_VESSEL_M = 3.65  # molded depth (for freeboard = D - Draft)


# -----------------------------------------------------------------------------
# Small utilities
# -----------------------------------------------------------------------------
def now_tag() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_dir(p: Path) -> Path:
    p.mkdir(parents=True, exist_ok=True)
    return p


def which_python() -> str:
    return sys.executable


def run_cmd(
    cmd: List[str], cwd: Path, log_path: Path, env: Optional[Dict[str, str]] = None
) -> int:
    """
    Run a subprocess, tee stdout/stderr into a log file.
    """
    # #region agent log
    debug_log(
        "run_cmd:114",
        "Function entry",
        {"cmd": cmd, "cwd": str(cwd), "log_path": str(log_path)},
        "F",
    )
    # #endregion
    log_path.parent.mkdir(parents=True, exist_ok=True)

    with log_path.open("w", encoding="utf-8", errors="replace") as f:
        f.write(f"[CMD] {' '.join(cmd)}\n")
        f.write(f"[CWD] {cwd}\n\n")
        f.flush()

        p = subprocess.Popen(
            cmd,
            cwd=str(cwd),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding="utf-8",
            errors="replace",
            env=env or os.environ.copy(),
        )
        assert p.stdout is not None
        for line in p.stdout:
            sys.stdout.write(line)
            f.write(line)
        return_code = p.wait()
        # #region agent log
        debug_log(
            "run_cmd:141", "Subprocess completed", {"return_code": return_code}, "F"
        )
        # #endregion
        return return_code


def find_first_existing(paths: List[Path]) -> Optional[Path]:
    for p in paths:
        if p.exists():
            return p
    return None


def resolve_script_path(base_dir: Path, rel_path: str, label: str = "SCRIPT") -> Path:
    """
    Resolve script path with fallback to parent dir and first filename match.
    """
    candidate = (base_dir / rel_path).resolve()
    if candidate.exists():
        print(f"[RESOLVE] {label}: {candidate} (base_dir)")
        return candidate
    parent = base_dir.parent
    alt = (parent / rel_path).resolve()
    if alt.exists():
        print(f"[RESOLVE] {label}: {alt} (parent_dir)")
        return alt
    name = Path(rel_path).name
    matches = sorted(parent.rglob(name))
    if matches:
        print(
            f"[RESOLVE] {label}: {matches[0].resolve()} (rglob, {len(matches)} matches)"
        )
        return matches[0].resolve()
    print(f"[RESOLVE] {label}: {candidate} (missing)")
    return candidate


def migrate_tide_outputs_to_parent(script_dir: Path, base_dir: Path) -> None:
    """
    If running from tide/ and outputs were created there previously, move them
    up to the parent base_dir to keep outputs in a single place.
    """
    if script_dir == base_dir:
        return
    if script_dir.name != "tide":
        return
    if base_dir != script_dir.parent:
        return

    patterns = ("pipeline_out_*", "final_output_*")
    for pattern in patterns:
        for src in sorted(script_dir.glob(pattern)):
            if not src.is_dir():
                continue
            dest = base_dir / src.name
            if dest.exists():
                print(f"[MIGRATE] Skip {src} -> {dest} (dest exists)")
                continue
            try:
                shutil.move(str(src), str(dest))
                print(f"[MIGRATE] Moved {src} -> {dest}")
            except Exception as e:
                print(f"[WARN] Failed to move {src}: {type(e).__name__}: {e}")

    file_patterns = ("*.csv", "*.xlsx", "*.md", "*.log", "*.txt")
    for pattern in file_patterns:
        for src in sorted(script_dir.glob(pattern)):
            if not src.is_file():
                continue
            dest = base_dir / src.name
            if dest.exists():
                print(f"[MIGRATE] Skip {src} -> {dest} (dest exists)")
                continue
            try:
                shutil.move(str(src), str(dest))
                print(f"[MIGRATE] Moved {src} -> {dest}")
            except Exception as e:
                print(f"[WARN] Failed to move {src}: {type(e).__name__}: {e}")


def ensure_bplus_inputs_data_dir(base_dir: Path) -> None:
    """
    Ensure bplus_inputs/data exists and has Frame_x_from_mid_m.json.
    This prevents Step-1 fallback warnings.
    """
    data_dir = base_dir / "bplus_inputs" / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    dest = data_dir / "Frame_x_from_mid_m.json"
    if dest.exists():
        return

    candidates = [
        base_dir / "Frame_x_from_mid_m.json",
        base_dir / "bplus_inputs" / "Frame_x_from_mid_m.json",
        base_dir.parent / "02_RAW_DATA" / "Frame_x_from_mid_m.json",
        base_dir.parent / "02_RAW_DATA" / "bplus_inputs" / "Frame_x_from_mid_m.json",
    ]

    for src in candidates:
        if src.exists():
            try:
                shutil.copy2(src, dest)
                print(f"[OK] Copied Frame_x_from_mid_m.json -> {dest}")
                return
            except Exception as e:
                print(
                    f"[WARN] Failed to copy Frame_x_from_mid_m.json from {src}: {type(e).__name__}: {e}"
                )
                return

    print(
        "[WARN] Frame_x_from_mid_m.json not found in expected locations. "
        "Proceeding with fallback."
    )


def check_dependencies(base_dir: Path) -> bool:
    """
    Check and auto-install required packages from requirements.txt.
    Returns True if all core dependencies are available, False otherwise.
    """
    requirements_file = base_dir / "requirements.txt"

    # Core dependencies required for pipeline execution
    core_packages = {
        "pandas": "pandas>=1.5.0",
        "numpy": "numpy>=1.23.0",
        "openpyxl": "openpyxl>=3.0.0",
        "scipy": "scipy>=1.9.0",
    }

    if not requirements_file.exists():
        print(f"[INFO] Creating requirements.txt at {requirements_file}")
        try:
            with open(requirements_file, "w", encoding="utf-8") as f:
                f.write("# Core dependencies for AGI RORO TR Pipeline\n")
                for spec in core_packages.values():
                    f.write(f"{spec}\n")
                f.write("\n# Optional dependencies\n")
                f.write("pyyaml>=6.0\n")
                f.write("loguru>=0.6.0\n")
            print("[OK] Created requirements.txt")
        except Exception as e:
            print(f"[WARN] Failed to create requirements.txt: {e}")

    missing_specs = []
    missing_names = []

    for import_name, package_spec in core_packages.items():
        try:
            __import__(import_name)
        except ImportError:
            missing_specs.append(package_spec)
            missing_names.append(package_spec.split(">=")[0])

    if missing_specs:
        print(f"[INFO] Missing packages detected: {', '.join(missing_names)}")
        print("[INFO] Auto-installing dependencies...")
        try:
            install_cmd = [
                sys.executable,
                "-m",
                "pip",
                "install",
                "--quiet",
                "--upgrade",
            ] + missing_specs
            result = subprocess.run(
                install_cmd,
                check=False,
                capture_output=True,
                text=True,
                timeout=300,
            )
            if result.returncode != 0:
                print("[ERROR] Failed to install packages:")
                print(f"  stdout: {result.stdout}")
                print(f"  stderr: {result.stderr}")
                print(
                    f"[INFO] Please install manually: pip install {' '.join(missing_specs)}"
                )
                return False
            print(f"[OK] Successfully installed: {', '.join(missing_names)}")
        except subprocess.TimeoutExpired:
            print("[ERROR] Installation timeout (exceeded 5 minutes)")
            return False
        except Exception as e:
            print(f"[ERROR] Installation failed: {e}")
            print(
                f"[INFO] Please install manually: pip install {' '.join(missing_specs)}"
            )
            return False

        still_missing = []
        for import_name in core_packages:
            try:
                __import__(import_name)
            except ImportError:
                still_missing.append(import_name)
        if still_missing:
            print(
                f"[ERROR] Some packages still missing after installation: {', '.join(still_missing)}"
            )
            return False

    # Try to read requirements.txt to check version requirements (optional)
    try:
        with open(requirements_file, "r", encoding="utf-8") as f:
            requirements_content = f.read()
            # Check for core packages in requirements.txt
            for spec in core_packages.values():
                if spec.split(">=")[0] not in requirements_content:
                    print(f"[WARN] {spec} not found in requirements.txt")
    except Exception as e:
        print(f"[WARN] Could not read requirements.txt: {e}")

    print(
        f"[OK] Core dependencies check passed (requirements.txt: {requirements_file})"
    )
    return True


# -----------------------------------------------------------------------------
# Profile / Sensor SSOT helpers (Patch-set)
# -----------------------------------------------------------------------------
def _argv_has_flag(flag: str, argv: List[str]) -> bool:
    """Return True if CLI argv contains flag (either '--x' or '--x=...')."""
    flag = flag.strip()
    neg = None
    if flag.startswith("--") and not flag.startswith("--no-"):
        neg = "--no-" + flag[2:]
    for arg in argv:
        if arg == flag or arg.startswith(flag + "="):
            return True
        if neg and (arg == neg or arg.startswith(neg + "=")):
            return True
    return False


def _try_read_csv_flexible(path: Path) -> pd.DataFrame:
    """Read a CSV with best-effort encoding + delimiter inference."""
    last_err: Optional[Exception] = None
    for enc in ("utf-8-sig", "utf-8", "cp949", "cp1252"):
        try:
            return pd.read_csv(path, sep=None, engine="python", encoding=enc)
        except Exception as e:
            last_err = e
            continue
    if last_err:
        debug_log(
            "_try_read_csv_flexible",
            "Falling back to default pandas.read_csv (encoding inference failed)",
            {"path": str(path), "last_error": str(last_err)},
        )
    return pd.read_csv(path, sep=None, engine="python")


def resolve_site_profile_path(
    profile_arg: str, base_dir: Path, inputs_dir: Path, site: str
) -> Optional[Path]:
    """Resolve a site profile JSON path. Precedence: explicit arg > known defaults."""
    if profile_arg:
        cand = Path(profile_arg)
        if not cand.is_absolute():
            # Prefer inputs_dir-relative, then base_dir-relative.
            for p in [(inputs_dir / cand), (base_dir / cand), cand]:
                if p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    site_upper = site.upper().strip()
    site_lower = site_upper.lower()
    candidates = [
        inputs_dir / "profiles" / f"{site_upper}.json",
        inputs_dir / "profiles" / f"{site_lower}.json",
        inputs_dir / "profiles" / f"site_{site_upper}.json",
        inputs_dir / "profiles" / f"site_{site_lower}.json",
        inputs_dir / f"site_profile_{site_upper}.json",
        inputs_dir / f"site_profile_{site_lower}.json",
        inputs_dir / "bplus_inputs" / "profiles" / f"{site_upper}.json",
        inputs_dir / "bplus_inputs" / "profiles" / f"{site_lower}.json",
        inputs_dir / "bplus_inputs" / "profiles" / f"site_{site_upper}.json",
        inputs_dir / "bplus_inputs" / "profiles" / f"site_{site_lower}.json",
        inputs_dir / "bplus_inputs" / f"site_profile_{site_upper}.json",
        inputs_dir / "bplus_inputs" / f"site_profile_{site_lower}.json",
    ]
    return find_first_existing([p.resolve() for p in candidates])


def load_site_profile_json(profile_path: Path) -> Dict[str, object]:
    """Load a site profile JSON file."""
    obj = json.loads(profile_path.read_text(encoding="utf-8"))
    if not isinstance(obj, dict):
        raise ValueError(f"Profile JSON must be an object: {profile_path}")
    return obj


def apply_site_profile_overrides(
    args: argparse.Namespace, profile: Dict[str, object], argv: List[str]
) -> None:
    """Apply profile values to argparse args, unless user explicitly passed the CLI flag."""
    keymap = {
        "fwd_max_m": ("fwd_max", "--fwd_max"),
        "aft_min_m": ("aft_min", "--aft_min"),
        "aft_max_m": ("aft_max", "--aft_max"),
        "trim_abs_limit_m": ("trim_abs_limit", "--trim_abs_limit"),
        "pump_rate_tph": ("pump_rate", "--pump_rate"),
        "forecast_tide_m": ("forecast_tide", "--forecast_tide"),
        "depth_ref_m": ("depth_ref", "--depth_ref"),
        "ukc_min_m": ("ukc_min", "--ukc_min"),
        "squat_m": ("squat", "--squat"),
        "safety_allow_m": ("safety_allow", "--safety_allow"),
        "tank_keywords": ("tank_keywords", "--tank_keywords"),
        "current_t_csv": ("current_t_csv", "--current_t_csv"),
        "trim_limit_enforced": ("trim_limit_enforced", "--trim-limit-enforced"),
        "freeboard_min_enforced": (
            "freeboard_min_enforced",
            "--freeboard-min-enforced",
        ),
        "freeboard_min_m": ("freeboard_min_m", "--freeboard-min-m"),
    }
    for k, (attr, flag) in keymap.items():
        if k not in profile:
            continue
        if _argv_has_flag(flag, argv):
            continue
        if hasattr(args, attr):
            setattr(args, attr, profile[k])


def apply_tide_ukc_profile_fallbacks(
    args: argparse.Namespace, profile: Optional[Dict[str, object]], argv: List[str]
) -> None:
    """Apply tide_ukc profile defaults when CLI flags are not provided."""
    if not isinstance(profile, dict):
        return
    tide_ukc = profile.get("tide_ukc", {})
    if not isinstance(tide_ukc, dict):
        return
    keymap = {
        "forecast_tide_m": ("forecast_tide", "--forecast_tide"),
        "depth_ref_m": ("depth_ref", "--depth_ref"),
        "ukc_min_m": ("ukc_min", "--ukc_min"),
        "squat_m": ("squat", "--squat"),
        "safety_allow_m": ("safety_allow", "--safety_allow"),
    }
    for k, (attr, flag) in keymap.items():
        if _argv_has_flag(flag, argv):
            continue
        if hasattr(args, attr) and k in tide_ukc:
            current_val = getattr(args, attr)
            # For parameters with default 0.0 (squat_m, safety_allow_m),
            # apply profile value even if current value is 0.0 (not None)
            # For other parameters (forecast_tide_m, depth_ref_m, ukc_min_m),
            # only apply if current value is None
            if current_val is None or (
                k in ("squat_m", "safety_allow_m") and current_val == 0.0
            ):
                setattr(args, attr, tide_ukc[k])
                print(
                    f"[OK] Applied {k}={tide_ukc[k]} from profile (was {current_val})"
                )


def resolve_current_t_sensor_csv(
    current_t_arg: str, base_dir: Path, inputs_dir: Path
) -> Optional[Path]:
    """Resolve sensor CSV for Current_t injection."""
    if current_t_arg:
        cand = Path(current_t_arg)
        if not cand.is_absolute():
            for p in [(inputs_dir / cand), (base_dir / cand), cand]:
                if p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    candidates = [
        inputs_dir / "current_t_sensor.csv",
        inputs_dir / "current_t.csv",
        inputs_dir / "sensors" / "current_t_sensor.csv",
        inputs_dir / "sensors" / "current_t.csv",
        inputs_dir / "plc" / "current_t.csv",
        inputs_dir / "iot" / "current_t.csv",
    ]
    found = find_first_existing([p.resolve() for p in candidates])
    if found:
        return found

    # Fallback: auto-detect current_t_*.csv in common directories.
    glob_dirs = [
        inputs_dir,
        inputs_dir / "sensors",
        base_dir,
        base_dir / "sensors",
    ]
    glob_patterns = ("current_t_*.csv", "current_t-*.csv")
    globbed = []
    for d in glob_dirs:
        if not d.exists():
            continue
        for pat in glob_patterns:
            globbed.extend(sorted(d.glob(pat)))
    if globbed:
        unique = []
        seen = set()
        for p in globbed:
            rp = p.resolve()
            if rp in seen:
                continue
            seen.add(rp)
            unique.append(rp)
        unique.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return unique[0]

    return None


def resolve_stage_tide_csv(
    stage_tide_arg: str,
    base_dir: Path,
    inputs_dir: Path,
    out_dir: Optional[Path],
    site: str,
) -> Optional[Path]:
    """Resolve stage_tide CSV path for tide assignment."""
    if stage_tide_arg:
        cand = Path(stage_tide_arg)
        if not cand.is_absolute():
            for p in [
                inputs_dir / cand,
                base_dir / cand,
                cand,
                out_dir / cand if out_dir else None,
                out_dir / "ssot" / cand if out_dir else None,
            ]:
                if p and p.exists():
                    return p.resolve()
        if cand.exists():
            return cand.resolve()

    site_upper = str(site).upper().strip()
    candidates = [
        inputs_dir / f"stage_tide_{site_upper}.csv",
        inputs_dir / "stage_tide_AGI.csv",
        inputs_dir / "stage_tide.csv",
        base_dir / f"stage_tide_{site_upper}.csv",
        base_dir / "stage_tide_AGI.csv",
        base_dir / "stage_tide.csv",
    ]
    if out_dir:
        candidates.extend(
            [
                out_dir / f"stage_tide_{site_upper}.csv",
                out_dir / "stage_tide_AGI.csv",
                out_dir / "stage_tide.csv",
                out_dir / "ssot" / f"stage_tide_{site_upper}.csv",
                out_dir / "ssot" / "stage_tide_AGI.csv",
                out_dir / "ssot" / "stage_tide.csv",
            ]
        )

    found = find_first_existing([p.resolve() for p in candidates if p is not None])
    if found:
        return found
    return None


def inject_current_t_from_sensor_csv(
    tank_ssot_csv: Path,
    sensor_csv: Path,
    strategy: str = "override",
    out_csv: Optional[Path] = None,
) -> Dict[str, object]:
    """
    Inject Current_t values from a sensor/PLC/IoT CSV into tank_ssot_for_solver.csv.

    Supported sensor columns (case-insensitive):
    - Tank identifier: Tank | tank_id | id | tag | name
    - Value (priority order):
        1) Current_t | tons | ton | amount_t | value_t
        2) level_pct | level_percent  (uses Capacity_t)
        3) volume_m3 | m3 + optional spgr (defaults 1.0)
    """
    out_csv = out_csv or tank_ssot_csv

    tank_df = pd.read_csv(tank_ssot_csv, encoding="utf-8-sig")
    sensor_df = _try_read_csv_flexible(sensor_csv)

    # Normalize sensor columns
    s_cols = {str(c).strip().lower(): c for c in sensor_df.columns}
    t_col = None
    for k in ("tank", "tank_id", "tankid", "id", "tag", "name"):
        if k in s_cols:
            t_col = s_cols[k]
            break
    if t_col is None:
        raise ValueError(
            f"Sensor CSV missing tank id column. Columns={list(sensor_df.columns)}"
        )

    # Value columns
    v_col = None
    for k in (
        "current_t",
        "current_ton",
        "current_tons",
        "tons",
        "ton",
        "amount_t",
        "value_t",
        "value",
    ):
        if k in s_cols:
            v_col = s_cols[k]
            break

    level_col = None
    for k in ("level_pct", "level_percent", "level_%"):
        if k in s_cols:
            level_col = s_cols[k]
            break

    vol_col = None
    for k in ("volume_m3", "vol_m3", "m3"):
        if k in s_cols:
            vol_col = s_cols[k]
            break

    spgr_col = None
    for k in ("spgr", "sg", "specific_gravity"):
        if k in s_cols:
            spgr_col = s_cols[k]
            break

    # Build exact lookup (case-insensitive)
    sensor_df = sensor_df.copy()
    sensor_df["_tank_key"] = sensor_df[t_col].astype(str).str.strip().str.upper()

    exact_map: Dict[str, float] = {}
    bad_value_rows: List[str] = []

    def _to_float(x) -> Optional[float]:
        try:
            if pd.isna(x):
                return None
            return float(x)
        except Exception:
            return None

    if v_col is not None:
        for _, r in sensor_df.iterrows():
            key = str(r.get("_tank_key", "")).strip()
            val = _to_float(r.get(v_col))
            if not key:
                continue
            if val is None:
                bad_value_rows.append(key)
                continue
            exact_map[key] = float(val)
        value_mode = "Current_t"
    elif level_col is not None:
        # We'll convert later using Capacity_t
        value_mode = "level_pct"
    elif vol_col is not None:
        value_mode = "volume_m3"
    else:
        raise ValueError(
            f"Sensor CSV missing value column. Columns={list(sensor_df.columns)}"
        )

    # Prepare group matching (e.g., 'FWB1' -> 'FWB1.P'/'FWB1.S')
    tank_df = tank_df.copy()
    tank_df["_tank_key"] = tank_df["Tank"].astype(str).str.strip().str.upper()
    tank_df["_tank_base"] = (
        tank_df["Tank"].astype(str).str.strip().str.upper().str.split(".").str[0]
    )

    # Group values from sensor where exact tank id doesn't exist but base exists
    group_map: Dict[str, float] = {}
    if value_mode == "Current_t":
        existing_tanks = set(tank_df["_tank_key"].tolist())
        for key, val in list(exact_map.items()):
            if key in existing_tanks:
                continue
            # Treat as base key if any tank base matches
            if key in set(tank_df["_tank_base"].tolist()):
                group_map[key] = val
                # Keep exact_map entry as-is; apply to bases in the update loop

    # Update tank_df
    updated_exact = 0
    updated_group = 0

    # Diff-audit (sensor injection) â€” write diff_audit.csv next to out_csv
    audit_rows: List[Dict[str, object]] = []
    audit_csv = Path(out_csv).resolve().parent / "diff_audit.csv"
    audit_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for i, row in tank_df.iterrows():
        tank_key = row["_tank_key"]
        tank_base = row["_tank_base"]

        current_old = float(row.get("Current_t", 0.0) or 0.0)

        # Audit fields
        match_type = "none"  # exact | group | base | none
        sensor_key_used = ""  # exact tank key or base key used for lookup
        sensor_raw_val = None  # raw value read from sensor csv (t / % / m3)
        sensor_raw_unit = ""  # "t" | "%" | "m3"
        sensor_spgr = None  # specific gravity for volume_m3 mode
        group_n = None  # number of members for group distribution
        computed_new_t = None  # derived tons before clamp (distributed/converted)

        if value_mode == "Current_t":
            new_val = None
            if tank_key in exact_map:
                new_val = exact_map[tank_key]
                match_type = "exact"
                sensor_key_used = tank_key
                sensor_raw_val = float(exact_map[tank_key])
                sensor_raw_unit = "t"
            elif tank_base in group_map:
                # distribute equally across group members
                members = tank_df[tank_df["_tank_base"] == tank_base]
                group_n = max(int(len(members)), 1)
                new_val = float(group_map[tank_base]) / float(group_n)
                match_type = "group"
                sensor_key_used = tank_base
                sensor_raw_val = float(group_map[tank_base])  # total on base key
                sensor_raw_unit = "t"
        elif value_mode == "level_pct":
            # Find row in sensor_df for exact or base match
            new_val = None
            match = sensor_df[sensor_df["_tank_key"] == tank_key]
            match_key_used = tank_key
            if match.empty:
                match = sensor_df[sensor_df["_tank_key"] == tank_base]
                match_key_used = tank_base
            if not match.empty:
                lvl = _to_float(match.iloc[0].get(level_col))
                if lvl is not None:
                    cap = float(row.get("Capacity_t", 0.0) or 0.0)
                    new_val = cap * float(lvl) / 100.0
                    match_type = "exact" if match_key_used == tank_key else "base"
                    sensor_key_used = match_key_used
                    sensor_raw_val = float(lvl)
                    sensor_raw_unit = "%"
        elif value_mode == "volume_m3":
            new_val = None
            match = sensor_df[sensor_df["_tank_key"] == tank_key]
            match_key_used = tank_key
            if match.empty:
                match = sensor_df[sensor_df["_tank_key"] == tank_base]
                match_key_used = tank_base
            if not match.empty:
                vol = _to_float(match.iloc[0].get(vol_col))
                if vol is not None:
                    sg = _to_float(match.iloc[0].get(spgr_col)) if spgr_col else 1.0
                    sg = 1.0 if sg is None else float(sg)
                    new_val = float(vol) * sg
                    match_type = "exact" if match_key_used == tank_key else "base"
                    sensor_key_used = match_key_used
                    sensor_raw_val = float(vol)
                    sensor_raw_unit = "m3"
                    sensor_spgr = float(sg)

        if new_val is None:
            # No match from sensor CSV for this tank (or base)
            audit_rows.append(
                {
                    "TS": audit_ts,
                    "SensorCSV": str(sensor_csv),
                    "Strategy": strategy,
                    "ValueMode": value_mode,
                    "Tank": str(row.get("Tank", "")),
                    "TankKey": tank_key,
                    "TankBase": tank_base,
                    "MatchType": match_type,
                    "SensorKey": sensor_key_used,
                    "SensorRaw": sensor_raw_val,
                    "SensorRawUnit": sensor_raw_unit,
                    "SensorSpGr": sensor_spgr,
                    "GroupN": group_n,
                    "CurrentOld_t": round(current_old, 6),
                    "ComputedNew_t": None,
                    "Min_t": None,
                    "Max_t": None,
                    "Clamped_t": None,
                    "Rounded_t": None,
                    "Delta_t": None,
                    "ClampedFlag": "N",
                    "Updated": "N",
                    "SkipReason": "NO_MATCH",
                }
            )
            continue

        computed_new_t = float(new_val)

        # Strategy application
        if strategy == "fill_missing" and abs(current_old) > 1e-9:
            # Keep existing value; do not overwrite
            audit_rows.append(
                {
                    "TS": audit_ts,
                    "SensorCSV": str(sensor_csv),
                    "Strategy": strategy,
                    "ValueMode": value_mode,
                    "Tank": str(row.get("Tank", "")),
                    "TankKey": tank_key,
                    "TankBase": tank_base,
                    "MatchType": match_type,
                    "SensorKey": sensor_key_used,
                    "SensorRaw": sensor_raw_val,
                    "SensorRawUnit": sensor_raw_unit,
                    "SensorSpGr": sensor_spgr,
                    "GroupN": group_n,
                    "CurrentOld_t": round(current_old, 6),
                    "ComputedNew_t": round(computed_new_t, 6),
                    "Min_t": None,
                    "Max_t": None,
                    "Clamped_t": None,
                    "Rounded_t": None,
                    "Delta_t": 0.0,
                    "ClampedFlag": "N",
                    "Updated": "N",
                    "SkipReason": "FILL_MISSING_KEEP",
                }
            )
            continue

        # Clamp to bounds if present
        min_t = float(row.get("Min_t", 0.0) or 0.0)
        max_t = float(row.get("Max_t", row.get("Capacity_t", 0.0)) or 0.0)
        clamped = max(min_t, min(float(computed_new_t), max_t))
        rounded_val = round(clamped, 6)
        tank_df.at[i, "Current_t"] = rounded_val

        clamp_flag = abs(float(computed_new_t) - float(clamped)) > 1e-9
        delta_t = float(rounded_val) - float(current_old)

        if tank_key in exact_map:
            updated_exact += 1
        elif tank_base in group_map:
            updated_group += 1

        audit_rows.append(
            {
                "TS": audit_ts,
                "SensorCSV": str(sensor_csv),
                "Strategy": strategy,
                "ValueMode": value_mode,
                "Tank": str(row.get("Tank", "")),
                "TankKey": tank_key,
                "TankBase": tank_base,
                "MatchType": match_type,
                "SensorKey": sensor_key_used,
                "SensorRaw": sensor_raw_val,
                "SensorRawUnit": sensor_raw_unit,
                "SensorSpGr": sensor_spgr,
                "GroupN": group_n,
                "CurrentOld_t": round(current_old, 6),
                "ComputedNew_t": round(computed_new_t, 6),
                "Min_t": round(min_t, 6),
                "Max_t": round(max_t, 6),
                "Clamped_t": round(clamped, 6),
                "Rounded_t": round(rounded_val, 6),
                "Delta_t": round(delta_t, 6),
                "ClampedFlag": "Y" if clamp_flag else "N",
                "Updated": "Y",
                "SkipReason": "",
            }
        )

    # Persist
    tank_df.drop(
        columns=[c for c in ("_tank_key", "_tank_base") if c in tank_df.columns],
        inplace=True,
    )
    tank_df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # Write diff_audit.csv (before/after + clamp flag)
    audit_error: Optional[str] = None
    try:
        audit_df = pd.DataFrame(audit_rows)
        # Helpful ordering: updated rows first, then by abs(delta)
        if not audit_df.empty and "Delta_t" in audit_df.columns:
            try:
                audit_df["_abs_delta"] = audit_df["Delta_t"].abs()
                audit_df.sort_values(
                    by=["Updated", "_abs_delta", "TankKey"],
                    ascending=[False, False, True],
                    inplace=True,
                )
                audit_df.drop(columns=["_abs_delta"], inplace=True)
            except Exception:
                pass
        audit_df.to_csv(audit_csv, index=False, encoding="utf-8-sig")
    except Exception as e:
        audit_error = f"{type(e).__name__}: {e}"

    return {
        "sensor_csv": str(sensor_csv),
        "strategy": strategy,
        "value_mode": value_mode,
        "updated_exact": updated_exact,
        "updated_group": updated_group,
        "bad_value_rows": bad_value_rows[:50],
        "diff_audit_csv": str(audit_csv),
        "diff_audit_error": audit_error,
        "diff_audit_rows": int(len(audit_rows)),
        "tank_rows": int(len(tank_df)),
    }


def apply_tank_overrides_from_profile(
    tank_ssot_csv: Path,
    profile: Optional[Dict[str, object]],
    out_csv: Optional[Path] = None,
) -> Dict[str, object]:
    """
    Apply per-tank overrides from site profile JSON into Tank SSOT CSV.

    Expected profile structure (extra keys are ignored):
      {
        "tank_overrides": {
          "VOID3":    {"mode":"FIXED","use_flag":"Y"},
          "FWCARGO1": {"mode":"FIXED","use_flag":"N"},
          "FWCARGO2": {"mode":"FIXED","use_flag":"N"}
        }
      }

    Matching logic:
      - Exact match on Tank name (case-insensitive)
      - Base match is allowed only when explicitly enabled in override
        (e.g., {"match":"base"})

    Supported override columns (if present in Tank SSOT):
      - use_flag (str)
      - mode (str)  [solver recognizes: FILL_DISCHARGE / DISCHARGE_ONLY / FILL_ONLY / FIXED / BLOCKED]
      - pump_rate_tph (float)
      - Min_t, Max_t (float)
      - priority_weight (float)
    """
    stats: Dict[str, object] = {
        "overrides_applied": 0,
        "tanks_touched": 0,
        "missing_keys": [],
        "base_skipped": 0,
        "base_skipped_keys": [],
    }
    if not profile or not isinstance(profile, dict):
        return stats

    overrides = (
        profile.get("tank_overrides")
        or profile.get("tank_override")
        or profile.get("tanks_overrides")
    )
    if not overrides or not isinstance(overrides, dict):
        return stats

    out_csv = out_csv or tank_ssot_csv
    df = pd.read_csv(tank_ssot_csv, encoding="utf-8-sig")

    def _norm(s: object) -> str:
        return str(s).strip().upper()

    def _base(tank_name: object) -> str:
        t = _norm(tank_name)
        return t.split(".", 1)[0] if "." in t else t

    if "Tank" not in df.columns:
        raise ValueError(f"Tank SSOT missing required column 'Tank': {tank_ssot_csv}")

    df["_tank_norm"] = df["Tank"].map(_norm)
    df["_tank_base"] = df["_tank_norm"].map(_base)

    editable_cols = {
        "use_flag": "str",
        "mode": "str",
        "pump_rate_tph": "float",
        "Min_t": "float",
        "Max_t": "float",
        "priority_weight": "float",
    }

    def _as_match_mode(ov: Dict[str, object]) -> str:
        return str(ov.get("match", "")).strip().lower()

    touched = set()
    processed_tanks = set()

    # Phase 1: exact matches only (keys with '.' OR match=="exact")
    for key, ov in overrides.items():
        if not isinstance(ov, dict):
            continue
        k_norm = _norm(key)
        match_mode = _as_match_mode(ov)
        is_exact_only = ("." in k_norm) or (match_mode == "exact")
        if not is_exact_only:
            continue
        exact_mask = df["_tank_norm"] == k_norm
        if not bool(exact_mask.any()):
            stats["missing_keys"].append(key)
            continue
        for col, typ in editable_cols.items():
            if col not in ov or col not in df.columns:
                continue
            val = ov.get(col)
            if val is None:
                continue
            try:
                if typ == "str":
                    df.loc[exact_mask, col] = str(val).strip()
                else:
                    df.loc[exact_mask, col] = float(val)
            except Exception:
                continue
        stats["overrides_applied"] = int(stats["overrides_applied"]) + 1
        for t in df.loc[exact_mask, "_tank_norm"].tolist():
            touched.add(t)
            processed_tanks.add(t)

    # Phase 2: base matches (explicitly enabled via match=="base" and not already processed by exact)
    for key, ov in overrides.items():
        if not isinstance(ov, dict):
            continue
        k_norm = _norm(key)
        match_mode = _as_match_mode(ov)
        if "." in k_norm or match_mode == "exact":
            continue
        if match_mode != "base":
            stats["base_skipped"] = int(stats["base_skipped"]) + 1
            stats["base_skipped_keys"].append(key)
            continue
        k_base = _base(k_norm)
        base_mask = (df["_tank_base"] == k_base) & (
            ~df["_tank_norm"].isin(processed_tanks)
        )
        if not bool(base_mask.any()):
            stats["missing_keys"].append(key)
            continue
        for col, typ in editable_cols.items():
            if col not in ov or col not in df.columns:
                continue
            val = ov.get(col)
            if val is None:
                continue
            try:
                if typ == "str":
                    df.loc[base_mask, col] = str(val).strip()
                else:
                    df.loc[base_mask, col] = float(val)
            except Exception:
                continue
        stats["overrides_applied"] = int(stats["overrides_applied"]) + 1
        for t in df.loc[base_mask, "_tank_norm"].tolist():
            touched.add(t)
            processed_tanks.add(t)

    df.drop(columns=["_tank_norm", "_tank_base"], inplace=True, errors="ignore")
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    stats["tanks_touched"] = len(touched)
    return stats


# ==========================================================
# 2.70m Split Gates (Captain vs Mammoet)
#   - Gate-A: Captain safety requirement (AFT draft >= 2.70m)
#   - Gate-B: Mammoet constraint for critical RoRo stages (FWD draft <= 2.70m, Chart Datum referenced)
# ==========================================================
CAPTAIN_AFT_MIN_DRAFT_M = 2.70
MAMMOET_FWD_MAX_DRAFT_M_CD = 2.70
# Gate Labels SSOT (avoid ambiguous "2.70m")
GATE_A_LABEL = "AFT_MIN_2p70"
GATE_B_LABEL = "FWD_MAX_2p70_critical_only"
GATE_A_VALUE_M = CAPTAIN_AFT_MIN_DRAFT_M
GATE_B_VALUE_M = MAMMOET_FWD_MAX_DRAFT_M_CD
DEFAULT_CRITICAL_STAGE_REGEX = (
    r"(preballast.*critical|6a.*critical|stage\s*5.*preballast|stage\s*6a)"
)
_AGI_STAGE_DESC_MAP = {
    "Stage 1": "Arrival (lightship)",
    "Stage 2": "TR1 ramp start",
    "Stage 3": "TR1 mid-ramp",
    "Stage 4": "TR1 on deck",
    "Stage 5": "TR1 final position",
    "Stage 5_PreBallast": "Water supply complete",
    "Stage 6A_Critical (Opt C)": "TR2 ramp entry",
    "Stage 6C": "Final stowage",
    "Stage 7": "Departure (cargo off)",
}
_CRITICAL_RAMP_PATTERNS = [
    r"ramp\s*start",
    r"ramp\s*entry",
    r"ramp\s*mid",
    r"roll.*on",
    r"loadout",
]
PROFILE_CRITICAL_STAGE_KEYS = [
    "critical_stages",
    "critical_stage_list",
    "gateB_critical_stages",
    "mammoet_gateb_critical_stages",
]
PROFILE_CRITICAL_REGEX_KEYS = [
    "critical_stage_regex",
    "gateB_critical_regex",
    "mammoet_gateb_critical_regex",
]
PROFILE_GATEB_CRITICAL_ONLY_KEYS = [
    "mammoet_gateb_critical_only",
    "gateB_critical_only",
]
PROFILE_GATEB_FWD_MAX_KEYS = ["mammoet_gateb_fwd_max_m_cd", "gateB_fwd_max_m_cd"]


def _profile_last_value(profile: Optional[Dict[str, object]], keys: List[str]):
    if not isinstance(profile, dict):
        return None
    val = None
    for k in keys:
        if k in profile:
            val = profile[k]
    return val


def _profile_truthy_any(profile: Optional[Dict[str, object]], keys: List[str]) -> bool:
    if not isinstance(profile, dict):
        return False
    return any(bool(profile.get(k)) for k in keys if k in profile)


def _normalize_stage_patterns(val: object) -> Optional[List[str]]:
    if isinstance(val, (list, tuple)):
        items = [str(v).strip() for v in val if str(v).strip()]
        return items if items else None
    return None


def _coerce_float(val: object, default: Optional[float] = None) -> Optional[float]:
    try:
        if val is None:
            return default
        return float(val)
    except Exception:
        return default


def _infer_col(df, candidates):
    """Return the first matching column name in df for any candidate (case-insensitive)."""
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return None


def _is_critical_stage(
    stage_name: object,
    stage_description: Optional[str] = None,
    patterns: Optional[List[str]] = None,
    regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
) -> bool:
    desc = stage_description or _AGI_STAGE_DESC_MAP.get(str(stage_name).strip())
    if patterns:
        if _match_stage(stage_name, patterns, regex):
            return True
        if desc and _match_stage(desc, patterns, regex):
            return True
        return False
    if _match_stage(stage_name, None, regex):
        return True
    if desc and re.search(regex, desc, flags=re.IGNORECASE):
        return True
    combined = f"{stage_name} {desc or ''}"
    for pattern in _CRITICAL_RAMP_PATTERNS:
        if re.search(pattern, combined, flags=re.IGNORECASE):
            return True
    return False


def _match_stage(
    stage: object,
    patterns: Optional[List[str]],
    regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
) -> bool:
    if stage is None:
        return False
    stage_str = str(stage)
    pat_list = _normalize_stage_patterns(patterns) if patterns else None
    if pat_list:
        for p in pat_list:
            p_str = str(p).strip()
            if not p_str:
                continue
            if p_str.startswith("re:"):
                expr = p_str[3:]
                try:
                    if re.search(expr, stage_str, flags=re.IGNORECASE):
                        return True
                except re.error:
                    continue
            elif ("*" in p_str) or ("?" in p_str):
                if fnmatch.fnmatchcase(stage_str.lower(), p_str.lower()):
                    return True
            else:
                if stage_str.strip().lower() == p_str.lower():
                    return True
        return False
    return re.search(regex, stage_str, flags=re.IGNORECASE) is not None


def add_split_270_gates(
    stage_df,
    aft_min_m: float = CAPTAIN_AFT_MIN_DRAFT_M,
    fwd_max_m_cd: float = MAMMOET_FWD_MAX_DRAFT_M_CD,
    critical_only: bool = False,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    critical_stage_list: Optional[List[str]] = None,
    tol_m: float = 0.0,
):
    """
    Add two split 2.70m gates into stage_df:
      - GateA_AFT_MIN_2p70_PASS (Captain): AFT >= 2.70
      - GateB_FWD_MAX_2p70_CD_PASS (Mammoet): FWD <= 2.70 for critical stages only
    Also adds margin columns (m) and applicability flag for Gate-B.

    Args:
        tol_m: Guard-band tolerance (m). Default 0.0 (strict). Use 0.02 for 2cm guard-band.
               Applied to both Gate-A (AFT >= 2.70 - tol_m) and Gate-B (FWD <= 2.70 + tol_m).
    """
    df = stage_df.copy()

    # Stage / Draft column inference (adjust candidates if your df uses different names)
    col_stage = _infer_col(df, ["Stage", "stage", "stage_name", "StageName"])
    col_fwd = _infer_col(
        df,
        [
            "Draft_FWD_m",
            "draft_fwd_m",
            "new_fwd_m",
            "New_FWD_m",
            "FWD_m",
            "fwd_m",
            "fwd_draft_m",
            "ForwardDraft_m",
        ],
    )
    col_aft = _infer_col(
        df,
        [
            "Draft_AFT_m",
            "draft_aft_m",
            "new_aft_m",
            "New_AFT_m",
            "AFT_m",
            "aft_m",
            "aft_draft_m",
            "AftDraft_m",
        ],
    )

    if col_fwd is None or col_aft is None:
        raise ValueError(
            f"[add_split_270_gates] Cannot infer FWD/AFT draft columns. fwd={col_fwd}, aft={col_aft}"
        )

    # Gate-B uses Chart Datum reference. Convert to CD if tide is available.
    col_tide = _infer_col(
        df,
        ["Forecast_Tide_m", "forecast_tide_m", "Tide_m", "tide_m"],
    )
    col_fwd_gate = col_fwd
    if col_tide is not None:
        try:
            df["Draft_FWD_m_CD"] = pd.to_numeric(
                df[col_fwd], errors="coerce"
            ) - pd.to_numeric(df[col_tide], errors="coerce")
            col_fwd_gate = "Draft_FWD_m_CD"
        except Exception:
            col_fwd_gate = col_fwd

    # Gate-A (Captain): AFT >= 2.70 (with guard-band tolerance)
    df["AFT_MIN_2p70_m"] = float(aft_min_m)
    df["AFT_Margin_2p70_m"] = (df[col_aft] - aft_min_m).round(2)
    # Guard-band ì ìš©: v >= -tol_m (tol_m=0.02ë©´ 2.68m ì´ìƒ PASS)
    df["Gate_AFT_MIN_2p70"] = df["AFT_Margin_2p70_m"].apply(
        lambda v: "OK" if pd.notna(v) and v >= -tol_m else "NG"
    )
    df["Gate_AFT_MIN_2p70_PASS"] = df["Gate_AFT_MIN_2p70"] == "OK"
    # Legacy aliases
    df["GateA_AFT_MIN_2p70_PASS"] = df["Gate_AFT_MIN_2p70_PASS"]
    df["GateA_AFT_MIN_2p70_Margin_m"] = df["AFT_Margin_2p70_m"]

    # Gate-B (Mammoet): FWD <= 2.70 (CD ref) for critical stages
    # ALWAYS compute applicability by critical matcher when stage column exists
    if col_stage is None:
        df["GateB_FWD_MAX_2p70_CD_applicable"] = True
    else:
        df["GateB_FWD_MAX_2p70_CD_applicable"] = df[col_stage].apply(
            lambda s: _is_critical_stage(
                s, patterns=critical_stage_list, regex=critical_regex
            )
        )
    df["FWD_MAX_applicable"] = df["GateB_FWD_MAX_2p70_CD_applicable"]

    # PASS rule: if not applicable -> PASS (but excluded from fail count via applicability flag)
    df["GateB_FWD_MAX_2p70_CD_PASS"] = True
    mask_app = df["GateB_FWD_MAX_2p70_CD_applicable"] == True
    # Guard-band ì ìš©: fwd_max_m_cd + tol_mê¹Œì§€ í—ˆìš© (tol_m=0.02ë©´ 2.72mê¹Œì§€ PASS)
    df.loc[mask_app, "GateB_FWD_MAX_2p70_CD_PASS"] = (
        df.loc[mask_app, col_fwd_gate] <= fwd_max_m_cd + tol_m
    )
    df["GateB_FWD_MAX_2p70_CD_Margin_m"] = (fwd_max_m_cd - df[col_fwd_gate]).round(2)
    df.loc[~mask_app, "GateB_FWD_MAX_2p70_CD_Margin_m"] = None

    # SSOT labels (critical-only)
    df["FWD_MAX_2p70_m"] = float(fwd_max_m_cd)
    df["Gate_B_Applies"] = df["GateB_FWD_MAX_2p70_CD_applicable"]
    df["FWD_Margin_2p70_m"] = (fwd_max_m_cd - df[col_fwd_gate]).round(2)
    df["Gate_FWD_MAX_2p70_critical_only"] = "N/A"
    # Guard-band ì ìš©: margin >= -tol_më©´ OK
    df.loc[mask_app, "Gate_FWD_MAX_2p70_critical_only"] = df.loc[
        mask_app, "GateB_FWD_MAX_2p70_CD_Margin_m"
    ].apply(lambda v: "OK" if pd.notna(v) and v >= -tol_m else "NG")
    df["Gate_FWD_MAX_2p70_critical_only_PASS"] = (
        df["Gate_FWD_MAX_2p70_critical_only"] == "OK"
    )

    return df


def generate_gate_fail_report_md(
    *,
    out_md: Path,
    site: str,
    profile_path: Optional[Path],
    stage_qa_csv: Path,
    tank_ssot_csv: Path,
    sensor_stats: Optional[Dict[str, object]] = None,
    ukc_inputs: Optional[Dict[str, object]] = None,
) -> Path:
    """Generate a Markdown report summarizing gate failures and likely root causes.

    PATCH: Extended with Gate_Hydro_Range and HardStop summary support.
    """

    # Helper functions
    def _norm(s):
        return str(s).strip() if pd.notna(s) else ""

    def _is_ng_gate(v):
        vv = _norm(v).upper()
        return vv in ("NG", "FAIL", "NOK")

    def _is_true(v):
        vv = _norm(v).upper()
        return vv in ("Y", "YES", "TRUE", "1", "HARDSTOP", "STOP")

    def _first_existing_col(header, candidates):
        for c in candidates:
            if c in header:
                return c
        return None

    qa = pd.read_csv(stage_qa_csv, encoding="utf-8-sig")
    tanks = pd.read_csv(tank_ssot_csv, encoding="utf-8-sig")

    def _derive_fwd_max_from_qa(df: pd.DataFrame) -> Optional[float]:
        if "FWD_MAX_m" not in df.columns:
            return None
        vals = pd.to_numeric(df["FWD_MAX_m"], errors="coerce").dropna()
        if vals.empty:
            return None
        return float(vals.iloc[0])

    profile_obj: Optional[Dict[str, object]] = None
    crit_list: Optional[List[str]] = None
    crit_regex = DEFAULT_CRITICAL_STAGE_REGEX
    gateb_critical_only = False
    gateb_fwd_max = None

    if profile_path and Path(profile_path).exists():
        try:
            profile_obj = load_site_profile_json(Path(profile_path))
        except Exception:
            profile_obj = None

    if profile_obj:
        crit_list = _normalize_stage_patterns(
            _profile_last_value(profile_obj, PROFILE_CRITICAL_STAGE_KEYS)
        )
        crit_regex_val = _profile_last_value(profile_obj, PROFILE_CRITICAL_REGEX_KEYS)
        if isinstance(crit_regex_val, str) and crit_regex_val.strip():
            crit_regex = crit_regex_val
        gateb_critical_only = _profile_truthy_any(
            profile_obj, PROFILE_GATEB_CRITICAL_ONLY_KEYS
        )
        gateb_fwd_max = _coerce_float(
            _profile_last_value(profile_obj, PROFILE_GATEB_FWD_MAX_KEYS), None
        )

    if gateb_fwd_max is None:
        gateb_fwd_max = _derive_fwd_max_from_qa(qa)
    if gateb_fwd_max is None:
        gateb_fwd_max = MAMMOET_FWD_MAX_DRAFT_M_CD

    # ----------------------------------------------------------
    # PATCH: add split 2.70m gates (Captain vs Mammoet)
    #   - GateA_AFT_MIN_2p70_PASS
    #   - GateB_FWD_MAX_2p70_CD_PASS (profile-driven critical stages)
    # ----------------------------------------------------------
    qa = add_split_270_gates(
        qa,
        aft_min_m=CAPTAIN_AFT_MIN_DRAFT_M,
        fwd_max_m_cd=gateb_fwd_max,
        critical_only=gateb_critical_only,
        critical_regex=crit_regex,
        critical_stage_list=crit_list,
    )

    # Basic tank stats
    tanks_current = pd.to_numeric(tanks.get("Current_t", 0.0), errors="coerce").fillna(
        0.0
    )
    tanks_zero = int((tanks_current.abs() <= 1e-9).sum())
    tanks_total = int(len(tanks))
    tanks_sum = float(tanks_current.sum())

    # Gate column auto-detection
    base_gate_cols = ["Gate_FWD_Max", "Gate_AFT_Min", "Gate_Freeboard", "Gate_UKC"]

    hydro_gate_col = _first_existing_col(
        qa.columns,
        [
            "Gate_Hydro_Range",
            "Gate_HydroRange",
            "Gate_Hydro_OutOfRange",
            "HydroOutOfRange",
            "Hydro_OutOfRange",
        ],
    )
    hardstop_col = _first_existing_col(
        qa.columns,
        ["HardStop", "HardStop_Flag", "HardStop_YN", "Hard_Stop", "HS_Any"],
    )
    hardstop_reason_col = _first_existing_col(
        qa.columns,
        [
            "HardStop_Reason",
            "HardStop_Reasons",
            "HardStopReason",
            "HardStop_Tag",
            "HardStop_Tags",
        ],
    )

    gate_cols = list(base_gate_cols)
    if hydro_gate_col and hydro_gate_col not in gate_cols:
        gate_cols.append(hydro_gate_col)
    if hardstop_col and hardstop_col not in gate_cols:
        gate_cols.append(hardstop_col)
    # Add split 2.70m gates
    gate_cols += ["GateA_AFT_MIN_2p70_PASS", "GateB_FWD_MAX_2p70_CD_PASS"]

    # Gate stats with extended columns
    def _cnt_ng(col: str) -> int:
        if col not in qa.columns:
            return 0
        if col == hardstop_col:
            return int(qa[col].astype(str).apply(lambda x: _is_true(x)).sum())
        if col == "Gate_FWD_Max" and "FWD_MAX_applicable" in qa.columns:
            mask_app = qa["FWD_MAX_applicable"] == True
            return int(
                (
                    (qa[col].astype(str).str.upper() == "NG") & mask_app.fillna(False)
                ).sum()
            )
        # Gate-B counts only where applicable
        if (
            col == "GateB_FWD_MAX_2p70_CD_PASS"
            and "GateB_FWD_MAX_2p70_CD_applicable" in qa.columns
        ):
            mask_app = qa["GateB_FWD_MAX_2p70_CD_applicable"] == True
            return int(((~qa[col]) & mask_app).sum())
        # For boolean pass columns (True=PASS, False=FAIL)
        if qa[col].dtype == bool:
            return int((~qa[col]).sum())
        return int((qa[col].astype(str).str.upper() == "NG").sum())

    cnt_fwd = _cnt_ng("Gate_FWD_Max")
    cnt_aft = _cnt_ng("Gate_AFT_Min")
    cnt_fb = _cnt_ng("Gate_Freeboard")
    cnt_ukc = _cnt_ng("Gate_UKC")
    cnt_hydro = _cnt_ng(hydro_gate_col) if hydro_gate_col else 0
    cnt_hardstop = _cnt_ng(hardstop_col) if hardstop_col else 0
    cnt_gate_a = _cnt_ng("GateA_AFT_MIN_2p70_PASS")
    cnt_gate_b = _cnt_ng("GateB_FWD_MAX_2p70_CD_PASS")

    # UKC N/A count
    na_ukc = 0
    if "Gate_UKC" in qa.columns:
        na_ukc = int((qa["Gate_UKC"].astype(str).str.upper() == "N/A").sum())

    # HardStop detailed summary
    hardstop_stages = []
    hardstop_reason_to_stages = defaultdict(list)

    if hardstop_col and hardstop_col in qa.columns:
        hardstop_mask = qa[hardstop_col].astype(str).apply(lambda x: _is_true(x))
        hardstop_rows = qa[hardstop_mask]

        for _, row in hardstop_rows.iterrows():
            stg = _norm(row.get("Stage", "")) or "(UnknownStage)"
            hardstop_stages.append(stg)

            # Reason extraction
            reason = (
                _norm(row.get(hardstop_reason_col))
                if hardstop_reason_col and hardstop_reason_col in qa.columns
                else ""
            )

            if not reason:
                # Fallback: derive reason from other columns
                fbmin = row.get("Freeboard_Min_m")
                try:
                    fbmin_v = float(fbmin) if pd.notna(fbmin) else None
                except Exception:
                    fbmin_v = None
                if fbmin_v is not None and fbmin_v < 0:
                    reason = "OverDepth/DeckWet"
                elif hydro_gate_col and _is_ng_gate(row.get(hydro_gate_col)):
                    reason = "HydroOutOfRange"
                else:
                    reason = "HardStop(Unspecified)"

            hardstop_reason_to_stages[reason].append(stg)

    # Heuristics
    heuristics: List[str] = []
    if tanks_total > 0 and tanks_zero == tanks_total:
        heuristics.append(
            "- **SSOT Current_tê°€ ì „ íƒ±í¬ 0.0**: Deballast(Discharge) ê¸°ë°˜ í•´ê²°ì´ ë¶ˆê°€/ì™œê³¡ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. (ì„¼ì„œ/ì´ˆê¸°ê°’ ì£¼ìž… í•„ìš”)"
        )
    elif tanks_total > 0 and (tanks_zero / float(tanks_total)) >= 0.95:
        heuristics.append(
            f"- **SSOT Current_tê°€ ëŒ€ë¶€ë¶„ 0.0** ({tanks_zero}/{tanks_total}): Gate Solver/Optimizer ê²°ê³¼ê°€ ì‹¤ì œ ìš´ìš©ê³¼ ë¶ˆì¼ì¹˜í•  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        )

    if "Freeboard_Min_m" in qa.columns:
        fb_min = pd.to_numeric(qa["Freeboard_Min_m"], errors="coerce").min()
        if pd.notna(fb_min) and float(fb_min) < 0:
            heuristics.append(
                "- **Freeboard_Min_m < 0**: ì¼ë¶€ Stageì—ì„œ Draftê°€ Molded Depthë¥¼ ì´ˆê³¼(ë˜ëŠ” ìž…ë ¥/ê³„ì‚° ë¶ˆì¼ì¹˜)í–ˆìŠµë‹ˆë‹¤. Stage ìž…ë ¥/í•˜ì¤‘/ìˆ˜ì¡°ì„  ë°ì´í„° ìž¬ê²€ì¦ í•„ìš”."
            )

    if "Gate_UKC" in qa.columns:
        ukc_vals = set(qa["Gate_UKC"].astype(str).str.upper().unique().tolist())
        if ukc_vals.issubset({"N/A", "NA", "", "NONE"}):
            heuristics.append(
                "- **UKC Gate ë¯¸í‰ê°€(N/A)**: forecast_tide / depth_ref / ukc_min ìž…ë ¥ì´ ì—†ìœ¼ë©´ UKC ì›ì¸ ë¶„ì„ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."
            )

    # Add Hydro Range and HardStop heuristics
    if hydro_gate_col and cnt_hydro > 0:
        heuristics.append(
            f"- **Hydro Range ìœ„ë°˜ ê°ì§€**: `{hydro_gate_col}` FAIL. Hydro_Table_Engineering.json ë²”ìœ„ í™•ìž¥ ë˜ëŠ” Stage Displacement ìž…ë ¥ ìž¬ê²€ì¦ í•„ìš”."
        )
    if hardstop_col and cnt_hardstop > 0:
        heuristics.append(
            f"- **HardStop ê°ì§€**: `{hardstop_col}`=TRUE Stage ì¡´ìž¬. ì´ ì¼€ì´ìŠ¤ëŠ” Solver/Optimizerë¡œ í•´ê²°í•˜ê¸° ì „ì— ìž…ë ¥/ì •ì˜ì—­(DeckWet, Hydro out-of-range ë“±)ë¶€í„° ì •ì • í•„ìš”."
        )

    if not heuristics:
        heuristics.append(
            "- ìžë™ ë¶„ë¥˜ ê°€ëŠ¥í•œ ì£¼ìš” ì›ì¸ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ìž…ë ¥ê°’/ì œì•½/ëª©í‘œ ìž¬í™•ì¸ í•„ìš”)"
        )

    # Build a compact violations table (Markdown) with extended columns
    view_cols = [
        "Stage",
        "Gate_FWD_Max",
        "FWD_Margin_m",
        "Gate_AFT_Min",
        "AFT_Margin_m",
        "Gate_Freeboard",
        "Freeboard_Min_m",
        "Gate_UKC",
    ]
    # Add split 2.70m gate columns
    view_cols.extend(
        [
            "GateA_AFT_MIN_2p70_PASS",
            "GateA_AFT_MIN_2p70_Margin_m",
            "GateB_FWD_MAX_2p70_CD_PASS",
            "GateB_FWD_MAX_2p70_CD_Margin_m",
            "GateB_FWD_MAX_2p70_CD_applicable",
        ]
    )
    # Add extended gate columns if they exist
    if hydro_gate_col and hydro_gate_col not in view_cols:
        view_cols.append(hydro_gate_col)
    if hardstop_col and hardstop_col not in view_cols:
        view_cols.append(hardstop_col)
    if hardstop_reason_col and hardstop_reason_col not in view_cols:
        view_cols.append(hardstop_reason_col)

    cols = [c for c in view_cols if c in qa.columns]
    qa_view = qa[cols].copy()

    # Sort rows by priority: HardStop > HydroRange > other gate fails
    def _row_score(row):
        score = 0
        if hardstop_col and hardstop_col in qa.columns:
            if _is_true(row.get(hardstop_col)):
                score += 1000
        if hydro_gate_col and hydro_gate_col in qa.columns:
            if _is_ng_gate(row.get(hydro_gate_col)):
                score += 200
        # Other gate fails
        for g in base_gate_cols:
            if g in qa.columns and _is_ng_gate(row.get(g)):
                score += 10
        return score

    qa_view["_sort_score"] = qa_view.apply(_row_score, axis=1)
    qa_view = qa_view.sort_values("_sort_score", ascending=False).drop(
        "_sort_score", axis=1
    )

    def _to_md_table(df: pd.DataFrame, max_rows: int = 50) -> str:
        df2 = df.head(max_rows)
        return df2.to_markdown(index=False)

    md = []
    md.append("# Gate FAIL ì›ì¸ë³„ ìžë™ ë¦¬í¬íŠ¸")
    md.append("")
    md.append(f"- **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md.append(f"- **Site:** {site}")
    md.append(
        f"- **Profile:** {str(profile_path) if profile_path else 'N/A (defaults/CLI)'}"
    )
    md.append(f"- **Stage QA:** {stage_qa_csv.name}")
    md.append(f"- **Tank SSOT:** {tank_ssot_csv.name}")
    md.append("")
    md.append("## 1) Gate ìœ„ë°˜ ìš”ì•½")
    md.append("")
    md.append("### Exec Summary (Captain vs Mammoet ê¸°ì¤€ 2.70m ì •ì˜)")
    md.append("")
    md.append(
        f"* **Captain(ì„ ìž¥) ê¸°ì¤€ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m:** **AFT ìµœì†Œ í˜ìˆ˜(AFT draft â‰¥ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m)** â€” ë¹„ìƒ ì‹œ **í”„ë¡œíŽ ëŸ¬ íš¨ìœ¨/ì¶”ì§„ í™•ë³´** ëª©ì ."
    )
    md.append(
        f"* **Mammoet(ê³„ì‚°/ì‹œë®¬ë ˆì´ì…˜/í”Œëžœ) ê¸°ì¤€ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m:** **FWD í˜ìˆ˜ ìƒí•œ(FWD draft â‰¤ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m, Chart Datum reference)** â€” **Critical RoRo ë‹¨ê³„ì—ì„œ Forward draftë¥¼ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m limit ë‚´**ë¡œ ìœ ì§€."
    )
    md.append(
        f"* ë”°ë¼ì„œ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}mëŠ” ë‹¨ì¼ ì¡°ê±´ì´ ì•„ë‹ˆë¼ **â€œAFT_MIN(â‰¥{CAPTAIN_AFT_MIN_DRAFT_M:.2f})â€ + â€œFWD_MAX(â‰¤{MAMMOET_FWD_MAX_DRAFT_M_CD:.2f})â€ 2ê°œ Gate**ë¡œ ë¶„ë¦¬í•´ í‘œê¸°í•œë‹¤."
    )
    md.append("")
    md.append(
        f"ENG-KR 1L: *Captain = AFT minimum {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m; Mammoet = FWD limit {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m (critical RoRo, chart datum referenced).*"
    )
    md.append("")
    md.append("### Visual-first (Gate ì •ì˜í‘œ â€” ë¬¸ì„œ/ê³„ì‚°ì— ê·¸ëŒ€ë¡œ ë³µë¶™)")
    md.append("")
    md.append(
        "| Owner/Source | Gate Name | Definition | Direction | When applied | Evidence |"
    )
    md.append("|---|---|---|---|---|---|")
    md.append(
        f"| **Captain** | **AFT_MIN_DRAFT** | AFT draft shall be **â‰¥ {CAPTAIN_AFT_MIN_DRAFT_M:.2f}m** for propeller effectiveness in emergency | Min | Emergency propulsion í•„ìš” ìš´ì˜êµ¬ê°„(íŠ¹ížˆ RoRo í¬í•¨) | |"
    )
    md.append(
        f"| **Mammoet / Ballast Plan** | **FWD_MAX_DRAFT (CD ref)** | Forward draft (Chart Datum referenced) shall be **â‰¤ {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}m** during **critical RoRo stages** | Max | Critical RoRo stages (ëž¨í”„/ë¡¤ì˜¨ í”¼í¬ êµ¬ê°„) | |"
    )
    md.append("")
    md.append("### (ìŠ¹ì¸/ê²€í† ìš©) â€œChart Datumâ€ì´ ë¶™ëŠ” ì´ìœ  â€” ì •ì˜ë§Œ")
    md.append("")
    md.append(
        "* **Chart Datum**ì€ í•´ë„ ìˆ˜ì‹¬(Charted depth)ê³¼ ì¡°ìœ„(Height of tide)ì˜ ê¸°ì¤€ë©´ìœ¼ë¡œ, **charted depth + tide height**ë¡œ í•´ë‹¹ ì‹œê°„ì˜ ìˆ˜ì‹¬ì„ ì‚°ì •í•  ìˆ˜ ìžˆë‹¤. ([IHO C-51](https://docs.iho.int/iho_pubs/CB/C_51/C_51_Ed500_062014.pdf))"
    )
    md.append(
        "* BLU Code Shipâ€“Shore Safety ChecklistëŠ” **berth available depth** ë° **arrival/departure draft**ë¥¼ shipâ€“shoreê°€ í•©ì˜í•´ ê¸°ìž…í•˜ë„ë¡ ìš”êµ¬í•œë‹¤. ([IMO BLU Code checklist](https://www.imorules.com/GUID-87E4BBE7-3CE1-4BDC-92F6-CDFDC8850810.html))"
    )
    md.append("")
    md.append("### ìš´ì˜ ì ìš© ë£°(í˜¼ì„  ë°©ì§€ 3ì¤„)")
    md.append("")
    md.append(
        f"1. ë¬¸ì„œ/ë¦¬í¬íŠ¸ì—ëŠ” ë°˜ë“œì‹œ **AFT_MIN {CAPTAIN_AFT_MIN_DRAFT_M:.2f}** ê³¼ **FWD_MAX {MAMMOET_FWD_MAX_DRAFT_M_CD:.2f}**ë¥¼ **ê°ê° ë³„ë„ Gateë¡œ í‘œê¸°**"
    )
    md.append(
        "2. Stageë³„ë¡œ ì–´ë–¤ Gateê°€ â€œê°•ì œ(applicable)â€ì¸ì§€ ëª…ì‹œ(íŠ¹ížˆ MammoetëŠ” â€œcritical RoRo stagesâ€ë§Œ ì ìš©)"
    )
    md.append(
        "3. â€œChart Datumâ€ ê´€ë ¨ ê³„ì‚°ì€ **ë™ì¼ datum ê¸°ë°˜ì˜ charted depth + ê³µì‹ tide** ì¡°í•©ìœ¼ë¡œë§Œ ìˆ˜í–‰(ìŠ¹ì¸ ëŒ€ì‘)"
    )
    md.append("")
    md.append("### Gate ìœ„ë°˜ ìš”ì•½ (Counts)")
    md.append("")
    md.append(f"- FWD_Max: **{cnt_fwd}** stage(s)")
    md.append(f"- AFT_Min: **{cnt_aft}** stage(s)")
    md.append(f"- Freeboard: **{cnt_fb}** stage(s)")
    md.append(f"- UKC: **{cnt_ukc}** stage(s) (N/A={na_ukc})")
    md.append(f"- GateA_AFT_MIN_2p70 (Captain): **{cnt_gate_a}** stage(s)")
    md.append(f"- GateB_FWD_MAX_2p70_CD (Mammoet): **{cnt_gate_b}** stage(s)")
    if hydro_gate_col:
        md.append(
            f"- {hydro_gate_col}: **{cnt_hydro}** stage(s) (Hydro Table range out-of-domain ê°ì§€)"
        )
    if hardstop_col:
        md.append(
            f"- {hardstop_col}: **{cnt_hardstop}** stage(s) (HardStop=TRUE â†’ ì¦‰ì‹œ ë°ì´í„°/ì •ì˜ì—­ ì ê²€ ê¶Œê³ )"
        )
    md.append("")

    # HardStop Summary Section (if exists)
    if hardstop_col and cnt_hardstop > 0:
        md.append("## 2) HardStop Summary")
        md.append("")
        md.append(f"- HardStop Count: **{cnt_hardstop} / {len(qa)}**")
        if hardstop_stages:
            stages_display = hardstop_stages[:50]
            if len(hardstop_stages) > 50:
                md.append(f"- HardStop Stages: {', '.join(stages_display)} ...")
            else:
                md.append(f"- HardStop Stages: {', '.join(stages_display)}")
        md.append("")

        if hardstop_reason_to_stages:
            md.append("### 2.1 HardStop Reason Breakdown")
            md.append("")
            md.append("| Reason | Count | Stages (up to 20) |")
            md.append("|---|---:|---|")
            for reason, stgs in sorted(
                hardstop_reason_to_stages.items(), key=lambda x: (-len(x[1]), x[0])
            ):
                stages_list = stgs[:20]
                stages_str = ", ".join(stages_list)
                if len(stgs) > 20:
                    stages_str += " ..."
                md.append(f"| {reason} | {len(stgs)} | {stages_str} |")
        md.append("")

    section_num_ssot = "2" if not (hardstop_col and cnt_hardstop > 0) else "3"
    md.append(f"## {section_num_ssot}) SSOT(Current_t) ìƒíƒœ ìš”ì•½")
    md.append("")
    md.append(f"- Tanks: {tanks_total}")
    md.append(f"- Current_t == 0.0: {tanks_zero}/{tanks_total}")
    md.append(f"- Total Current_t (sum): {tanks_sum:.3f} t")
    if sensor_stats:
        sensor_section = "2.1" if not (hardstop_col and cnt_hardstop > 0) else "3.1"
        md.append("")
        md.append(f"### {sensor_section} Sensor Sync ì ìš© ê²°ê³¼")
        md.append("")
        md.append(f"- Sensor CSV: {sensor_stats.get('sensor_csv')}")
        md.append(f"- Strategy: {sensor_stats.get('strategy')}")
        md.append(f"- Value mode: {sensor_stats.get('value_mode')}")
        md.append(f"- Updated (exact): {sensor_stats.get('updated_exact')}")
        md.append(f"- Updated (group): {sensor_stats.get('updated_group')}")
        if sensor_stats.get("bad_value_rows"):
            md.append(
                f"- Bad value rows (sample): {sensor_stats.get('bad_value_rows')}"
            )

    if ukc_inputs:
        ukc_section = "3" if not (hardstop_col and cnt_hardstop > 0) else "4"
        md.append("")
        md.append(f"## {ukc_section}) UKC ìž…ë ¥ ìƒíƒœ")
        md.append("")
        for k, v in ukc_inputs.items():
            md.append(f"- {k}: {v}")

    stage_section = "4" if not (hardstop_col and cnt_hardstop > 0) else "5"
    md.append("")
    md.append(f"## {stage_section}) Stageë³„ Gate ìƒíƒœ")
    md.append("")
    md.append(_to_md_table(qa_view))

    heuristics_section = "5" if not (hardstop_col and cnt_hardstop > 0) else "6"
    md.append("")
    md.append(f"## {heuristics_section}) ìžë™ ì›ì¸ ì¶”ì •(Heuristics)")
    md.append("")
    md.extend(heuristics)

    action_section = "6" if not (hardstop_col and cnt_hardstop > 0) else "7"
    md.append("")
    md.append(f"## {action_section}) ê¶Œê³  ì¡°ì¹˜")
    md.append("")
    md.append(
        "- **Current_t ì‹¤ì¸¡/ì„¼ì„œ ê°’ ì£¼ìž… í›„ ìž¬ì‹¤í–‰** (Stage QA â†’ Solver â†’ Optimizer ìˆœ)"
    )
    md.append(
        "- **Site Profile(AGI/DAS)ë¡œ Gate/íŽŒí”„ìœ¨/UKC ìž…ë ¥ê°’ì„ ë¶„ë¦¬ ê´€ë¦¬** (ì½”ë“œ ìˆ˜ì • ì—†ì´ JSON ìˆ˜ì •)"
    )
    md.append(
        "- **Freeboard ìŒìˆ˜ ë°œìƒ ì‹œ Stage í•˜ì¤‘/ìˆ˜ì¡°ì„  ìž…ë ¥ ìž¬ê²€ì¦** (Draft > Molded Depth ì¼€ì´ìŠ¤)"
    )
    if hardstop_col and cnt_hardstop > 0:
        md.append(
            "- **HardStop ë°œìƒ Stage: ìž…ë ¥ ë°ì´í„°/ì •ì˜ì—­(Hydro Table ë²”ìœ„ ë“±) ìž¬ê²€ì¦ í›„ ìž¬ì‹¤í–‰** (Solver/Optimizer ì‹¤í–‰ ì „ í•„ìˆ˜)"
        )
    md.append("")

    out_md.write_text("\n".join(md), encoding="utf-8")
    return out_md


def append_solver_section_to_gate_report(
    report_md: Path,
    solver_out_summary: Path,
    solver_out_plan: Path,
    solver_out_stage_plan: Path,
) -> None:
    """Append solver output pointers + summary to an existing gate report."""
    if not report_md.exists():
        return

    lines = []
    lines.append("\n---\n")
    lines.append("## 7) Step 3 (Gate Solver) ê²°ê³¼ ìš”ì•½\n")
    lines.append(
        f"- solver_ballast_summary.csv: {solver_out_summary.name} ({'OK' if solver_out_summary.exists() else 'MISSING'})"
    )
    lines.append(
        f"- solver_ballast_plan.csv: {solver_out_plan.name} ({'OK' if solver_out_plan.exists() else 'MISSING'})"
    )
    lines.append(
        f"- solver_ballast_stage_plan.csv: {solver_out_stage_plan.name} ({'OK' if solver_out_stage_plan.exists() else 'MISSING'})\n"
    )

    if solver_out_summary.exists():
        try:
            s = pd.read_csv(solver_out_summary, encoding="utf-8-sig")
            if not s.empty:
                row = s.iloc[0].to_dict()
                keys = [
                    "New_FWD_m",
                    "New_AFT_m",
                    "Freeboard_MIN_m",
                    "UKC_m",
                    "viol_fwd_max_m",
                    "viol_aft_min_m",
                    "viol_fb_min_m",
                    "viol_ukc_min_m",
                ]
                lines.append("### Solver Summary (first row)\n")
                for k in keys:
                    if k in row:
                        lines.append(f"- {k}: {row.get(k)}")
        except Exception as e:
            lines.append(
                f"- [WARN] Solver summary parse failed: {type(e).__name__}: {e}"
            )
    report_md.write_text(
        report_md.read_text(encoding="utf-8") + "\n".join(lines), encoding="utf-8"
    )


def append_dnv_mitigation_section_to_gate_report(
    report_md: Path,
    stage_qa_csv: Path,
    propeller_diameter_m: float = 1.38,
) -> None:
    """Append DNV mitigation measures for incomplete propeller immersion (Gate-A failures)."""
    if not report_md.exists() or not stage_qa_csv.exists():
        return

    try:
        import numpy as np

        existing = report_md.read_text(encoding="utf-8")
        if "DNV Mitigation Measures (Incomplete Propeller Immersion)" in existing:
            return
    except Exception:
        existing = ""

    try:
        qa = pd.read_csv(stage_qa_csv, encoding="utf-8-sig")

        gate_a_col = None
        if "GateA_AFT_MIN_2p70_PASS" in qa.columns:
            gate_a_col = "GateA_AFT_MIN_2p70_PASS"
        elif "Gate_AFT_Min" in qa.columns:
            gate_a_col = "Gate_AFT_Min"
        if gate_a_col is None:
            return

        def _is_fail(val) -> bool:
            if isinstance(val, (bool, np.bool_)):
                return not bool(val)
            s = str(val).strip().lower()
            return s in ("ng", "fail", "false", "0")

        failing_stages = qa[qa[gate_a_col].apply(_is_fail)]
        if failing_stages.empty:
            return

        lines = []
        lines.append("\n---\n")
        lines.append("## 8) DNV Mitigation Measures (Incomplete Propeller Immersion)\n")
        lines.append(
            "**Reference:** DNV Guidance on incomplete propeller immersion risk\n"
        )
        lines.append(
            "**Risk:** Aft bearing damage and propulsion loss due to excessive eccentric thrust.\n"
        )
        lines.append("### Stages Requiring Mitigation:\n")

        for _, row in failing_stages.iterrows():
            stage_name = str(row.get("Stage", "Unknown"))
            aft_draft = row.get("Draft_AFT_m", "N/A")
            aft_min = row.get("AFT_MIN_m", 2.70)
            deficit = None
            if pd.notna(aft_draft) and pd.notna(aft_min):
                try:
                    deficit = float(aft_min) - float(aft_draft)
                except Exception:
                    deficit = None

            lines.append(f"#### {stage_name}")
            lines.append(f"- AFT Draft: {aft_draft} m (Required: {aft_min} m)")
            if deficit is not None:
                lines.append(f"- Deficit: {deficit:.2f} m")
            lines.append("")
            lines.append("Required Mitigation Measures (DNV):")
            lines.append("1. RPM/Power reduction (specify % based on engine guidance)")
            lines.append("2. Steering angle limitation (specify degrees)")
            lines.append(
                "3. Tug standby (immediately available; operational control with tug)"
            )
            lines.append("4. Abort criteria:")
            lines.append("   - AFT draft falls below 2.50 m")
            lines.append(
                "   - Propeller shaft bearing temperature exceeds normal range"
            )
            lines.append("   - Loss of propulsion effectiveness detected")
            lines.append("   - Weather exceeds operational limits")
            lines.append("5. Monitoring: 4-corner draft + bearing temperature")
            lines.append("")

        lines.append("### ITTC Shaft Centreline Immersion Note")
        lines.append(
            "**Important:** Record shaft centreline immersion (not AFT draft) in approval docs."
        )
        lines.append(f"- Propeller Diameter (D): {propeller_diameter_m} m")
        lines.append(f"- Minimum: 1.5D = {1.5 * propeller_diameter_m:.2f} m")
        lines.append(f"- Recommended: 2.0D = {2.0 * propeller_diameter_m:.2f} m")
        lines.append("- Calculation: immersion_shaftCL = draft_at_prop - z_shaftCL")
        lines.append(
            "- Do not confuse AFT draft with shaft immersion in approval documentation."
        )

        report_md.write_text(existing + "\n".join(lines), encoding="utf-8")
    except Exception as e:
        print(
            f"[WARN] Failed to append DNV mitigation section: {type(e).__name__}: {e}"
        )


def generate_tug_operational_sop_md(
    out_md: Path,
    stage_qa_csv: Path,
    site: str = "AGI",
) -> Path:
    """Generate TUG-assisted operational SOP document (DNV-ST-N001 aligned)."""
    try:
        import numpy as np

        qa = pd.read_csv(stage_qa_csv, encoding="utf-8-sig")

        gate_a_col = None
        if "GateA_AFT_MIN_2p70_PASS" in qa.columns:
            gate_a_col = "GateA_AFT_MIN_2p70_PASS"
        elif "Gate_AFT_Min" in qa.columns:
            gate_a_col = "Gate_AFT_Min"
        if gate_a_col is None:
            return out_md

        def _is_fail(val) -> bool:
            if isinstance(val, (bool, np.bool_)):
                return not bool(val)
            s = str(val).strip().lower()
            return s in ("ng", "fail", "false", "0")

        failing_stages = qa[qa[gate_a_col].apply(_is_fail)]
        if failing_stages.empty:
            return out_md

        lines = []
        lines.append("# TUG-Assisted Operational SOP (DNV-ST-N001 aligned)\n")
        lines.append(f"Site: {site}")
        lines.append(f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append("## Executive Summary")
        lines.append(
            "This SOP defines tug-assisted procedures for stages where AFT draft is below the propulsion gate."
        )
        lines.append(
            "Operational philosophy: no reliance on main propulsion during incomplete immersion."
        )
        lines.append("Control authority: tug has operational control.\n")

        lines.append("## 1) Pre-RoRo Phase (T-3h to T-1h)")
        lines.append("1. Tug deployment: minimum 2 tugs (bow + stern), 1 standby")
        lines.append(
            "2. Main propulsion restriction: minimum RPM/power, limited steering angle"
        )
        lines.append(
            "3. Abort briefing: AFT draft below 2.50 m, bearing temp alarm, loss of thrust"
        )
        lines.append("")

        lines.append("## 2) RoRo / Ramp / Roll-on Phase (T0 to T+X)")
        lines.append(
            "1. Tug holds heading and position; LCT propulsion is emergency backup only"
        )
        lines.append("2. Mooring + tug for hang-up/trim change response")
        lines.append(
            "3. Real-time monitoring: 4-corner draft, freeboard, weather, bearing temp"
        )
        lines.append("")

        lines.append("## 3) Post-RoRo Phase (T+X to T+X+2h)")
        lines.append("1. Recover AFT draft >= 2.70 m if possible (deballast/transfer)")
        lines.append(
            "2. Restore propulsion only after safe draft or maintain tug escort"
        )
        lines.append("")

        lines.append("## 4) Critical Stages Requiring Tug Assistance\n")
        for _, row in failing_stages.iterrows():
            stage_name = str(row.get("Stage", "Unknown"))
            aft_draft = row.get("Draft_AFT_m", "N/A")
            aft_min = row.get("AFT_MIN_m", 2.70)
            freeboard_min = row.get("Freeboard_Min_m", "N/A")
            lines.append(
                f"- {stage_name}: AFT={aft_draft} m (req {aft_min} m), "
                f"Freeboard(min)={freeboard_min} m"
            )

        lines.append("\n## 5) DNV-ST-N001 Compliance Checklist")
        lines.append("- Design calculations (ballast plan, stability, trim)")
        lines.append("- Operational procedures (this SOP)")
        lines.append(
            "- Monitoring requirements (draft, freeboard, weather, bearing temp)"
        )
        lines.append("- Load cases (critical stages documented)")
        lines.append(
            "- Risk assessment and mitigation (RPM limit, steering limit, tug standby)"
        )

        out_md.write_text("\n".join(lines), encoding="utf-8")
        return out_md
    except Exception as e:
        print(f"[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}")
        return out_md


# -----------------------------------------------------------------------------
# Converters
# -----------------------------------------------------------------------------
def convert_tank_catalog_json_to_solver_csv(
    tank_catalog_json: Path,
    out_csv: Path,
    pump_rate_tph: float = 100.0,
    include_keywords: Optional[List[str]] = None,
) -> Path:
    """
    Convert tank_catalog_from_tankmd.json (object with {"tanks":[...]}) into
    Tank SSOT CSV format for ballast_gate_solver_v4.py.

    IMPORTANT: Coordinate convention (align to ops_final_r3 script):
        x_from_mid_m = MIDSHIP_FROM_AP_M - lcg_from_ap_m
        (+AFT / -FWD)

    The solver requires at least:
        Tank, x_from_mid_m, Current_t, Min_t, Max_t, mode, use_flag, pump_rate_tph, priority_weight
    """
    # #region agent log
    debug_log(
        "convert_tank_catalog_json_to_solver_csv:168",
        "Function entry",
        {"tank_catalog_json": str(tank_catalog_json), "out_csv": str(out_csv)},
        "A",
    )
    # #endregion

    try:
        file_content = tank_catalog_json.read_text(encoding="utf-8")
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:172",
            "File read success",
            {"file_size": len(file_content)},
            "B",
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:175",
            "File read error",
            {
                "error": str(e),
                "error_type": type(e).__name__,
                "traceback": traceback.format_exc(),
            },
            "B",
        )
        # #endregion
        raise

    try:
        obj = json.loads(file_content)
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:181",
            "JSON parse success",
            {"obj_keys": list(obj.keys()) if isinstance(obj, dict) else "not_dict"},
            "A",
        )
        # #endregion
    except json.JSONDecodeError as e:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:185",
            "JSON parse error",
            {
                "error": str(e),
                "error_type": type(e).__name__,
                "position": getattr(e, "pos", None),
            },
            "A",
        )
        # #endregion
        raise
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:189",
            "JSON parse unexpected error",
            {"error": str(e), "error_type": type(e).__name__},
            "A",
        )
        # #endregion
        raise

    tanks = obj.get("tanks", [])
    if not isinstance(tanks, list):
        raise ValueError(f"Unexpected tank catalog structure: {tank_catalog_json}")

    include_keywords = include_keywords or ["BALLAST", "VOID", "FWB", "FW", "DB"]
    # #region agent log
    debug_log(
        "convert_tank_catalog_json_to_solver_csv:195",
        "Before tank loop",
        {"tanks_count": len(tanks), "include_keywords": include_keywords},
        "C",
    )
    # #endregion

    rows = []
    for idx, t in enumerate(tanks):
        tid = str(t.get("id", "")).strip()
        if not tid or tid == "**Total**":
            continue

        cat = str(t.get("category", "")).strip()
        try:
            cap_t_val = t.get("cap_t", 0.0) or 0.0
            cap_t = float(cap_t_val)
            # #region agent log
            debug_log(
                f"convert_tank_catalog_json_to_solver_csv:207_tank_{idx}",
                "cap_t conversion",
                {"tid": tid, "cap_t_val": cap_t_val, "cap_t": cap_t},
                "C",
            )
            # #endregion
        except (ValueError, TypeError) as e:
            # #region agent log
            debug_log(
                f"convert_tank_catalog_json_to_solver_csv:212_tank_{idx}",
                "cap_t conversion error",
                {"tid": tid, "cap_t_val": cap_t_val, "error": str(e)},
                "C",
            )
            # #endregion
            cap_t = 0.0

        try:
            lcg_val = t.get("lcg_m", 0.0) or 0.0
            lcg_from_ap = float(lcg_val)
            # #region agent log
            debug_log(
                f"convert_tank_catalog_json_to_solver_csv:220_tank_{idx}",
                "lcg_m conversion",
                {"tid": tid, "lcg_val": lcg_val, "lcg_from_ap": lcg_from_ap},
                "C",
            )
            # #endregion
        except (ValueError, TypeError) as e:
            # #region agent log
            debug_log(
                f"convert_tank_catalog_json_to_solver_csv:225_tank_{idx}",
                "lcg_m conversion error",
                {"tid": tid, "lcg_val": lcg_val, "error": str(e)},
                "C",
            )
            # #endregion
            lcg_from_ap = 0.0
        x_from_mid = MIDSHIP_FROM_AP_M - lcg_from_ap

        key_blob = f"{tid} {cat}".upper()
        use_flag = "Y" if any(k in key_blob for k in include_keywords) else "N"

        # Priority heuristic: favor the usual discharge candidates if present
        pr = 5.0
        if "FWB2" in tid.upper():
            pr = 1.0
        elif "VOIDDB2" in tid.upper():
            pr = 2.0
        elif "VOIDDB1" in tid.upper():
            pr = 3.0

        rows.append(
            {
                "Tank": tid,
                "Capacity_t": cap_t,
                "x_from_mid_m": round(x_from_mid, 4),
                "Current_t": 0.0,
                "Min_t": 0.0,
                "Max_t": cap_t,
                "mode": "FILL_DISCHARGE",
                "use_flag": use_flag,
                "pump_rate_tph": float(pump_rate_tph),
                "priority_weight": pr,
            }
        )

    # #region agent log
    debug_log(
        "convert_tank_catalog_json_to_solver_csv:235",
        "After tank loop",
        {"rows_count": len(rows)},
        "C",
    )
    # #endregion

    if not rows:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:239",
            "No rows produced error",
            {},
            "C",
        )
        # #endregion
        raise ValueError("No tank rows produced. Check tank catalog content/filters.")

    try:
        df = pd.DataFrame(rows)
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:245",
            "DataFrame created",
            {"columns": list(df.columns), "shape": df.shape},
            "D",
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:249",
            "DataFrame creation error",
            {"error": str(e), "error_type": type(e).__name__},
            "D",
        )
        # #endregion
        raise

    try:
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:255",
            "CSV write success",
            {"out_csv": str(out_csv)},
            "D",
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_tank_catalog_json_to_solver_csv:259",
            "CSV write error",
            {"error": str(e), "error_type": type(e).__name__, "out_csv": str(out_csv)},
            "D",
        )
        # #endregion
        raise

    return out_csv


def convert_hydro_engineering_json_to_solver_csv(
    hydro_json: Path,
    out_csv: Path,
    lbp_m: float = LPP_M,
) -> Path:
    """
    Convert Hydro_Table_Engineering.json into the solver-required columns.

    solver expects columns:
      Tmean_m, TPC_t_per_cm, MTC_t_m_per_cm, LCF_m, LBP_m
    In your engineering table, MTC may be stored as MCTC_t_m_per_cm and LCF as LCF_m_from_midship.
    """
    # #region agent log
    debug_log(
        "convert_hydro_engineering_json_to_solver_csv:237",
        "Function entry",
        {"hydro_json": str(hydro_json), "out_csv": str(out_csv)},
        "A",
    )
    # #endregion

    try:
        file_content = hydro_json.read_text(encoding="utf-8")
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:243",
            "File read success",
            {"file_size": len(file_content)},
            "B",
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:246",
            "File read error",
            {"error": str(e), "error_type": type(e).__name__},
            "B",
        )
        # #endregion
        raise

    try:
        raw = json.loads(file_content)
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:252",
            "JSON parse success",
            {"raw_type": type(raw).__name__},
            "A",
        )
        # #endregion
    except json.JSONDecodeError as e:
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:255",
            "JSON parse error",
            {"error": str(e), "position": getattr(e, "pos", None)},
            "A",
        )
        # #endregion
        raise
    if isinstance(raw, list):
        df = pd.DataFrame(raw)
    elif isinstance(raw, dict) and "rows" in raw:
        df = pd.DataFrame(raw.get("rows", []))
    else:
        # last resort: try dict itself
        df = pd.DataFrame(raw)

    # Normalize columns
    colmap = {}
    if "MCTC_t_m_per_cm" in df.columns and "MTC_t_m_per_cm" not in df.columns:
        colmap["MCTC_t_m_per_cm"] = "MTC_t_m_per_cm"
    if "LCF_m_from_midship" in df.columns and "LCF_m" not in df.columns:
        colmap["LCF_m_from_midship"] = "LCF_m"
    df = df.rename(columns=colmap)

    # Required columns
    required = ["Tmean_m", "TPC_t_per_cm", "MTC_t_m_per_cm", "LCF_m"]
    # #region agent log
    debug_log(
        "convert_hydro_engineering_json_to_solver_csv:262",
        "Before column check",
        {"df_columns": list(df.columns), "required": required},
        "E",
    )
    # #endregion
    missing = [c for c in required if c not in df.columns]
    if missing:
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:267",
            "Missing columns error",
            {"missing": missing, "available_columns": list(df.columns)},
            "E",
        )
        # #endregion
        raise ValueError(f"Hydro table missing required columns: {missing}")

    if "LBP_m" not in df.columns:
        df["LBP_m"] = float(lbp_m)

    try:
        df = df[["Tmean_m", "TPC_t_per_cm", "MTC_t_m_per_cm", "LCF_m", "LBP_m"]].copy()
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:275",
            "DataFrame column selection",
            {"final_columns": list(df.columns), "shape": df.shape},
            "E",
        )
        # #endregion
    except KeyError as e:
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:279",
            "Column selection KeyError",
            {"error": str(e), "available_columns": list(df.columns)},
            "E",
        )
        # #endregion
        raise

    try:
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:285",
            "CSV write success",
            {"out_csv": str(out_csv)},
            "D",
        )
        # #endregion
    except Exception as e:
        # #region agent log
        debug_log(
            "convert_hydro_engineering_json_to_solver_csv:289",
            "CSV write error",
            {"error": str(e), "error_type": type(e).__name__},
            "D",
        )
        # #endregion
        raise

    return out_csv


def build_stage_table_from_stage_results(
    stage_results_csv: Path,
    out_csv: Path,
    fwd_max_m: float,
    aft_min_m: float,
    aft_max_m_for_optimizer: float,
    trim_abs_limit_m: float,
    critical_only: bool = False,
    critical_stage_list: Optional[List[str]] = None,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    forecast_tide_m: Optional[float] = None,
    depth_ref_m: Optional[float] = None,
    ukc_min_m: Optional[float] = None,
    d_vessel_m: float = D_VESSEL_M,
    hydro_json_path: Optional[Path] = None,
    strict_hardstop: bool = False,
    draft_tol_m: float = 1e-6,
    hydro_disp_tol_t: float = 1e-3,
) -> Path:
    """
    stage_results.csv -> stage_table_unified.csv

    PATCH (HardStop + ì‹ ê·œ ê²Œì´íŠ¸ ì¤€ë¹„):
      - stage_resultsì—ì„œ Disp_t/Tmean_m/Trim_cm (ê°€ëŠ¥í•˜ë©´) stage_tableë¡œ carry
      - Hydro table displacement ë²”ìœ„(min/max) ì‚°ì¶œ í›„ stage_tableì— ê¸°ë¡
      - HardStop í”Œëž˜ê·¸(HS_*) ê³„ì‚°
        * strict_hardstop=Trueì´ë©´ ValueErrorë¡œ ì¦‰ì‹œ ì¤‘ë‹¨
        * strict_hardstop=Falseì´ë©´ HS í”Œëž˜ê·¸ë§Œ ë‚¨ê¸°ê³  ë‹¤ìŒ ë‹¨ê³„(QA)ë¡œ ì§„í–‰
    """
    import math
    import numpy as np

    # Compatibility mapping: aft_max_m_for_optimizer -> aft_max_m
    aft_max_m = aft_max_m_for_optimizer

    stage_results_csv = Path(stage_results_csv)
    out_csv = Path(out_csv)

    df = pd.read_csv(stage_results_csv)

    # -----------------------------
    # 1) Flexible column detection
    # -----------------------------
    stage_col = "Stage" if "Stage" in df.columns else None
    if stage_col is None:
        for c in df.columns:
            if str(c).strip().lower() in ("stage_name", "name"):
                stage_col = c
                break
    if stage_col is None:
        raise ValueError(
            f"[HARDSTOP] stage_results.csv missing Stage column. "
            f"Expected one of: Stage / stage_name / name. Columns={list(df.columns)}"
        )

    dfwd_col = (
        "Dfwd_m"
        if "Dfwd_m" in df.columns
        else ("FWD_m" if "FWD_m" in df.columns else None)
    )
    if dfwd_col is None:
        for c in df.columns:
            if str(c).strip().lower() in (
                "fwd draft(m)",
                "fwd draft (m)",
                "fwd_draft_m",
            ):
                dfwd_col = c
                break
    if dfwd_col is None:
        raise ValueError(
            f"[HARDSTOP] stage_results.csv missing Forward draft column. "
            f"Expected one of: Dfwd_m / FWD_m / Fwd Draft(m). Columns={list(df.columns)}"
        )

    daft_col = (
        "Daft_m"
        if "Daft_m" in df.columns
        else ("AFT_m" if "AFT_m" in df.columns else None)
    )
    if daft_col is None:
        for c in df.columns:
            if str(c).strip().lower() in (
                "aft draft(m)",
                "aft draft (m)",
                "aft_draft_m",
            ):
                daft_col = c
                break
    if daft_col is None:
        raise ValueError(
            f"[HARDSTOP] stage_results.csv missing Aft draft column. "
            f"Expected one of: Daft_m / AFT_m / Aft Draft(m). Columns={list(df.columns)}"
        )

    # Optional fields (for traceability + new gate)
    disp_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            "disp_t",
            "disp",
            "displacement_t",
            "displacement",
        ):
            disp_col = c
            break

    tmean_col = None
    for c in df.columns:
        if str(c).strip().lower() in ("tmean_m", "tmean", "mean_draft_m", "mean_draft"):
            tmean_col = c
            break

    trim_col = None
    for c in df.columns:
        if str(c).strip().lower() in ("trim_cm", "trim"):
            trim_col = c
            break

    linkspan_fb_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            "linkspan_freeboard_m",
            "linkspan freeboard (m)",
            "linkspan_freeboard",
        ):
            linkspan_fb_col = c
            break

    ramp_angle_col = None
    for c in df.columns:
        if str(c).strip().lower() in ("ramp_angle_deg", "ramp angle (deg)"):
            ramp_angle_col = c
            break

    deck_flood_col = None
    for c in df.columns:
        if str(c).strip().lower() in (
            "gate_deck_flooding_pass",
            "deck_flooding_pass",
            "gate_deck_flooding",
        ):
            deck_flood_col = c
            break

    # -----------------------------
    # 2) Build stage table (existing required columns)
    # -----------------------------
    # Draft ê°’ ë¬¼ë¦¬ì  ì œí•œ: Molded Depthë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
    draft_fwd_raw = pd.to_numeric(df[dfwd_col], errors="coerce")
    draft_aft_raw = pd.to_numeric(df[daft_col], errors="coerce")

    # Molded Depth ì´ˆê³¼ ë°©ì§€ (ë¬¼ë¦¬ì  ë¶ˆê°€ëŠ¥ ìƒíƒœ ì œê±°)
    draft_fwd_clipped = draft_fwd_raw.clip(upper=float(d_vessel_m))
    draft_aft_clipped = draft_aft_raw.clip(upper=float(d_vessel_m))

    # ì œí•œëœ Stage ë¡œê¹…
    clipped_mask = (draft_fwd_raw > float(d_vessel_m)) | (
        draft_aft_raw > float(d_vessel_m)
    )
    if clipped_mask.any():
        clipped_stages = df.loc[clipped_mask, stage_col].values
        print(
            f"[WARNING] Draft values clipped to D_vessel ({d_vessel_m}m) for {len(clipped_stages)} stages:"
        )
        for stage_name, fwd_raw, aft_raw, fwd_clipped, aft_clipped in zip(
            clipped_stages,
            draft_fwd_raw[clipped_mask],
            draft_aft_raw[clipped_mask],
            draft_fwd_clipped[clipped_mask],
            draft_aft_clipped[clipped_mask],
        ):
            if fwd_raw > float(d_vessel_m) or aft_raw > float(d_vessel_m):
                print(
                    f"  - {stage_name}: FWD {fwd_raw:.2f} -> {fwd_clipped:.2f}m, "
                    f"AFT {aft_raw:.2f} -> {aft_clipped:.2f}m"
                )

    out = pd.DataFrame(
        {
            "Stage": df[stage_col].astype(str).str.strip(),
            "Current_FWD_m": draft_fwd_clipped,
            "Current_AFT_m": draft_aft_clipped,
            "AFT_MIN_m": float(aft_min_m),
            "D_vessel_m": float(d_vessel_m),
        }
    )

    fwd_limit = float(fwd_max_m) if fwd_max_m is not None else float("nan")
    if critical_only:
        if stage_col is None:
            out["FWD_MAX_applicable"] = True
        else:
            out["FWD_MAX_applicable"] = out["Stage"].apply(
                lambda s: _is_critical_stage(
                    s, patterns=critical_stage_list, regex=critical_regex
                )
            )
        out["FWD_MAX_m"] = np.where(out["FWD_MAX_applicable"], fwd_limit, np.nan)
    else:
        out["FWD_MAX_applicable"] = True
        out["FWD_MAX_m"] = fwd_limit

    # Optimizer-only columns (keep existing meaning)
    out["FWD_Limit_m"] = out[
        "FWD_MAX_m"
    ]  # SSOT: same as FWD_MAX_m unless overridden upstream
    out["AFT_Limit_m"] = float(aft_max_m) if aft_max_m is not None else 3.50
    out["Trim_Abs_Limit_m"] = (
        float(trim_abs_limit_m) if trim_abs_limit_m is not None else 0.50
    )

    # Definition-split inputs (optional)
    # í•­ìƒ ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ë˜, ê°’ì´ ì—†ìœ¼ë©´ NaNìœ¼ë¡œ ì„¤ì • (enrich_stage_table_with_tide_ukcì—ì„œ ì±„ì›€)
    if "Forecast_Tide_m" not in out.columns:
        out["Forecast_Tide_m"] = np.nan
    if forecast_tide_m is not None:
        # CLI ê°’ì´ ëª…ì‹œì ìœ¼ë¡œ ì œê³µë˜ë©´ ëª¨ë“  Stageì— ì§ì ‘ ì ìš© (ìš°ì„ ìˆœìœ„ ìµœìƒ)
        # ì´ë ‡ê²Œ í•˜ë©´ enrich_stage_table_with_tide_ukcì—ì„œ stage_tide_csvë³´ë‹¤ CLI ê°’ì´ ìš°ì„ ë¨
        out["Forecast_Tide_m"] = float(forecast_tide_m)
        print(
            f"[OK] Applied forecast_tide_m={forecast_tide_m} to stage_table (CLI override, all stages)"
        )

    if "DepthRef_m" not in out.columns:
        out["DepthRef_m"] = np.nan
    if depth_ref_m is not None:
        out["DepthRef_m"] = out["DepthRef_m"].fillna(float(depth_ref_m))
        print(
            f"[OK] Applied depth_ref_m={depth_ref_m} to stage_table (default for all stages)"
        )

    if "UKC_Min_m" not in out.columns:
        out["UKC_Min_m"] = np.nan
    if ukc_min_m is not None:
        out["UKC_Min_m"] = out["UKC_Min_m"].fillna(float(ukc_min_m))
        print(
            f"[OK] Applied ukc_min_m={ukc_min_m} to stage_table (default for all stages)"
        )

    # Carry trace fields (optional)
    if disp_col is not None:
        out["Input_Disp_t"] = pd.to_numeric(df[disp_col], errors="coerce")
    if tmean_col is not None:
        out["Input_Tmean_m"] = pd.to_numeric(df[tmean_col], errors="coerce")
    if trim_col is not None:
        out["Input_Trim_cm"] = pd.to_numeric(df[trim_col], errors="coerce")
    if linkspan_fb_col is not None:
        out["Linkspan_freeboard_m"] = pd.to_numeric(
            df[linkspan_fb_col], errors="coerce"
        )
    if ramp_angle_col is not None:
        out["Ramp_angle_deg"] = pd.to_numeric(df[ramp_angle_col], errors="coerce")
    if deck_flood_col is not None:
        out["Gate_Deck_Flooding_pass"] = df[deck_flood_col].astype(str)

    # -----------------------------
    # 3) HARDSTOP flags: physical validity
    # -----------------------------
    out["HS_DraftNaN"] = out["Current_FWD_m"].isna() | out["Current_AFT_m"].isna()
    out["HS_DraftNegative"] = (out["Current_FWD_m"] < -draft_tol_m) | (
        out["Current_AFT_m"] < -draft_tol_m
    )
    out["HS_DraftOverDepth"] = (
        out["Current_FWD_m"] > float(d_vessel_m) + draft_tol_m
    ) | (out["Current_AFT_m"] > float(d_vessel_m) + draft_tol_m)

    # -----------------------------
    # 4) HARDSTOP flags: hydro disp range (optional)
    # -----------------------------
    out["Hydro_Disp_Min_t"] = math.nan
    out["Hydro_Disp_Max_t"] = math.nan
    out["HS_HydroOutOfRange"] = False

    def _auto_find_hydro_json(start: Path):
        candidates = [
            start.parent / "bplus_inputs" / "Hydro_Table_Engineering.json",
            start.parent.parent / "bplus_inputs" / "Hydro_Table_Engineering.json",
            start.parent / "Hydro_Table_Engineering.json",
        ]
        for p in candidates:
            if p.exists():
                return p
        return None

    hydro_path = (
        Path(hydro_json_path)
        if hydro_json_path
        else _auto_find_hydro_json(stage_results_csv)
    )

    if disp_col is not None and hydro_path is not None and hydro_path.exists():
        try:
            with open(hydro_path, "r", encoding="utf-8") as f:
                hydro_obj = json.load(f)

            # Normalize to list[dict]
            rows = None
            if isinstance(hydro_obj, list):
                rows = hydro_obj
            elif isinstance(hydro_obj, dict):
                for k in ("rows", "table", "data", "HydroTable"):
                    if k in hydro_obj and isinstance(hydro_obj[k], list):
                        rows = hydro_obj[k]
                        break
                if rows is None and all(
                    isinstance(v, dict) for v in hydro_obj.values()
                ):
                    rows = list(hydro_obj.values())

            disp_vals = []
            if isinstance(rows, list):
                for r in rows:
                    if not isinstance(r, dict):
                        continue
                    for k, v in r.items():
                        if str(k).strip().lower() in (
                            "disp_t",
                            "disp",
                            "displacement_t",
                            "displacement",
                        ):
                            try:
                                disp_vals.append(float(v))
                            except Exception:
                                pass
                            break

            if disp_vals:
                hmin = min(disp_vals)
                hmax = max(disp_vals)
                out["Hydro_Disp_Min_t"] = hmin
                out["Hydro_Disp_Max_t"] = hmax
                out["HS_HydroOutOfRange"] = (
                    out["Input_Disp_t"].isna()
                    | (out["Input_Disp_t"] < hmin - hydro_disp_tol_t)
                    | (out["Input_Disp_t"] > hmax + hydro_disp_tol_t)
                )
        except Exception:
            # If hydro parse fails, leave HS_HydroOutOfRange=False and range NaN
            pass

    out["HS_Any"] = (
        out["HS_DraftNaN"]
        | out["HS_DraftNegative"]
        | out["HS_DraftOverDepth"]
        | out["HS_HydroOutOfRange"]
    )

    out.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # ê¸°ë³¸ì€ "QAì—ì„œ HardStop"ìœ¼ë¡œ ìœ ë„ (strict_hardstop=False)
    if strict_hardstop and bool(out["HS_Any"].any()):
        bad = out.loc[
            out["HS_Any"] == True, ["Stage", "Current_FWD_m", "Current_AFT_m"]
        ].copy()
        if "Input_Disp_t" in out.columns:
            bad["Input_Disp_t"] = out.loc[out["HS_Any"] == True, "Input_Disp_t"].values
        msg = [
            "[HARDSTOP] Invalid stage_results detected while building stage_table_unified.csv",
            f"  - stage_results_csv: {stage_results_csv}",
            f"  - out_csv          : {out_csv}",
            "  - failing stages (Stage, FWD, AFT, Disp if available):",
        ]
        for _, r in bad.iterrows():
            disp_str = (
                f", Disp={r['Input_Disp_t']}" if "Input_Disp_t" in bad.columns else ""
            )
            msg.append(
                f"    * {r['Stage']}: FWD={r['Current_FWD_m']}, AFT={r['Current_AFT_m']}{disp_str}"
            )
        raise ValueError("\n".join(msg))

    return out_csv


def enrich_stage_table_with_tide_ukc(
    stage_table_csv: Path,
    *,
    forecast_tide_m: Optional[float] = None,
    depth_ref_m: Optional[float] = None,
    ukc_min_m: Optional[float] = None,
    squat_m: float = 0.0,
    safety_allow_m: float = 0.0,
    tide_tol_m: float = DEFAULT_TIDE_TOL_M,
    stage_tide_csv_path: Optional[Path] = None,
    tide_table_path: Optional[Path] = None,
    stage_schedule_path: Optional[Path] = None,
    tide_strategy: str = "keep_csv",
) -> Path:
    """
    AGI ê¸°ì¤€: stage_table_unified.csvì— Tide/UKC íŒŒìƒ í•„ë“œ í†µí•©(ì„ íƒ).

    - Forecast_Tide_m ìš°ì„ ìˆœìœ„:
        1) stage_table ë‚´ ê°’(ì¡´ìž¬ ì‹œ)
        2) (--tide_table + --stage_schedule)ë¡œ ë³´ê°„(ì˜µì…˜)
        3) CLI --forecast_tide (fallback)

    - Output columns (ì¶”ê°€/ê°±ì‹ ):
        Tide_required_m, Forecast_tide_m, Tide_margin_m,
        UKC_fwd_m, UKC_aft_m, UKC_min_actual_m,
        Tide_verification, Tide_note,
        Squat_m, SafetyAllow_m (ì—†ìœ¼ë©´ ìƒì„±)

    NOTE:
    - stage_tableì€ Solver ìž…ë ¥ SSOTì´ë©°, ë³¸ í•¨ìˆ˜ëŠ” SSOTì— "íŒŒìƒ í•„ë“œ"ë¥¼ ì¶”ê°€í•˜ëŠ” ì„±ê²©.
    - Solver/QAëŠ” ì—¬ì „ížˆ Forecast_Tide_m/DepthRef_m/UKC_Min_më¥¼ ê¸°ì¤€ìœ¼ë¡œ Gate ê³„ì‚°.
    """
    import numpy as np

    stage_table_csv = Path(stage_table_csv)
    df = pd.read_csv(stage_table_csv, encoding="utf-8-sig")

    # Ensure base numeric inputs
    if "DepthRef_m" not in df.columns:
        df["DepthRef_m"] = np.nan
    df["DepthRef_m"] = pd.to_numeric(df["DepthRef_m"], errors="coerce")
    if depth_ref_m is not None:
        filled_count = df["DepthRef_m"].isna().sum()
        df["DepthRef_m"] = df["DepthRef_m"].fillna(float(depth_ref_m))
        print(
            f"[OK] Applied depth_ref_m={depth_ref_m} to stage_table (filled {filled_count}/{len(df)} rows)"
        )
    else:
        print(f"[WARN] depth_ref_m is None, DepthRef_m will remain empty")

    if "UKC_Min_m" not in df.columns:
        df["UKC_Min_m"] = np.nan
    df["UKC_Min_m"] = pd.to_numeric(df["UKC_Min_m"], errors="coerce")
    if ukc_min_m is not None:
        filled_count = df["UKC_Min_m"].isna().sum()
        df["UKC_Min_m"] = df["UKC_Min_m"].fillna(float(ukc_min_m))
        print(
            f"[OK] Applied ukc_min_m={ukc_min_m} to stage_table (filled {filled_count}/{len(df)} rows)"
        )
    else:
        print(f"[WARN] ukc_min_m is None, UKC_Min_m will remain empty")

    if "Squat_m" not in df.columns:
        df["Squat_m"] = float(squat_m)
    else:
        df["Squat_m"] = pd.to_numeric(df["Squat_m"], errors="coerce").fillna(
            float(squat_m)
        )

    if "SafetyAllow_m" not in df.columns:
        df["SafetyAllow_m"] = float(safety_allow_m)
    else:
        df["SafetyAllow_m"] = pd.to_numeric(
            df["SafetyAllow_m"], errors="coerce"
        ).fillna(float(safety_allow_m))

    if "Forecast_Tide_m" not in df.columns:
        df["Forecast_Tide_m"] = np.nan
    df["Forecast_Tide_m"] = pd.to_numeric(df["Forecast_Tide_m"], errors="coerce")

    # Priority 0: CLI forecast_tide_m (ìµœìš°ì„  - ëª…ì‹œì ìœ¼ë¡œ ì œê³µëœ ê²½ìš°)
    # CLI ê°’ì´ ì´ë¯¸ build_stage_table_from_stage_resultsì—ì„œ ì„¤ì •ë˜ì—ˆì„ ìˆ˜ ìžˆì§€ë§Œ,
    # ëª…ì‹œì ìœ¼ë¡œ CLI ê°’ì´ ì œê³µë˜ë©´ stage_tide_csvë³´ë‹¤ ìš°ì„  ì ìš©
    if forecast_tide_m is not None:
        # CLI ê°’ì´ ìžˆìœ¼ë©´ ëª¨ë“  Stageì— ì§ì ‘ ì ìš© (stage_tide_csvë³´ë‹¤ ìš°ì„ )
        df["Forecast_Tide_m"] = float(forecast_tide_m)
        print(
            f"[OK] CLI forecast_tide_m={forecast_tide_m} applied (override stage_tide_csv and tide_table)"
        )
    else:
        # CLI ê°’ì´ ì—†ì„ ë•Œë§Œ stage_tide_csv ì‚¬ìš©
        # Priority 1: Direct stage_tide_csv (highest priority when CLI not provided)
        if stage_tide_csv_path and Path(stage_tide_csv_path).exists():
            try:
                df_tide = pd.read_csv(stage_tide_csv_path, encoding="utf-8-sig")

                # Normalize stage key for matching
                def _norm_stage_key(s):
                    return str(s).strip()

                # Find stage column in tide CSV
                stage_col_tide = None
                for c in df_tide.columns:
                    if str(c).strip().lower() in ("stage", "stagekey", "stage_key"):
                        stage_col_tide = c
                        break
                if stage_col_tide is None:
                    stage_col_tide = df_tide.columns[0]  # Fallback to first column

                # Find Forecast_tide_m column
                tide_col = None
                for c in df_tide.columns:
                    if str(c).strip().lower() in (
                        "forecast_tide_m",
                        "forecast_tide",
                        "tide_m",
                    ):
                        tide_col = c
                        break

                if stage_col_tide and tide_col:
                    # Create mapping
                    df_tide["_stage_norm"] = (
                        df_tide[stage_col_tide].astype(str).map(_norm_stage_key)
                    )
                    tide_map = dict(
                        zip(
                            df_tide["_stage_norm"],
                            pd.to_numeric(df_tide[tide_col], errors="coerce"),
                        )
                    )

                    # Apply to df
                    df["_stage_norm"] = df["Stage"].astype(str).map(_norm_stage_key)
                    df["Forecast_Tide_m"] = (
                        df["_stage_norm"].map(tide_map).fillna(df["Forecast_Tide_m"])
                    )
                    df = df.drop(columns=["_stage_norm"])

                    assigned = df["Forecast_Tide_m"].notna().sum()
                    print(
                        f"[OK] Loaded stage_tide_csv: {assigned}/{len(df)} stages assigned from {stage_tide_csv_path.name}"
                    )
                else:
                    print(
                        f"[WARN] stage_tide_csv missing required columns (Stage/StageKey, Forecast_tide_m)"
                    )
            except Exception as e:
                print(f"[WARN] Failed to load stage_tide_csv: {type(e).__name__}: {e}")

    # Priority 2: Optional: per-stage tide assignment via tide_table + stage_schedule
    if (
        tide_table_path
        and stage_schedule_path
        and load_tide_table_any
        and load_stage_schedule_any
        and apply_forecast_tide_from_table
    ):
        try:
            tide_df = load_tide_table_any(Path(tide_table_path))
            sched_df = load_stage_schedule_any(Path(stage_schedule_path))
            strategy = (
                "override"
                if str(tide_strategy).lower()
                in ("override_from_table", "override", "force")
                else "fillna"
            )
            df = apply_forecast_tide_from_table(
                df,
                tide_df,
                sched_df,
                stage_col="Stage",
                out_col="Forecast_Tide_m",
                strategy=strategy,
            )
        except Exception as e:
            print(f"[WARN] Tide table apply failed: {type(e).__name__}: {e}")

    # Fallback: CLI forecast tide to fill missing (CLIê°€ ì—†ì„ ë•Œë§Œ)
    # Note: CLI ê°’ì´ ì´ë¯¸ Priority 0ì—ì„œ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
    # í•˜ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ ìœ ì§€ (CLIê°€ Noneì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    if forecast_tide_m is not None:
        # ì´ë¯¸ Priority 0ì—ì„œ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ, fillnaë§Œ ìˆ˜í–‰ (ì¤‘ë³µ ë°©ì§€)
        df["Forecast_Tide_m"] = df["Forecast_Tide_m"].fillna(float(forecast_tide_m))

    # Derived fields
    df["Forecast_tide_m"] = df["Forecast_Tide_m"]

    req_list = []
    margin_list = []
    uf_list = []
    ua_list = []
    umin_list = []
    stat_list = []
    note_list = []

    for _, r in df.iterrows():
        dfwd = r.get("Current_FWD_m", np.nan)
        daft = r.get("Current_AFT_m", np.nan)
        draft_ref = np.nan
        if pd.notna(dfwd) and pd.notna(daft):
            draft_ref = max(float(dfwd), float(daft))
        elif pd.notna(dfwd):
            draft_ref = float(dfwd)
        elif pd.notna(daft):
            draft_ref = float(daft)

        depth = r.get("DepthRef_m", np.nan)
        ukc_min = r.get("UKC_Min_m", np.nan)
        wl = r.get("Forecast_Tide_m", np.nan)
        squat = r.get("Squat_m", float(squat_m))
        safety = r.get("SafetyAllow_m", float(safety_allow_m))

        if required_tide_m and ukc_fwd_aft_min and verify_tide:
            req = required_tide_m(
                float(depth) if pd.notna(depth) else None,
                float(draft_ref) if pd.notna(draft_ref) else None,
                float(ukc_min) if pd.notna(ukc_min) else None,
                float(squat) if pd.notna(squat) else 0.0,
                float(safety) if pd.notna(safety) else 0.0,
                clamp_zero=True,
            )
            uf, ua, umin = ukc_fwd_aft_min(
                float(depth) if pd.notna(depth) else None,
                float(wl) if pd.notna(wl) else None,
                float(dfwd) if pd.notna(dfwd) else None,
                float(daft) if pd.notna(daft) else None,
                float(squat) if pd.notna(squat) else 0.0,
                float(safety) if pd.notna(safety) else 0.0,
            )
            margin = float(wl - req) if pd.notna(wl) and pd.notna(req) else np.nan
            stat, note = verify_tide(
                req, float(wl) if pd.notna(wl) else None, tolerance_m=float(tide_tol_m)
            )
        else:
            req = np.nan
            uf = ua = umin = np.nan
            margin = np.nan
            stat, note = ("VERIFY", "tide_ukc_engine not available")

        req_list.append(req)
        margin_list.append(margin)
        uf_list.append(uf)
        ua_list.append(ua)
        umin_list.append(umin)
        stat_list.append(stat)
        note_list.append(note)

    df["Tide_required_m"] = req_list
    df["Tide_margin_m"] = margin_list
    df["UKC_fwd_m"] = uf_list
    df["UKC_aft_m"] = ua_list
    df["UKC_min_actual_m"] = umin_list
    df["Tide_verification"] = stat_list
    df["Tide_note"] = note_list

    df.to_csv(stage_table_csv, index=False, encoding="utf-8-sig")
    return stage_table_csv


def generate_stage_QA_csv(
    stage_table_csv: Path,
    out_qa_csv: Path,
    fwd_max_m: float = None,  # Legacy param, read from CSV instead
    aft_min_m: float = None,  # Legacy param, read from CSV instead
    d_vessel_m: float = None,  # Legacy param, read from CSV instead
    forecast_tide_m: Optional[float] = None,  # Legacy param, read from CSV instead
    depth_ref_m: Optional[float] = None,  # Legacy param, read from CSV instead
    ukc_min_m: Optional[float] = None,  # Legacy param, read from CSV instead
    critical_only: bool = False,
    critical_stage_list: Optional[List[str]] = None,
    critical_regex: str = DEFAULT_CRITICAL_STAGE_REGEX,
    gateb_fwd_max_m_cd: Optional[float] = None,
    squat_m: float = 0.0,
    safety_allow_m: float = 0.0,
    hydro_json_path: Optional[Path] = None,
    strict_hardstop: bool = True,
    tol_m: float = 1e-6,
    hydro_disp_tol_t: float = 1e-3,
    solver_summary_csv: Optional[Path] = None,
    hmax_wave_m: Optional[float] = None,
    four_corner_monitoring: bool = False,
    tide_tol_m: float = DEFAULT_TIDE_TOL_M,
    trim_abs_limit_m: float = 0.50,
    trim_limit_enforced: bool = True,
    freeboard_min_m: float = 0.0,
    freeboard_min_enforced: bool = True,
) -> Path:
    """
    stage_table_unified.csv -> pipeline_stage_QA.csv

    PATCH:
      - Gate_Hydro_Range ì¶”ê°€ (Disp_t in hydro disp range)
      - HardStop_Any(Y/N) + HardStop_Reason í•©ì„±
      - raw vs post-solve draft ë¶„ë¦¬ (solver_summary_csv ì ìš© ì‹œ)
      - Gate_Freeboard_ND (GL Noble Denton 0013/ND effective freeboard)
      - strict_hardstop=Trueì´ë©´ QA CSV ìƒì„± í›„ ValueErrorë¡œ ì¤‘ë‹¨
    """
    import math

    import numpy as np

    stage_table_csv = Path(stage_table_csv)
    out_qa_csv = Path(out_qa_csv)

    df = pd.read_csv(stage_table_csv)

    required = [
        "Stage",
        "Current_FWD_m",
        "Current_AFT_m",
        "FWD_MAX_m",
        "AFT_MIN_m",
        "D_vessel_m",
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(
            f"[HARDSTOP] stage_table_unified.csv missing required columns: {missing}"
        )

    # Drafts (raw)
    df["Draft_FWD_m_raw"] = pd.to_numeric(df["Current_FWD_m"], errors="coerce")
    df["Draft_AFT_m_raw"] = pd.to_numeric(df["Current_AFT_m"], errors="coerce")
    df["Draft_FWD_m"] = df["Draft_FWD_m_raw"].copy()
    df["Draft_AFT_m"] = df["Draft_AFT_m_raw"].copy()
    df["Draft_Source"] = "raw"

    # Apply solver results (post-solve drafts) if provided
    if solver_summary_csv is not None and Path(solver_summary_csv).exists():
        try:
            solver_df = pd.read_csv(solver_summary_csv, encoding="utf-8-sig")
            col_fwd = (
                "New_FWD_m"
                if "New_FWD_m" in solver_df.columns
                else ("new_fwd_m" if "new_fwd_m" in solver_df.columns else None)
            )
            col_aft = (
                "New_AFT_m"
                if "New_AFT_m" in solver_df.columns
                else ("new_aft_m" if "new_aft_m" in solver_df.columns else None)
            )
            if not solver_df.empty and col_fwd and col_aft:
                if "Stage" in solver_df.columns:
                    solver_df = solver_df.copy()
                    solver_df["_stage_norm"] = (
                        solver_df["Stage"].astype(str).str.strip().str.lower()
                    )
                    for idx, row in df.iterrows():
                        stage_name = str(row.get("Stage", "")).strip().lower()
                        match = solver_df[solver_df["_stage_norm"] == stage_name]
                        if match.empty:
                            continue
                        solver_row = match.iloc[0]
                        if pd.notna(solver_row.get(col_fwd)):
                            df.at[idx, "Draft_FWD_m"] = float(solver_row[col_fwd])
                            df.at[idx, "Draft_Source"] = "solver"
                        if pd.notna(solver_row.get(col_aft)):
                            df.at[idx, "Draft_AFT_m"] = float(solver_row[col_aft])
                            df.at[idx, "Draft_Source"] = "solver"
                else:
                    solver_row = solver_df.iloc[0]
                    if pd.notna(solver_row.get(col_fwd)) and pd.notna(
                        solver_row.get(col_aft)
                    ):
                        df["Draft_FWD_m"] = float(solver_row[col_fwd])
                        df["Draft_AFT_m"] = float(solver_row[col_aft])
                        df["Draft_Source"] = "solver"
                print(
                    f"[OK] Solver results applied to QA: {Path(solver_summary_csv).name}"
                )
        except Exception as e:
            print(
                f"[WARN] Failed to apply solver results to QA ({type(e).__name__}: {e})"
            )

    # Draft clipping flags (raw vs solver)
    df["Draft_Max_raw_m"] = df[["Draft_FWD_m_raw", "Draft_AFT_m_raw"]].max(axis=1)
    df["Draft_Max_solver_m"] = df[["Draft_FWD_m", "Draft_AFT_m"]].max(axis=1)
    dv = pd.to_numeric(df["D_vessel_m"], errors="coerce")
    df["D_vessel_m"] = dv
    df["Draft_Clipped_raw"] = df["Draft_Max_raw_m"] > (dv + tol_m)
    df["Draft_Clipped_solver"] = df["Draft_Max_solver_m"] > (dv + tol_m)

    # Freeboard
    df["Freeboard_FWD_m"] = (
        pd.to_numeric(df["D_vessel_m"], errors="coerce") - df["Draft_FWD_m"]
    )
    df["Freeboard_AFT_m"] = (
        pd.to_numeric(df["D_vessel_m"], errors="coerce") - df["Draft_AFT_m"]
    )
    df["Freeboard_Min_m"] = df[["Freeboard_FWD_m", "Freeboard_AFT_m"]].min(axis=1)
    # Explicit label: Bow/Stern min freeboard (avoid linkspan freeboard confusion)
    df["Freeboard_Min_BowStern_m"] = df["Freeboard_Min_m"]

    # Trim (cm) for gate evaluation
    # Always use solver/plan drafts to avoid Current_* vs Draft_* mismatch.
    trim_cm_from_draft = (df["Draft_AFT_m"] - df["Draft_FWD_m"]) * 100.0
    df["Trim_cm"] = trim_cm_from_draft
    if "Input_Trim_cm" in df.columns:
        input_trim_cm = pd.to_numeric(df["Input_Trim_cm"], errors="coerce")
        df["Input_Trim_cm"] = input_trim_cm
        df["Trim_cm_diff_plan_vs_input"] = trim_cm_from_draft - input_trim_cm
    else:
        df["Trim_cm_diff_plan_vs_input"] = math.nan

    # Gate-FB (GL Noble Denton 0013/ND) - Effective freeboard requirement
    if hmax_wave_m is not None and hmax_wave_m > 0:
        if four_corner_monitoring:
            df["Freeboard_Req_ND_m"] = 0.50 + 0.50 * float(hmax_wave_m)
            df["Freeboard_ND_Monitoring"] = "4-corner"
        else:
            df["Freeboard_Req_ND_m"] = 0.80 + 0.50 * float(hmax_wave_m)
            df["Freeboard_ND_Monitoring"] = "None"
        df["Gate_Freeboard_ND"] = np.where(
            df["Freeboard_Min_m"] >= df["Freeboard_Req_ND_m"] - tol_m, "OK", "NG"
        )
        df["Freeboard_ND_Margin_m"] = df["Freeboard_Min_m"] - df["Freeboard_Req_ND_m"]
    else:
        df["Freeboard_Req_ND_m"] = math.nan
        df["Gate_Freeboard_ND"] = "N/A"
        df["Freeboard_ND_Margin_m"] = math.nan
        df["Freeboard_ND_Monitoring"] = "N/A"

    # Option B: Gate-Trim (hard constraint if enabled)
    trim_limit_cm = float(trim_abs_limit_m) * 100.0
    trim_tol_cm = float(tol_m) * 100.0
    if trim_limit_enforced:
        df["Gate_Trim"] = np.where(
            df["Trim_cm"].abs() <= (trim_limit_cm + trim_tol_cm), "OK", "NG"
        )
        df["Trim_Margin_cm"] = trim_limit_cm - df["Trim_cm"].abs()
    else:
        df["Gate_Trim"] = "N/A"
        df["Trim_Margin_cm"] = math.nan

    # Option B: Gate-Freeboard minimum (hard constraint if enabled)
    if freeboard_min_enforced:
        df["Gate_Freeboard_MIN"] = np.where(
            df["Freeboard_Min_m"] >= float(freeboard_min_m) - tol_m, "OK", "NG"
        )
        df["Freeboard_MIN_Margin_m"] = df["Freeboard_Min_m"] - float(freeboard_min_m)
    else:
        df["Gate_Freeboard_MIN"] = "N/A"
        df["Freeboard_MIN_Margin_m"] = math.nan

    # Gates (ê¸°ì¡´)
    fwd_lim = pd.to_numeric(df["FWD_MAX_m"], errors="coerce")
    # Gate-B uses Chart Datum. Convert drafts to CD if tide is available.
    fwd_gate = df["Draft_FWD_m"]
    if "Forecast_Tide_m" in df.columns:
        df["Draft_FWD_m_CD"] = pd.to_numeric(
            df["Draft_FWD_m"], errors="coerce"
        ) - pd.to_numeric(df["Forecast_Tide_m"], errors="coerce")
        fwd_gate = df["Draft_FWD_m_CD"]
    if "FWD_MAX_applicable" in df.columns:
        mask_app = df["FWD_MAX_applicable"] == True
    else:
        mask_app = fwd_lim.notna()

    df["Gate_FWD_Max"] = "OK"
    mask_ng = mask_app & (fwd_gate > fwd_lim + tol_m)
    df.loc[mask_ng, "Gate_FWD_Max"] = "NG"
    df["Gate_AFT_Min"] = np.where(
        df["Draft_AFT_m"] >= pd.to_numeric(df["AFT_MIN_m"], errors="coerce") - tol_m,
        "OK",
        "NG",
    )
    df["Gate_Freeboard"] = np.where(df["Freeboard_Min_m"] >= -tol_m, "OK", "NG")

    # Margins
    df["FWD_Margin_m"] = np.nan
    df.loc[mask_app, "FWD_Margin_m"] = fwd_lim[mask_app] - fwd_gate[mask_app]
    df["AFT_Margin_m"] = df["Draft_AFT_m"] - pd.to_numeric(
        df["AFT_MIN_m"], errors="coerce"
    )

    # ----------------------------------------------------------
    # Gate-A / Gate-B SSOT labels (avoid ambiguous "2.70m")
    # ----------------------------------------------------------
    df["AFT_MIN_2p70_m"] = float(GATE_A_VALUE_M)
    df["AFT_Margin_2p70_m"] = pd.to_numeric(df["Draft_AFT_m"], errors="coerce") - float(
        GATE_A_VALUE_M
    )
    df["Gate_AFT_MIN_2p70"] = np.where(df["AFT_Margin_2p70_m"] >= -tol_m, "OK", "NG")
    df["Gate_AFT_MIN_2p70_PASS"] = df["Gate_AFT_MIN_2p70"] == "OK"
    if "GateA_AFT_MIN_2p70_PASS" not in df.columns:
        df["GateA_AFT_MIN_2p70_PASS"] = df["Gate_AFT_MIN_2p70_PASS"]

    # Gate-B scope (critical-only) â€” ALWAYS compute by critical matcher
    stage_is_critical = (
        df["Stage"]
        .astype(str)
        .apply(
            lambda s: _is_critical_stage(
                s, patterns=critical_stage_list, regex=critical_regex
            )
        )
        .fillna(False)
    )
    df["Gate_B_Applies"] = stage_is_critical

    df["FWD_MAX_2p70_m"] = float(GATE_B_VALUE_M)
    df["FWD_Margin_2p70_m"] = float(GATE_B_VALUE_M) - pd.to_numeric(
        fwd_gate, errors="coerce"
    )
    df["Gate_FWD_MAX_2p70_critical_only"] = np.where(
        df["Gate_B_Applies"],
        np.where(df["FWD_Margin_2p70_m"] >= -tol_m, "OK", "NG"),
        "N/A",
    )
    df.loc[~df["Gate_B_Applies"], "Gate_FWD_MAX_2p70_critical_only"] = "N/A"
    # Extra safety: ensure all non-critical are truly "N/A" string (not nan)
    df["Gate_FWD_MAX_2p70_critical_only"] = df[
        "Gate_FWD_MAX_2p70_critical_only"
    ].fillna("N/A")
    df["Gate_FWD_MAX_2p70_critical_only_PASS"] = (
        df["Gate_FWD_MAX_2p70_critical_only"] == "OK"
    )

    # -----------------------------
    # UKC (Definition-split)
    # -----------------------------
    has_ukc_inputs = (
        ("Forecast_Tide_m" in df.columns)
        and ("DepthRef_m" in df.columns)
        and ("UKC_Min_m" in df.columns)
    )
    if has_ukc_inputs:
        df["UKC_Min_Required_m"] = pd.to_numeric(df["UKC_Min_m"], errors="coerce")

        available_depth = pd.to_numeric(
            df["DepthRef_m"], errors="coerce"
        ) + pd.to_numeric(df["Forecast_Tide_m"], errors="coerce")
        df["UKC_FWD_m"] = available_depth - (
            df["Draft_FWD_m"] + float(squat_m) + float(safety_allow_m)
        )
        df["UKC_AFT_m"] = available_depth - (
            df["Draft_AFT_m"] + float(squat_m) + float(safety_allow_m)
        )
        df["UKC_Min_m"] = df[["UKC_FWD_m", "UKC_AFT_m"]].min(axis=1)

        df["Gate_UKC"] = np.where(
            df["UKC_Min_m"] >= df["UKC_Min_Required_m"] - tol_m, "OK", "NG"
        )
        df["UKC_Margin_m"] = df["UKC_Min_m"] - df["UKC_Min_Required_m"]

        df["Required_WL_for_UKC_m"] = (
            df[["Draft_FWD_m", "Draft_AFT_m"]].max(axis=1)
            + float(squat_m)
            + float(safety_allow_m)
            + df["UKC_Min_Required_m"]
            - pd.to_numeric(df["DepthRef_m"], errors="coerce")
        )
    else:
        df["UKC_Min_Required_m"] = math.nan
        df["UKC_FWD_m"] = math.nan
        df["UKC_AFT_m"] = math.nan
        df["UKC_Min_m"] = math.nan
        df["Gate_UKC"] = "N/A"
        df["UKC_Margin_m"] = math.nan
        df["Required_WL_for_UKC_m"] = math.nan

    # -----------------------------
    # NEW Gate: Hydro range
    # -----------------------------
    df["Gate_Hydro_Range"] = "N/A"
    df["HydroDisp_Margin_t"] = math.nan
    hmin = None
    hmax = None
    disp = None

    def _auto_find_hydro_json(start: Path):
        candidates = [
            start.parent / "bplus_inputs" / "Hydro_Table_Engineering.json",
            start.parent.parent / "bplus_inputs" / "Hydro_Table_Engineering.json",
            start.parent / "Hydro_Table_Engineering.json",
        ]
        for p in candidates:
            if p.exists():
                return p
        return None

    if "Input_Disp_t" in df.columns:
        disp = pd.to_numeric(df["Input_Disp_t"], errors="coerce")

        if (
            ("Hydro_Disp_Min_t" in df.columns)
            and ("Hydro_Disp_Max_t" in df.columns)
            and (not df["Hydro_Disp_Min_t"].isna().all())
        ):
            hmin = float(
                pd.to_numeric(df["Hydro_Disp_Min_t"], errors="coerce").dropna().iloc[0]
            )
            hmax = float(
                pd.to_numeric(df["Hydro_Disp_Max_t"], errors="coerce").dropna().iloc[0]
            )
    else:
        hydro_path = (
            Path(hydro_json_path)
            if hydro_json_path
            else _auto_find_hydro_json(stage_table_csv)
        )
        if hydro_path is not None and hydro_path.exists():
            try:
                with open(hydro_path, "r", encoding="utf-8") as f:
                    hydro_obj = json.load(f)

                rows = None
                if isinstance(hydro_obj, list):
                    rows = hydro_obj
                elif isinstance(hydro_obj, dict):
                    for k in ("rows", "table", "data", "HydroTable"):
                        if k in hydro_obj and isinstance(hydro_obj[k], list):
                            rows = hydro_obj[k]
                            break
                    if rows is None and all(
                        isinstance(v, dict) for v in hydro_obj.values()
                    ):
                        rows = list(hydro_obj.values())

                disp_vals = []
                if isinstance(rows, list):
                    for r in rows:
                        if not isinstance(r, dict):
                            continue
                        for k, v in r.items():
                            if str(k).strip().lower() in (
                                "disp_t",
                                "disp",
                                "displacement_t",
                                "displacement",
                            ):
                                try:
                                    disp_vals.append(float(v))
                                except Exception:
                                    pass
                                break

                if disp_vals:
                    hmin = min(disp_vals)
                    hmax = max(disp_vals)
                    df["Hydro_Disp_Min_t"] = hmin
                    df["Hydro_Disp_Max_t"] = hmax
            except Exception:
                pass

    if hmin is not None and hmax is not None and disp is not None:
        ok = (disp >= hmin - hydro_disp_tol_t) & (disp <= hmax + hydro_disp_tol_t)
        df["Gate_Hydro_Range"] = np.where(ok, "OK", "NG")

        lower_margin = disp - hmin
        upper_margin = hmax - disp
        df["HydroDisp_Margin_t"] = np.where(
            ok,
            np.minimum(lower_margin, upper_margin),
            -np.minimum(abs(lower_margin), abs(upper_margin)),
        )

    # -----------------------------
    # HARDSTOP synthesis
    # -----------------------------
    if "HS_Any" in df.columns:
        hardstop_bool = df["HS_Any"] == True
    else:
        hardstop_bool = (
            (df["Gate_Freeboard"] == "NG")
            | (df["Gate_Hydro_Range"] == "NG")
            | df["Draft_FWD_m"].isna()
            | df["Draft_AFT_m"].isna()
        )
    if trim_limit_enforced:
        hardstop_bool = hardstop_bool | (df["Gate_Trim"] == "NG")
    if freeboard_min_enforced:
        hardstop_bool = hardstop_bool | (df["Gate_Freeboard_MIN"] == "NG")

    df["HardStop_Any"] = np.where(hardstop_bool, "Y", "N")

    reasons = []
    for _, row in df.iterrows():
        r = []
        if pd.isna(row["Draft_FWD_m"]) or pd.isna(row["Draft_AFT_m"]):
            r.append("DraftNaN")
        if row.get("Gate_Freeboard") == "NG":
            r.append("OverDepth/DeckWet")
        if trim_limit_enforced and row.get("Gate_Trim") == "NG":
            r.append("TrimLimit")
        if freeboard_min_enforced and row.get("Gate_Freeboard_MIN") == "NG":
            r.append("FreeboardMin")
        if row.get("Gate_Hydro_Range") == "NG":
            r.append("HydroOutOfRange")
        reasons.append("|".join(r) if r else "")
    df["HardStop_Reason"] = reasons

    # Add Tmean_m column for compatibility
    if "Tmean_m" not in df.columns:
        df["Tmean_m"] = (df["Draft_FWD_m"] + df["Draft_AFT_m"]) / 2.0

    # ----------------------------------------------------------
    # PATCH: split 2.70m gates (Captain vs Mammoet) into QA CSV
    #   - GateA_AFT_MIN_2p70_PASS (Captain)
    #   - GateB_FWD_MAX_2p70_CD_PASS (Mammoet; profile-driven critical stages)
    # ----------------------------------------------------------
    try:
        gateb_fwd_max = (
            float(gateb_fwd_max_m_cd)
            if gateb_fwd_max_m_cd is not None
            else MAMMOET_FWD_MAX_DRAFT_M_CD
        )
        df = add_split_270_gates(
            df,
            aft_min_m=CAPTAIN_AFT_MIN_DRAFT_M,
            fwd_max_m_cd=gateb_fwd_max,
            critical_only=critical_only,
            critical_regex=critical_regex,
            critical_stage_list=critical_stage_list,
            tol_m=float(tol_m),
        )
    except Exception as e:
        # Do not hard-stop QA generation if the split gate columns cannot be added
        print(f"[WARN] add_split_270_gates skipped in generate_stage_QA_csv: {e}")

    # ------------------------------------------------------------------
    # Tide/UKC user-facing columns (AGI ê¸°ì¤€ í†µí•© ì¶œë ¥)
    # ------------------------------------------------------------------
    try:
        # Required tide (clamped to >=0), margin, and aliases requested by operations
        df["Tide_required_m"] = pd.to_numeric(
            df.get("Required_WL_for_UKC_m"), errors="coerce"
        ).clip(lower=0.0)
        df["Forecast_tide_m"] = pd.to_numeric(
            df.get("Forecast_Tide_m"), errors="coerce"
        )
        df["Tide_margin_m"] = df["Forecast_tide_m"] - df["Tide_required_m"]

        # UKC requirement vs actual ends
        df["UKC_min_m"] = pd.to_numeric(df.get("UKC_Min_Required_m"), errors="coerce")
        df["UKC_fwd_m"] = pd.to_numeric(df.get("UKC_FWD_m"), errors="coerce")
        df["UKC_aft_m"] = pd.to_numeric(df.get("UKC_AFT_m"), errors="coerce")
        df["UKC_min_actual_m"] = pd.to_numeric(df.get("UKC_Min_m"), errors="coerce")

        # Tide verification (OK/LIMIT/FAIL/VERIFY) based on margin
        tol = float(tide_tol_m) if tide_tol_m is not None else float(DEFAULT_TIDE_TOL_M)
        df["Tide_verification"] = np.where(
            df["Forecast_tide_m"].isna(),
            "VERIFY",
            np.where(
                df["Tide_margin_m"] < 0.0,
                "FAIL",
                np.where(df["Tide_margin_m"] < tol, "LIMIT", "OK"),
            ),
        )
    except Exception as e:
        print(f"[WARN] Tide alias columns skipped in QA CSV: {e}")

    df.to_csv(out_qa_csv, index=False, encoding="utf-8-sig")

    if strict_hardstop and (df["HardStop_Any"] == "Y").any():
        failing = df.loc[
            df["HardStop_Any"] == "Y",
            [
                "Stage",
                "HardStop_Reason",
                "Draft_FWD_m",
                "Draft_AFT_m",
                "Gate_Hydro_Range",
            ],
        ].copy()
        msg = [
            "[HARDSTOP] One or more stages failed HardStop gates (QA).",
            f"  - stage_table_csv: {stage_table_csv}",
            f"  - out_qa_csv      : {out_qa_csv}",
            "  - failing rows (Stage, Reason, FWD, AFT, HydroGate):",
        ]
        for _, r in failing.iterrows():
            msg.append(
                f"    * {r['Stage']}: {r['HardStop_Reason']} | FWD={r['Draft_FWD_m']}, AFT={r['Draft_AFT_m']}, Hydro={r['Gate_Hydro_Range']}"
            )
        raise ValueError("\n".join(msg))

    return out_qa_csv


def debug_report_step(
    *,
    out_dir: Path,
    stage_qa_csv: Path,
    hydro_json_path: Optional[Path] = None,
    hydro_csv: Optional[Path] = None,
) -> int:
    """
    Generate feasibility debug report from pipeline QA and hydro table.
    """
    try:
        root_dir = Path(__file__).resolve().parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))
        from debug_report import write_debug_report
    except Exception as exc:
        print(f"[WARN] Debug report module unavailable: {exc}")
        return 0

    if not stage_qa_csv.exists():
        print(f"[WARN] Stage QA CSV not found: {stage_qa_csv}")
        return 0

    debug_dir = ensure_dir(out_dir)
    hydro_csv_path = hydro_csv or (debug_dir / "hydro_table_for_solver.csv")
    if not hydro_csv_path.exists():
        if hydro_json_path and Path(hydro_json_path).exists():
            try:
                with open(hydro_json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                rows = data.get("rows", data.get("table", data.get("data", [])))
                hydro_df = pd.DataFrame(rows)
                hydro_df.to_csv(hydro_csv_path, index=False, encoding="utf-8-sig")
                print(f"[OK] Hydro CSV created: {hydro_csv_path}")
            except Exception as exc:
                print(f"[WARN] Failed to build hydro CSV: {exc}")
                return 0
        else:
            print("[WARN] Hydro source not available; skipping debug report.")
            return 0

    out_md = debug_dir / "debug_feasibility_pipeline.md"
    out_flags = debug_dir / "debug_stage_flags.csv"

    try:
        write_debug_report(
            stage_qa_csv=stage_qa_csv,
            hydro_csv=hydro_csv_path,
            out_md=out_md,
            out_flags_csv=out_flags,
        )
        print("[OK] Debug report generated")
        print(f"  - {out_md}")
        print(f"  - {out_flags}")
        return 0
    except Exception as exc:
        print(f"[WARN] Debug report generation failed: {exc}")
        return 1


# -----------------------------------------------------------------------------
# Pipeline steps
# -----------------------------------------------------------------------------
@dataclass
class StepResult:
    ok: bool
    returncode: int
    log: Path


def run_tidying_for_csv(
    file_path: Path,
    base_dir: Path,
    registry_path: Optional[Path] = None,
    deliverable_id: Optional[str] = None,
    tank_catalog_path: Optional[Path] = None,
    verbose: bool = True,
) -> Optional[Dict[str, Any]]:
    """
    CSV íŒŒì¼ì— tidying ë° ê²€ì¦ì„ ì‹¤í–‰í•˜ëŠ” ê³µí†µ í—¬í¼ í•¨ìˆ˜

    Args:
        file_path: ê²€ì¦í•  CSV íŒŒì¼ ê²½ë¡œ
        base_dir: Base directory (sys.path ì¶”ê°€ìš©)
        registry_path: Headers registry ê²½ë¡œ
        deliverable_id: Deliverable ID
        tank_catalog_path: Tank catalog ê²½ë¡œ (BALLAST íŒŒì¼ìš©)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not file_path.exists():
        return None

    try:
        import sys

        if str(base_dir) not in sys.path:
            sys.path.insert(0, str(base_dir))

        from ssot.data_quality_validator import DataQualityValidator

        validator_registry = registry_path or (base_dir / "headers_registry.json")
        if not validator_registry.exists():
            if verbose:
                print(
                    f"[WARN] Registry not found: {validator_registry}, skipping tidying"
                )
            return None

        validator = DataQualityValidator(registry_path=validator_registry)

        # BALLAST ì‹œí€€ìŠ¤ íŒŒì¼ì€ íŠ¹ë³„ ì²˜ë¦¬
        if "BALLAST" in file_path.name and (
            "EXEC" in file_path.name or "OPTION" in file_path.name
        ):
            results = validator.tidy_and_validate_ballast_sequence(
                file_path=file_path,
                tank_catalog_path=tank_catalog_path,
                deliverable_id=deliverable_id or "BALLAST_CSV",
            )
        else:
            # ì¼ë°˜ CSV tidying
            results = validator.tidy_and_validate_csv(
                file_path=file_path,
                deliverable_id=deliverable_id or "GENERIC_CSV",
            )

        if verbose:
            if results["llm_context"]:
                print(
                    f"[OK] Tidying: {file_path.name} "
                    f"({results['validated_count']}/{results['total_rows']} rows validated)"
                )
            else:
                error_count = len(results["tidying"]["errors"]) + len(
                    results["validation"]["errors"]
                )
                if error_count > 0:
                    print(
                        f"[WARN] Tidying failed: {file_path.name} ({error_count} errors)"
                    )

        # Save validation report
        report_path = file_path.parent / f"tidying_{file_path.stem}_report.json"
        import json

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        return results

    except Exception as e:
        if verbose:
            print(
                f"[WARN] Tidying failed for {file_path.name}: {type(e).__name__}: {e}"
            )
        return None


def step_run_script(
    step_id: int,
    name: str,
    script: Path,
    args: List[str],
    cwd: Path,
    out_dir: Path,
    env: Optional[Dict[str, str]] = None,
) -> StepResult:
    if not script.exists():
        log = out_dir / "logs" / f"{step_id:02d}_{name}_MISSING.log"
        log.write_text(f"Missing script: {script}\n", encoding="utf-8")
        return StepResult(False, 127, log)

    log = out_dir / "logs" / f"{step_id:02d}_{name}.log"
    cmd = [which_python(), str(script)] + args
    rc = run_cmd(cmd, cwd=cwd, log_path=log, env=env)
    return StepResult(rc == 0, rc, log)


def collect_all_output_files(
    base_dir: Path, out_dir: Path, site: str, inputs_dir: Optional[Path] = None
) -> None:
    """
    Collect all output files generated by pipeline steps into the output directory.
    ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ì¶œë ¥ íŒŒì¼ì„ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    Also copies input/dependency files to outputs/inputs/ subdirectory.
    ì˜ì¡´ì„± íŒŒì¼ ë° ê¸°ë³¸ ë¡œìš° ë°ì´í„° íŒŒì¼ë„ outputs/inputs/ í•˜ìœ„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
    """
    collected_count = 0

    # Step 1 outputs (TR Excel ë° ê´€ë ¨ íŒŒì¼)
    # TR ìŠ¤í¬ë¦½íŠ¸ëŠ” base_dir.parentì— íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ìžˆìŒ
    step1_patterns = [
        f"LCT_BUSHRA_{site}_TR_Final_v*.xlsx",
        "RORO_Summary.png",
    ]

    # base_dirê³¼ base_dir.parent ëª¨ë‘ ê²€ìƒ‰
    search_dirs = [base_dir, base_dir.parent]

    for pattern in step1_patterns:
        for search_dir in search_dirs:
            for src_file in search_dir.glob(pattern):
                if src_file.is_file():
                    dest_file = out_dir / src_file.name
                    # ì´ë¯¸ ìˆ˜ì§‘ëœ íŒŒì¼ì€ ìŠ¤í‚µ
                    if dest_file.exists():
                        continue
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-1 output: {src_file.name} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {src_file.name}: {type(e).__name__}: {e}"
                        )

    # Step 2 outputs (ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆì„ ìˆ˜ ìžˆìŒ)
    # Step 2 ìŠ¤í¬ë¦½íŠ¸ë„ base_dir.parentì— íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ìžˆìŒ
    step2_files = [
        f"OPS_FINAL_R3_{site}_Ballast_Integrated.xlsx",
        "OPS_FINAL_R3_Report_Integrated.md",
    ]

    for filename in step2_files:
        # base_dirê³¼ base_dir.parent ëª¨ë‘ ê²€ìƒ‰
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-2 output: {filename} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                        break  # íŒŒì¼ì„ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {filename}: {type(e).__name__}: {e}"
                        )

    # Step 0 outputs (SPMT unified)
    step0_files = [
        "AGI_SPMT_Shuttle_Output.xlsx",
        "stage_loads.csv",
        "stage_summary.csv",
    ]

    for filename in step0_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-0 output: {filename} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {filename}: {type(e).__name__}: {e}"
                        )

    # Step 5 outputs (Bryan template)
    step5_files = [
        "Bryan_Submission_Data_Pack_Template.xlsx",
        "Bryan_Submission_Data_Pack_Populated.xlsx",
    ]

    for filename in step5_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-5 output: {filename} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {filename}: {type(e).__name__}: {e}"
                        )

    # Step 4b outputs (Ballast Sequence & Checklist)
    step4b_files = [
        "BALLAST_SEQUENCE.csv",
        "BALLAST_OPTION.csv",
        "BALLAST_EXEC.csv",
        "BALLAST_SEQUENCE.xlsx",
        "BALLAST_OPERATIONS_CHECKLIST.md",
        "HOLD_POINT_SUMMARY.csv",
    ]

    for filename in step4b_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-4b output: {filename} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {filename}: {type(e).__name__}: {e}"
                        )

    # Step 4c outputs (Valve Lineup)
    step4c_files = [
        "BALLAST_SEQUENCE_WITH_VALVES.md",
    ]

    for filename in step4c_files:
        for search_dir in search_dirs:
            src_file = search_dir / filename
            if src_file.exists() and src_file.is_file():
                dest_file = out_dir / filename
                if not dest_file.exists():
                    try:
                        shutil.copy2(src_file, dest_file)
                        print(
                            f"[OK] Collected Step-4c output: {filename} -> {out_dir.name}/ (from {search_dir.name})"
                        )
                        collected_count += 1
                        break
                    except Exception as e:
                        print(
                            f"[WARN] Failed to collect {filename}: {type(e).__name__}: {e}"
                        )

    # Copy input/dependency files (ì˜ì¡´ì„± íŒŒì¼ ë° ê¸°ë³¸ ë¡œìš° ë°ì´í„° íŒŒì¼)
    if inputs_dir is None:
        inputs_dir = base_dir

    input_files = [
        "tank_catalog_from_tankmd.json",
        "bplus_inputs/Hydro_Table_Engineering.json",
        "GM_Min_Curve.json",
        "Acceptance_Criteria.json",
        "Structural_Limits.json",
        "Securing_Input.json",
        "ISCODE_Criteria.json",
    ]

    # Create inputs subdirectory in output
    inputs_out_dir = out_dir / "inputs"
    inputs_out_dir.mkdir(exist_ok=True)

    for input_file in input_files:
        src_file = inputs_dir / input_file
        if src_file.exists() and src_file.is_file():
            # Preserve directory structure for bplus_inputs
            if input_file.startswith("bplus_inputs/"):
                dest_file = inputs_out_dir / input_file
                dest_file.parent.mkdir(parents=True, exist_ok=True)
            else:
                dest_file = inputs_out_dir / Path(input_file).name

            if not dest_file.exists():
                try:
                    shutil.copy2(src_file, dest_file)
                    print(f"[OK] Collected input file: {input_file} -> inputs/")
                    collected_count += 1
                except Exception as e:
                    print(
                        f"[WARN] Failed to collect {input_file}: {type(e).__name__}: {e}"
                    )

    if collected_count > 0:
        print(f"[OK] Total {collected_count} file(s) collected to output directory")
    else:
        print(
            "[INFO] No additional files to collect (all outputs already in output directory)"
        )


def merge_excel_files_to_one(
    out_dir: Path,
    site: str,
    base_dir: Optional[Path] = None,
    include_sequence: bool = False,
) -> Optional[Path]:
    """
    Merge all Excel files generated by pipeline into one consolidated Excel file.
    ëª¨ë“  íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ëœ Excel íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ í†µí•© Excel íŒŒì¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
    """
    try:
        import openpyxl
        from openpyxl import load_workbook, Workbook
    except ImportError:
        print("[WARN] openpyxl not available. Skipping Excel merge.")
        return None

    # Excel files to merge (in order)
    excel_files = [
        f"LCT_BUSHRA_{site}_TR_Final_v*.xlsx",  # TR Excel
        f"OPS_FINAL_R3_{site}_Ballast_Integrated.xlsx",  # OPS Final
        "optimizer_ballast_plan.xlsx",  # Optimizer
    ]

    # Search directories: out_dir first, then base_dir and base_dir.parent if provided
    search_dirs = [out_dir]
    if base_dir:
        search_dirs.extend([base_dir, base_dir.parent])

    # Find all Excel files that exist
    found_files = []
    for pattern in excel_files:
        file_found = False
        for search_dir in search_dirs:
            if "*" in pattern:
                matches = list(search_dir.glob(pattern))
                if matches:
                    # Get latest file if multiple matches
                    latest_file = sorted(
                        matches, key=lambda p: p.stat().st_mtime, reverse=True
                    )[0]
                    found_files.append(latest_file)
                    file_found = True
                    print(
                        f"[INFO] Found Excel file: {latest_file.name} (in {search_dir.name})"
                    )
                    break
            else:
                file_path = search_dir / pattern
                if file_path.exists():
                    found_files.append(file_path)
                    file_found = True
                    print(
                        f"[INFO] Found Excel file: {file_path.name} (in {search_dir.name})"
                    )
                    break
        if not file_found:
            print(f"[WARN] Excel file not found: {pattern}")

    if not found_files:
        print("[INFO] No Excel files found to merge")
        return None

    # Create merged workbook
    merged_wb = Workbook()
    merged_wb.remove(merged_wb.active)  # Remove default sheet
    try:
        merged_wb.calculation.calcMode = "auto"
        merged_wb.calculation.fullCalcOnLoad = True
        merged_wb.calculation.calcOnSave = True
    except Exception:
        pass

    sheet_count = 0
    existing_titles = set(ws.title for ws in merged_wb.worksheets)

    def _copy_sheet_values_only(source_sheet, target_sheet) -> None:
        for row in source_sheet.iter_rows(values_only=True):
            target_sheet.append(row)

    for excel_file in found_files:
        try:
            print(f"[INFO] Merging: {excel_file.name}")
            source_wb = load_workbook(excel_file, data_only=False)

            for sheet_name in source_wb.sheetnames:
                source_sheet = source_wb[sheet_name]

                # Create unique sheet name if duplicate
                target_sheet_name = sheet_name
                if target_sheet_name in existing_titles:
                    # Add prefix from source filename
                    prefix = excel_file.stem[:15]  # Use first 15 chars of filename
                    target_sheet_name = f"{prefix}_{sheet_name}"[
                        :31
                    ]  # Excel limit is 31 chars

                # Copy sheet (values/formulas only, skip styles to avoid StyleProxy errors)
                merged_sheet = merged_wb.create_sheet(title=target_sheet_name)
                existing_titles.add(target_sheet_name)

                # Copy all data (including formulas)
                _copy_sheet_values_only(source_sheet, merged_sheet)

                # Copy column dimensions
                try:
                    for col, dim in source_sheet.column_dimensions.items():
                        merged_sheet.column_dimensions[col].width = dim.width
                except Exception:
                    pass  # Skip if dimension copy fails

                # Copy merged cell ranges (without styles)
                try:
                    for merged in source_sheet.merged_cells.ranges:
                        merged_sheet.merge_cells(str(merged))
                except Exception:
                    pass

                sheet_count += 1
                print(f"  [OK] Copied sheet: {target_sheet_name}")

            source_wb.close()

        except Exception as e:
            print(f"[WARN] Failed to merge {excel_file.name}: {type(e).__name__}: {e}")
            continue

    if sheet_count == 0:
        print("[WARN] No sheets were copied")
        return None

    if include_sequence:
        seq_xlsx = out_dir / "BALLAST_SEQUENCE.xlsx"
        if seq_xlsx.exists():
            try:
                print(f"[INFO] Adding sequence sheets from {seq_xlsx.name}")
                seq_wb = load_workbook(seq_xlsx, data_only=False)
                for sheet_name in seq_wb.sheetnames:
                    src_sheet = seq_wb[sheet_name]
                    base_name = f"Sequence_{sheet_name}"[:31]
                    target_name = base_name
                    if target_name in existing_titles:
                        suffix = 1
                        base_trim = base_name[:27]
                        while target_name in existing_titles:
                            target_name = f"{base_trim}_{suffix}"[:31]
                            suffix += 1

                    dst_sheet = merged_wb.create_sheet(title=target_name)
                    existing_titles.add(target_name)

                    _copy_sheet_values_only(src_sheet, dst_sheet)

                    try:
                        for merged in src_sheet.merged_cells.ranges:
                            dst_sheet.merge_cells(str(merged))
                    except Exception:
                        pass

                    sheet_count += 1
                    print(f"  [OK] Copied sequence sheet: {target_name}")

                seq_wb.close()
            except Exception as e:
                print(
                    f"[WARN] Failed to merge BALLAST_SEQUENCE.xlsx: {type(e).__name__}: {e}"
                )

    # PATCH: Update Summary sheets with Solver results (New_FWD_m/New_AFT_m)
    # OPTIMIZED: Bulk update using values_only + batch assignment instead of cell-by-cell
    solver_summary_csv = out_dir / "solver_ballast_summary.csv"
    if solver_summary_csv.exists():
        try:
            import pandas as pd

            solver_df = pd.read_csv(solver_summary_csv, encoding="utf-8-sig")
            if "New_FWD_m" in solver_df.columns and "New_AFT_m" in solver_df.columns:
                print("[INFO] Updating Summary sheets with Solver results...")
                # Create lookup dict for faster matching
                solver_lookup = dict(
                    zip(
                        solver_df["Stage"].astype(str).str.strip(),
                        zip(solver_df["New_FWD_m"], solver_df["New_AFT_m"]),
                    )
                )

                for sheet_name in merged_wb.sheetnames:
                    if "Summary" in sheet_name or "Stage" in sheet_name:
                        ws = merged_wb[sheet_name]
                        stage_col = None
                        fwd_col = None
                        aft_col = None
                        for c in range(1, min(ws.max_column + 1, 30)):
                            header = str(ws.cell(1, c).value or "").strip()
                            if "Stage" in header and stage_col is None:
                                stage_col = c
                            if (
                                "New_FWD" in header or "FWD" in header
                            ) and fwd_col is None:
                                fwd_col = c
                            if (
                                "New_AFT" in header or "AFT" in header
                            ) and aft_col is None:
                                aft_col = c

                        if stage_col and fwd_col and aft_col:
                            # Bulk read rows (values_only for performance)
                            updates = []
                            for r in range(2, ws.max_row + 1):
                                stage_name = str(
                                    ws.cell(r, stage_col).value or ""
                                ).strip()
                                if stage_name and stage_name in solver_lookup:
                                    new_fwd, new_aft = solver_lookup[stage_name]
                                    updates.append((r, fwd_col, new_fwd))
                                    updates.append((r, aft_col, new_aft))

                            # Batch update (faster than individual cell assignments)
                            if updates:
                                for r, col, val in updates:
                                    ws.cell(r, col).value = val
                                print(
                                    f"  [OK] Updated {len(updates) // 2} rows in sheet: {sheet_name}"
                                )
        except Exception as e:
            print(
                f"[WARN] Failed to update Summary with Solver results: {type(e).__name__}: {e}"
            )

    # Save merged workbook
    merged_file = out_dir / f"PIPELINE_CONSOLIDATED_{site}_{now_tag()}.xlsx"
    try:
        merged_wb.save(merged_file)
        print(f"[OK] Merged Excel saved: {merged_file.name} ({sheet_count} sheets)")
        return merged_file
    except Exception as e:
        print(f"[ERROR] Failed to save merged Excel: {type(e).__name__}: {e}")
        return None


def _norm_stage_name(s: object) -> str:
    """
    Canonical stage key for Excel/CSV matching.
    - Removes NBSP/zero-width chars
    - Normalizes unicode
    - Collapses whitespace
    - Unifies separators and strips punctuation (keeps alnum)
    Example:
      "Stage 6A_Critical (Opt C)" -> "stage6acriticaloptc"
      " stage\u00a01 " -> "stage1"
    """
    if s is None:
        return ""
    x = str(s)
    x = unicodedata.normalize("NFKC", x)
    x = (
        x.replace("\u00a0", " ")
        .replace("\u200b", "")
        .replace("\u200c", "")
        .replace("\u200d", "")
    )
    x = re.sub(r"\s+", " ", x).strip().lower()
    x = x.replace("-", " ").replace("_", " ")
    x = re.sub(r"\s+", " ", x).strip()
    x = re.sub(r"[^a-z0-9]", "", x)
    # Strip known suffixes that appear only on Excel variants
    # e.g., "stage6ctotalmassopt" -> "stage6c"
    suffixes = ["totalmassopt", "totalmass", "optc", "opt"]
    for suf in suffixes:
        if x.endswith(suf) and len(x) > len(suf) + 1:
            x = x[: -len(suf)]
            break
    return x


def _canon_stage_key(k: str) -> str:
    # Canonical alias map for known variants
    alias = {
        "stage6ctotalmassopt": "stage6c",
        "stage6ctotalmass": "stage6c",
    }
    return alias.get(k, k)


def _find_header_row_and_stage_col(ws, max_scan_rows: int = 60):
    for r in range(1, max_scan_rows + 1):
        for c in range(1, ws.max_column + 1):
            v = ws.cell(row=r, column=c).value
            if v is None:
                continue
            if str(v).strip().lower() in ("stage", "stage_name", "stagename"):
                return r, c
    return None, None


def _find_best_header(
    ws, max_scan_rows: int = 60, sample_rows: int = 10, min_ok: int = 3
):
    """
    Find best header row/Stage column by validating Stage-like values beneath header.
    Prefers headers where Stage column is in Column 1 (main data table).
    Returns (header_row, stage_col, score) or (None, None, 0.0).
    """
    best = (None, None, 0.0)
    for r in range(1, max_scan_rows + 1):
        for c in range(1, ws.max_column + 1):
            v = ws.cell(row=r, column=c).value
            if v is None:
                continue
            if str(v).strip().lower() not in ("stage", "stage_name", "stagename"):
                continue
            sample_ok = 0
            sample_total = 0
            for rr in range(r + 1, min(r + 1 + sample_rows, ws.max_row + 1)):
                vv = ws.cell(row=rr, column=c).value
                if vv is None:
                    continue
                sample_total += 1
                if _norm_stage_name(vv).startswith("stage"):
                    sample_ok += 1
            if sample_ok < min_ok:
                continue
            score = sample_ok / max(1, sample_total)
            # Bonus for Stage column being Column 1 (primary data table indicator)
            if c == 1:
                score = score * 1.5
            if score > best[2]:
                best = (r, c, score)
    return best


def _ensure_column(ws, header_row: int, header_name: str) -> int:
    header_name_norm = header_name.strip().lower()
    for c in range(1, ws.max_column + 1):
        v = ws.cell(row=header_row, column=c).value
        if v is None:
            continue
        if str(v).strip().lower() == header_name_norm:
            return c
    new_c = ws.max_column + 1
    ws.cell(row=header_row, column=new_c).value = header_name
    return new_c


def update_excel_with_solver_results(
    merged_excel_path: Path,
    qa_csv_path: Path,
    sheet_hint: Optional[str] = "RORO",
    update_all_matching_sheets: bool = False,
    max_scan_rows: int = 60,
    debug: bool = False,
) -> None:
    """
    Inject post-solve results from pipeline_stage_QA.csv into consolidated Excel.
    Adds/updates columns:
      - Draft_FWD_m_solver
      - Draft_AFT_m_solver
      - Draft_Source
      - GateA_AFT_MIN_2p70_PASS
      - Gate_Freeboard_ND / Freeboard_Req_ND_m / Freeboard_ND_Margin_m
    """
    if not merged_excel_path.exists() or not qa_csv_path.exists():
        print("[WARN] Cannot inject solver results: files missing")
        return

    try:
        import pandas as pd
        from openpyxl import load_workbook
    except Exception as e:
        print(f"[WARN] Cannot inject solver results (missing deps): {e}")
        return

    try:
        qa = pd.read_csv(qa_csv_path, encoding="utf-8-sig")
    except Exception as e:
        print(f"[WARN] QA CSV read failed: {e}")
        return

    if "Stage" not in qa.columns:
        print("[WARN] QA CSV missing 'Stage' column; cannot map rows.")
        return

    col_map = {
        "Draft_FWD_m_solver": "Draft_FWD_m",
        "Draft_AFT_m_solver": "Draft_AFT_m",
        "Draft_Source": "Draft_Source",
        # Tide/UKC columns
        "Forecast_tide_m": "Forecast_tide_m",
        "DepthRef_m": "DepthRef_m",
        "UKC_min_m": "UKC_min_m",
        "UKC_fwd_m": "UKC_fwd_m",
        "UKC_aft_m": "UKC_aft_m",
        "Tide_required_m": "Tide_required_m",
        "Tide_margin_m": "Tide_margin_m",
        "Tide_verdict": "Tide_verdict",
        # Freeboard columns (Option C clarification)
        "Freeboard_Min_m": "Freeboard_Min_m",
        "Freeboard_Min_BowStern_m": "Freeboard_Min_BowStern_m",
        "Freeboard_FWD_m": "Freeboard_FWD_m",
        "Freeboard_AFT_m": "Freeboard_AFT_m",
        # Gate-A SSOT (preferred)
        "Gate_AFT_MIN_2p70_PASS": "Gate_AFT_MIN_2p70_PASS",
        "AFT_Margin_2p70_m": "AFT_Margin_2p70_m",
        # Gate-B SSOT (preferred, critical-only)
        "Gate_B_Applies": "Gate_B_Applies",
        "Gate_FWD_MAX_2p70_critical_only": "Gate_FWD_MAX_2p70_critical_only",
        "FWD_Margin_2p70_m": "FWD_Margin_2p70_m",
        # ND gate
        "Gate_Freeboard_ND": "Gate_Freeboard_ND",
        "Freeboard_Req_ND_m": "Freeboard_Req_ND_m",
        "Freeboard_ND_Margin_m": "Freeboard_ND_Margin_m",
        # Draft clipping flags
        "D_vessel_m": "D_vessel_m",
        "Draft_Max_raw_m": "Draft_Max_raw_m",
        "Draft_Max_solver_m": "Draft_Max_solver_m",
        "Draft_Clipped_raw": "Draft_Clipped_raw",
        "Draft_Clipped_solver": "Draft_Clipped_solver",
    }

    # Backward compatibility: map legacy columns if SSOT columns missing
    if (
        "Gate_AFT_MIN_2p70_PASS" not in qa.columns
        and "GateA_AFT_MIN_2p70_PASS" in qa.columns
    ):
        qa["Gate_AFT_MIN_2p70_PASS"] = qa["GateA_AFT_MIN_2p70_PASS"]
    if (
        "AFT_Margin_2p70_m" not in qa.columns
        and "GateA_AFT_MIN_2p70_Margin_m" in qa.columns
    ):
        qa["AFT_Margin_2p70_m"] = qa["GateA_AFT_MIN_2p70_Margin_m"]
    if (
        "Gate_B_Applies" not in qa.columns
        and "GateB_FWD_MAX_2p70_CD_applicable" in qa.columns
    ):
        qa["Gate_B_Applies"] = qa["GateB_FWD_MAX_2p70_CD_applicable"]
    if (
        "Gate_FWD_MAX_2p70_critical_only" not in qa.columns
        and "GateB_FWD_MAX_2p70_CD_PASS" in qa.columns
    ):
        qa["Gate_FWD_MAX_2p70_critical_only"] = qa["GateB_FWD_MAX_2p70_CD_PASS"].apply(
            lambda v: "OK" if bool(v) else "NG"
        )
    if (
        "FWD_Margin_2p70_m" not in qa.columns
        and "GateB_FWD_MAX_2p70_CD_Margin_m" in qa.columns
    ):
        qa["FWD_Margin_2p70_m"] = qa["GateB_FWD_MAX_2p70_CD_Margin_m"]
    if "Forecast_tide_m" not in qa.columns and "Forecast_Tide_m" in qa.columns:
        qa["Forecast_tide_m"] = qa["Forecast_Tide_m"]
    if "DepthRef_m" not in qa.columns and "Depth_Ref_m" in qa.columns:
        qa["DepthRef_m"] = qa["Depth_Ref_m"]
    if "UKC_min_m" not in qa.columns and "UKC_Min_m" in qa.columns:
        qa["UKC_min_m"] = qa["UKC_Min_m"]
    if "UKC_min_m" not in qa.columns and "UKC_Min_Required_m" in qa.columns:
        qa["UKC_min_m"] = qa["UKC_Min_Required_m"]
    if "UKC_fwd_m" not in qa.columns and "UKC_FWD_m" in qa.columns:
        qa["UKC_fwd_m"] = qa["UKC_FWD_m"]
    if "UKC_aft_m" not in qa.columns and "UKC_AFT_m" in qa.columns:
        qa["UKC_aft_m"] = qa["UKC_AFT_m"]
    if "Tide_required_m" not in qa.columns and "Required_WL_for_UKC_m" in qa.columns:
        qa["Tide_required_m"] = qa["Required_WL_for_UKC_m"]
    if "Tide_verdict" not in qa.columns and "Tide_verification" in qa.columns:
        qa["Tide_verdict"] = qa["Tide_verification"]

    qa["_stage_norm"] = qa["Stage"].apply(
        lambda s: _canon_stage_key(_norm_stage_name(s))
    )
    dup = qa["_stage_norm"].value_counts()
    dup = dup[dup > 1]
    if len(dup) > 0:
        print(f"[WARN] Duplicate stage keys after normalization: {dup.to_dict()}")
        qa = qa.drop_duplicates(subset=["_stage_norm"], keep="first")
    qa_idx = qa.set_index("_stage_norm")
    if "" in qa_idx.index:
        print("[WARN] Empty stage key found in QA CSV after normalization.")

    try:
        wb = load_workbook(merged_excel_path)
    except Exception as e:
        print(f"[WARN] Failed to open merged Excel: {e}")
        return

    # choose target sheet(s)
    candidates = []
    for name in wb.sheetnames:
        ws0 = wb[name]
        hr, sc, score = _find_best_header(ws0, max_scan_rows=max_scan_rows)
        if hr and sc:
            candidates.append((ws0, hr, sc, score))
    if not candidates:
        print("[WARN] No suitable sheet found to update with solver results.")
        wb.close()
        return

    if update_all_matching_sheets:
        target_list = candidates
        if debug:
            print(
                f"[INFO] Multi-sheet mode ON: updating {len(target_list)} candidate sheet(s)"
            )
    else:
        hinted = (
            [
                c
                for c in candidates
                if sheet_hint and sheet_hint.lower() in c[0].title.lower()
            ]
            if sheet_hint
            else []
        )
        if hinted:
            target_list = [max(hinted, key=lambda x: x[3])]
        else:
            target_list = [max(candidates, key=lambda x: x[3])]
        if debug:
            ws0, hr0, sc0, score0 = target_list[0]
            print(
                f"[INFO] Single-sheet mode: picked sheet='{ws0.title}', "
                f"header_row={hr0}, stage_col={sc0}, score={score0:.2f}"
            )

    total_updated = 0
    for ws, header_row, stage_col, score in target_list:
        out_cols: Dict[str, int] = {}
        for out_name in col_map.keys():
            out_cols[out_name] = _ensure_column(ws, header_row, out_name)
        if debug:
            print(
                f"[DEBUG] Created/found columns on '{ws.title}': {[(k, v) for k, v in out_cols.items()]}"
            )

        # OPTIMIZED: Batch collect updates, then apply in one pass
        updates = []  # List of (row, col, value) tuples
        updated_rows = set()  # Track unique rows updated
        misses = 0
        for r in range(header_row + 1, ws.max_row + 1):
            stage_val = ws.cell(row=r, column=stage_col).value
            if stage_val is None:
                continue
            key = _canon_stage_key(_norm_stage_name(stage_val))
            if not key or key not in qa_idx.index:
                misses += 1
                if debug and misses <= 20:
                    print(
                        f"[WARN] Stage not found: excel={repr(stage_val)} norm={repr(key)} "
                        f"sheet={ws.title} row={r}"
                    )
                continue
            row = qa_idx.loc[key]
            if not isinstance(row, pd.Series):
                if isinstance(row, pd.DataFrame):
                    row = row.iloc[0]
                else:
                    if debug:
                        print(
                            f"[WARN] Unexpected row type: {type(row)} for key={repr(key)}"
                        )
                    continue
            updated_rows.add(r)  # Track this row as updated
            for out_name, src_name in col_map.items():
                if src_name not in qa.columns:
                    continue
                try:
                    v = (
                        row[src_name]
                        if isinstance(row, pd.Series)
                        else row.get(src_name)
                    )
                except (KeyError, AttributeError):
                    v = None
                # Special handling for Gate_FWD_MAX: inject "N/A" instead of None for non-critical stages
                if out_name == "Gate_FWD_MAX_2p70_critical_only" and pd.isna(v):
                    updates.append((r, out_cols[out_name], "N/A"))
                else:
                    updates.append((r, out_cols[out_name], None if pd.isna(v) else v))

        # Batch apply all updates (faster than individual cell assignments)
        for r, col, val in updates:
            ws.cell(row=r, column=col).value = val

        updated = len(updated_rows)

        total_updated += updated
        if debug:
            print(
                f"[DEBUG] sheet='{ws.title}' header_row={header_row} stage_col={stage_col} "
                f"score={score:.2f} updated={updated} misses={misses}"
            )
        print(f"[OK] Excel sheet updated: sheet='{ws.title}', rows={updated}")

    # Add/update a Tide summary sheet
    tide_cols = [
        "Stage",
        "Forecast_tide_m",
        "DepthRef_m",
        "UKC_min_m",
        "Tide_required_m",
        "Tide_margin_m",
        "UKC_fwd_m",
        "UKC_aft_m",
        "Tide_verdict",
    ]
    tide_avail = [c for c in tide_cols if c in qa.columns]
    if tide_avail:
        if "TIDE_BY_STAGE" in wb.sheetnames:
            del wb["TIDE_BY_STAGE"]
        ws_tide = wb.create_sheet("TIDE_BY_STAGE")
        for c_idx, name in enumerate(tide_avail, start=1):
            ws_tide.cell(row=1, column=c_idx, value=name)
        for r_idx, (_, row) in enumerate(qa[tide_avail].iterrows(), start=2):
            for c_idx, name in enumerate(tide_avail, start=1):
                v = row.get(name)
                ws_tide.cell(row=r_idx, column=c_idx, value=None if pd.isna(v) else v)
        print(
            f"[OK] TIDE_BY_STAGE sheet updated: cols={len(tide_avail)} rows={len(qa)}"
        )
    else:
        print("[WARN] TIDE_BY_STAGE not created: no tide/ukc columns found in QA CSV.")

    try:
        wb.save(merged_excel_path)
        print(
            f"[OK] Excel updated with solver results: total_rows={total_updated}, file={merged_excel_path.name}"
        )
    except Exception as e:
        print(f"[WARN] Failed to save merged Excel after injection: {e}")
    finally:
        wb.close()


def _sha256(p: Path) -> str:
    h = hashlib.sha256()
    with open(p, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def create_final_output_folder(
    base_dir: Path, out_dir: Path, merged_excel: Optional[Path]
) -> Path:
    ts = now_tag()
    final_dir = base_dir / f"final_output_{ts}"
    final_dir.mkdir(parents=True, exist_ok=True)

    candidates: List[Path] = []
    ssot_dir = out_dir / "ssot"

    if merged_excel and Path(merged_excel).exists():
        candidates.append(Path(merged_excel))

    for rel in [
        "pipeline_stage_QA.csv",
        "stage_table_unified.csv",
        "tank_ssot_for_solver.csv",
        "tank_ssot_for_solver__aftmin.csv",
        "hydro_table_for_solver.csv",
    ]:
        p = ssot_dir / rel
        if p.exists():
            candidates.append(p)

    for rel in [
        "gate_fail_report.md",
        "TUG_Operational_SOP_DNV_ST_N001.md",
        "OPS_FINAL_R3_Report_Integrated.md",
        "solver_ballast_stage_plan.csv",
        "solver_ballast_summary.csv",
        "BALLAST_SEQUENCE.csv",
        "BALLAST_OPTION.csv",
        "BALLAST_EXEC.csv",
        "BALLAST_OPERATIONS_CHECKLIST.md",
        "BALLAST_SEQUENCE_WITH_VALVES.md",
        "HOLD_POINT_SUMMARY.csv",
    ]:
        p = out_dir / rel
        if p.exists():
            candidates.append(p)

    for p in out_dir.glob("*.xlsx"):
        candidates.append(p)

    manifest = {
        "created": ts,
        "base_dir": str(base_dir),
        "out_dir": str(out_dir),
        "files": [],
    }
    seen = set()
    for p in candidates:
        if not p.exists():
            continue
        if p.resolve() in seen:
            continue
        seen.add(p.resolve())
        dst = final_dir / p.name
        try:
            shutil.copy2(p, dst)
            manifest["files"].append(
                {
                    "name": dst.name,
                    "sha256": _sha256(dst),
                    "size": dst.stat().st_size,
                }
            )
        except Exception as e:
            print(f"[WARN] Failed to copy {p.name}: {e}")

    (final_dir / "manifest.json").write_text(
        json.dumps(manifest, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    print(f"[OK] Final output folder: {final_dir}")
    return final_dir


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main() -> int:
    ap = argparse.ArgumentParser(
        description="Integrated Pipeline (Definition-split aware)"
    )

    ap.add_argument(
        "--site",
        choices=["AGI", "DAS"],
        default="AGI",
        help="Site label for logging (does not change math automatically).",
    )

    ap.add_argument(
        "--profile_json",
        default="",
        help="Site profile JSON (AGI/DAS). Overrides defaults unless the CLI flag is explicitly provided.",
    )
    ap.add_argument(
        "--current_t_csv",
        default="",
        help="Sensor/PLC/IoT CSV for Current_t injection into Tank SSOT (optional).",
    )
    ap.add_argument(
        "--current_t_strategy",
        choices=["override", "fill_missing"],
        default="override",
        help="How to apply sensor values to Current_t: override | fill_missing.",
    )
    ap.add_argument(
        "--no_gate_report",
        action="store_true",
        help="Disable Gate FAIL auto report generation.",
    )
    ap.add_argument(
        "--debug_report",
        action="store_true",
        help="Generate debug feasibility report after QA CSV generation.",
    )
    ap.add_argument(
        "--auto_debug_report",
        action="store_true",
        default=False,
        help="Automatically generate debug report (default: False).",
    )
    ap.add_argument(
        "--base_dir",
        default="",
        help="Folder containing the 4 scripts (default: this script folder).",
    )
    ap.add_argument(
        "--inputs_dir",
        default="",
        help="Folder containing shared JSON inputs (default: base_dir).",
    )
    ap.add_argument(
        "--out_dir",
        default="",
        help="Output folder (default: base_dir/pipeline_out_<timestamp>).",
    )

    # Step control
    ap.add_argument("--from_step", type=int, default=1, help="Start step (0..5).")
    ap.add_argument("--to_step", type=int, default=5, help="End step (0..5).")
    ap.add_argument(
        "--enable-sequence",
        action="store_true",
        help="Enable Step 4b: Ballast Sequence & Checklist (standalone modules).",
    )
    ap.add_argument(
        "--enable-valve-lineup",
        action="store_true",
        help="Enable Step 4c: Valve Lineup (requires valve_lineup_generator.py).",
    )

    # Script paths (override if needed)
    ap.add_argument("--tr_script", default="agi_tr_patched_v6_6_defsplit_v1.py")
    ap.add_argument(
        "--ops_script",
        default="ops_final_r3_integrated_defs_split_v4_patched_TIDE_v1.py",
    )
    ap.add_argument("--solver_script", default="ballast_gate_solver_v4_TIDE_v1.py")
    ap.add_argument("--optimizer_script", default="Untitled-2_patched_defsplit_v1_1.py")
    ap.add_argument(
        "--spmt_script",
        default="spmt v1/agi_spmt_unified.py",
        help="AGI SPMT unified script (optional Step 0).",
    )
    ap.add_argument(
        "--bryan_template_script",
        default="tide/bryan_template_unified_TIDE_v1.py",
        help="Bryan template unified script (Step 5).",
    )
    ap.add_argument(
        "--headers-registry",
        default="",
        help="Path to headers_registry.json for SSOT header management (optional). Auto-compiles from HEADERS_MASTER.xlsx if needed.",
    )
    ap.add_argument(
        "--enable-headers-ssot",
        action="store_true",
        default=False,
        help="Enable headers SSOT for all outputs (requires --headers-registry).",
    )
    ap.add_argument(
        "--head-registry",
        default="",
        help="Path to HEAD_REGISTRY_AGI_v3.0.yaml for header validation (optional).",
    )
    ap.add_argument(
        "--auto-head-guard",
        action="store_true",
        default=False,
        help="Automatically run Head Guard validation after pipeline execution (requires --head-registry).",
    )

    # Input files (override if needed)
    ap.add_argument("--tank_catalog", default="tank_catalog_from_tankmd.json")
    ap.add_argument(
        "--hydro",
        default="",
        help="Hydro table json (default: inputs_dir/bplus_inputs/Hydro_Table_Engineering.json)",
    )
    ap.add_argument(
        "--stage_results",
        default="",
        help="stage_results.csv (default: generated by TR script 'csv' mode)",
    )
    ap.add_argument(
        "--spmt_config",
        default="spmt v1/spmt_shuttle_example_config_AGI_FR_M.json",
        help="SPMT config JSON for agi_spmt_unified.py (optional Step 0).",
    )

    # Gates / Limits (definition-split aligned)
    ap.add_argument(
        "--fwd_max", type=float, default=2.70, help="FWD maximum draft gate (m)."
    )
    ap.add_argument(
        "--aft_min",
        type=float,
        default=2.70,
        help="AFT minimum draft gate (m) (Captain / prop immersion).",
    )
    ap.add_argument(
        "--aft_max",
        type=float,
        default=3.50,
        help="AFT maximum draft limit (m) for optimizer (upper limit).",
    )
    ap.add_argument(
        "--trim_abs_limit",
        type=float,
        default=0.50,
        help="Absolute trim limit (m) for optimizer stage mode.",
    )
    ap.add_argument(
        "--trim-limit-enforced",
        dest="trim_limit_enforced",
        action="store_true",
        default=True,
        help="Enforce trim limit as hard constraint (default: enabled).",
    )
    ap.add_argument(
        "--no-trim-limit-enforced",
        dest="trim_limit_enforced",
        action="store_false",
        help="Disable hard enforcement of trim limit.",
    )
    ap.add_argument(
        "--freeboard-min-m",
        type=float,
        default=0.0,
        help="Minimum freeboard requirement (m).",
    )
    ap.add_argument(
        "--freeboard-min-enforced",
        dest="freeboard_min_enforced",
        action="store_true",
        default=True,
        help="Enforce minimum freeboard as hard constraint (default: enabled).",
    )
    ap.add_argument(
        "--no-freeboard-min-enforced",
        dest="freeboard_min_enforced",
        action="store_false",
        help="Disable hard enforcement of minimum freeboard.",
    )
    ap.add_argument(
        "--stateful_solver",
        "--stateful",
        action="store_true",
        help="Enable stateful solver (carry-forward Current_t across stages).",
    )
    ap.add_argument(
        "--reset_tank_state",
        default="",
        help="Comma list or regex (prefix 're:') to reset Current_t from SSOT.",
    )
    ap.add_argument(
        "--state_trace_csv",
        default="",
        help="Optional CSV path for stateful solver tank snapshots.",
    )
    ap.add_argument(
        "--tank_operability_json",
        default="",
        help="Tank operability/profile JSON for PRE_BALLAST_ONLY enforcement.",
    )
    ap.add_argument(
        "--operational_stage_regex",
        default=r"(6a|critical|ramp|roll|loadout)",
        help="Regex to decide operational stages for PRE_BALLAST_ONLY enforcement.",
    )
    ap.add_argument(
        "--disable_preballast_only_on_operational_stages",
        action="store_true",
        default=False,
        help="Disable PRE_BALLAST_ONLY enforcement on operational stages.",
    )

    # Tide/Depth/UKC optional inputs (for solver UKC split)
    ap.add_argument(
        "--forecast_tide", type=float, default=None, help="Forecast tide (m, CD)."
    )
    ap.add_argument(
        "--depth_ref",
        type=float,
        default=None,
        help="Reference depth (m, CD) for UKC calculation.",
    )
    ap.add_argument(
        "--ukc_min", type=float, default=None, help="Minimum UKC requirement (m)."
    )
    ap.add_argument(
        "--squat", type=float, default=0.0, help="Squat allowance (m) for solver UKC."
    )
    ap.add_argument(
        "--safety_allow",
        type=float,
        default=0.0,
        help="Additional safety allowance (m) for solver UKC.",
    )

    # Tide table simulation (optional)
    ap.add_argument(
        "--tide_table",
        default="",
        help="Optional official tide table file (csv/xlsx/json). Used with --stage_schedule to assign per-stage Forecast_Tide_m by interpolation.",
    )
    ap.add_argument(
        "--stage_schedule",
        default="",
        help="Optional stage schedule file (csv/xlsx) containing Stage + Timestamp/Datetime. Used with --tide_table.",
    )
    ap.add_argument(
        "--stage_tide_csv",
        default="",
        help="Pre-computed stage tide CSV (Stage/StageKey + Forecast_tide_m columns). Highest priority for tide assignment. Generated by tide_stage_mapper.py.",
    )
    ap.add_argument(
        "--tide_strategy",
        choices=["keep_csv", "override_from_table"],
        default="keep_csv",
        help="If Forecast_Tide_m already exists in stage_table, keep it or override it with tide_table interpolation.",
    )
    ap.add_argument(
        "--tide_tol",
        type=float,
        default=DEFAULT_TIDE_TOL_M,
        help="Tide margin tolerance (m) for LIMIT vs OK (default 0.10m).",
    )
    ap.add_argument(
        "--hmax_wave_m",
        type=float,
        default=None,
        help="Maximum wave height (m) for GL Noble Denton 0013/ND freeboard check. "
        "Freeboard_Req = 0.50 + 0.50*Hmax (with 4-corner monitoring) "
        "or 0.80 + 0.50*Hmax (without).",
    )
    ap.add_argument(
        "--four_corner_monitoring",
        action="store_true",
        default=False,
        help="Enable 4-corner freeboard monitoring (reduces ND freeboard requirement).",
    )
    ap.add_argument(
        "--gate_guard_band_cm",
        type=float,
        default=2.0,
        help="Gate guard-band tolerance in cm (default 2.0cm). "
        "Applied to Gate-A (AFT_MIN_2p70) and Gate-B (FWD_MAX_2p70_critical_only). "
        "Production: 2.0cm, Development: 1.0cm, Strict: 0.0cm.",
    )

    # Data conversion options
    ap.add_argument(
        "--pump_rate",
        type=float,
        default=100.0,
        help="Default pump rate (t/h) used when generating solver tank CSV.",
    )
    ap.add_argument(
        "--tank_keywords",
        default="BALLAST,VOID,FWB,FW,DB",
        help="Comma keywords to mark use_flag=Y in tank conversion.",
    )
    ap.add_argument(
        "--exclude_fwd_tanks",
        action="store_true",
        default=False,
        help="Exclude FWD tanks (x_from_mid_m < 0) from solver by setting use_flag=N after profile overrides.",
    )
    ap.add_argument(
        "--exclude_fwd_tanks_aftmin_only",
        action="store_true",
        default=False,
        help="For AFT-min stages, set FWD tanks to DISCHARGE_ONLY (fill prohibited, discharge allowed). "
        "Uses stage flag Ban_FWD_Tanks for per-stage handling.",
    )

    # Execution
    ap.add_argument(
        "--dry_run",
        action="store_true",
        help="Print resolved paths and planned actions, then exit.",
    )
    args = ap.parse_args()

    script_dir = Path(__file__).parent.resolve()
    if args.base_dir:
        base_dir = Path(args.base_dir).resolve()
    else:
        base_dir = script_dir.parent if script_dir.name == "tide" else script_dir
    inputs_dir = Path(args.inputs_dir).resolve() if args.inputs_dir else base_dir

    # Headers SSOT support
    registry_path = None
    if args.enable_headers_ssot:
        if args.headers_registry:
            registry_path = Path(args.headers_registry)
            if not registry_path.is_absolute():
                registry_path = base_dir / registry_path
        else:
            # Auto-compile from HEADERS_MASTER.xlsx if available
            excel_master = base_dir / "HEADERS_MASTER.xlsx"
            json_registry = base_dir / "headers_registry.json"
            if excel_master.exists():
                try:
                    # Add base_dir to path for import
                    import sys

                    if str(base_dir) not in sys.path:
                        sys.path.insert(0, str(base_dir))
                    from compile_headers_registry import compile_from_excel

                    compile_needed = (
                        not json_registry.exists()
                        or excel_master.stat().st_mtime > json_registry.stat().st_mtime
                    )
                    if compile_needed:
                        compile_from_excel(excel_master, json_registry)
                        print(f"[OK] Headers registry compiled: {json_registry.name}")
                    registry_path = json_registry
                except Exception as e:
                    print(f"[WARN] Headers registry compilation failed: {e}")
            elif json_registry.exists():
                registry_path = json_registry

    if registry_path and registry_path.exists():
        print(f"[OK] Headers SSOT enabled: {registry_path.name}")
    else:
        print(f"[WARN] Headers registry not found, continuing without SSOT")
        args.enable_headers_ssot = False

    # Head Guard registry support
    head_registry_path = None
    if args.auto_head_guard:
        if args.head_registry:
            head_registry_path = Path(args.head_registry)
            if not head_registry_path.is_absolute():
                head_registry_path = base_dir / head_registry_path
        else:
            default_registry = base_dir / "HEAD_REGISTRY_AGI_v3.0.yaml"
            if default_registry.exists():
                head_registry_path = default_registry

        if head_registry_path and head_registry_path.exists():
            print(f"[OK] Head Guard registry: {head_registry_path.name}")
        else:
            print("[WARN] Head registry not found, skipping auto validation")
            args.auto_head_guard = False

    if not args.base_dir and not args.dry_run:
        migrate_tide_outputs_to_parent(script_dir, base_dir)

    if not args.dry_run:
        ensure_bplus_inputs_data_dir(base_dir)

    # Check dependencies before proceeding
    if not check_dependencies(base_dir):
        print("[ERROR] Dependency check failed. Please install required packages.")
        return 1

    # Ensure sys is available (fix for UnboundLocalError)
    import sys

    argv = sys.argv[1:]

    # ---------------------------------------------------------------------
    # Site profile (AGI/DAS): parameter-set separation (measured values)
    # - Precedence: explicit CLI flags > profile JSON > argparse defaults
    # ---------------------------------------------------------------------
    resolved_profile_path: Optional[Path] = resolve_site_profile_path(
        args.profile_json, base_dir=base_dir, inputs_dir=inputs_dir, site=args.site
    )
    profile_obj: Optional[Dict[str, object]] = None
    if resolved_profile_path is not None:
        try:
            profile_obj = load_site_profile_json(resolved_profile_path)
            apply_site_profile_overrides(args, profile_obj, argv)
            apply_tide_ukc_profile_fallbacks(args, profile_obj, argv)
            print(f"[OK] Site profile loaded: {resolved_profile_path}")
        except Exception as e:
            print(
                f"[WARN] Site profile load failed: {resolved_profile_path} ({type(e).__name__}: {e})"
            )
            resolved_profile_path = None
            profile_obj = None

    # Gate-B critical-only selectors (profile-driven, CLI fwd_max overrides profile fwd max)
    cli_has_fwd_max = _argv_has_flag("--fwd_max", argv)
    gateb_fwd_max_val = _profile_last_value(profile_obj, PROFILE_GATEB_FWD_MAX_KEYS)
    gateb_fwd_max_cd = _coerce_float(gateb_fwd_max_val, None)
    if cli_has_fwd_max or gateb_fwd_max_cd is None:
        gateb_fwd_max_cd = float(args.fwd_max)

    gateb_critical_only = _profile_truthy_any(
        profile_obj, PROFILE_GATEB_CRITICAL_ONLY_KEYS
    )
    gateb_crit_list = _normalize_stage_patterns(
        _profile_last_value(profile_obj, PROFILE_CRITICAL_STAGE_KEYS)
    )
    gateb_crit_regex_val = _profile_last_value(profile_obj, PROFILE_CRITICAL_REGEX_KEYS)
    gateb_crit_regex = (
        gateb_crit_regex_val
        if isinstance(gateb_crit_regex_val, str) and gateb_crit_regex_val.strip()
        else DEFAULT_CRITICAL_STAGE_REGEX
    )

    out_dir = (
        Path(args.out_dir).resolve()
        if args.out_dir
        else (base_dir / f"pipeline_out_{now_tag()}")
    )
    ensure_dir(out_dir)

    # Resolve scripts
    tr_script = resolve_script_path(base_dir, args.tr_script, label="TR")
    tr_base_dir = tr_script.parent if tr_script.exists() else base_dir
    ops_script = resolve_script_path(base_dir, args.ops_script, label="OPS")
    solver_script = resolve_script_path(base_dir, args.solver_script, label="SOLVER")
    optimizer_script = resolve_script_path(
        base_dir, args.optimizer_script, label="OPTIMIZER"
    )
    spmt_script = resolve_script_path(base_dir, args.spmt_script, label="SPMT")
    bryan_template_script = resolve_script_path(
        base_dir, args.bryan_template_script, label="BRYAN_TPL"
    )
    spmt_config = resolve_script_path(base_dir, args.spmt_config, label="SPMT_CFG")

    # Resolve inputs
    tank_catalog = (inputs_dir / args.tank_catalog).resolve()

    hydro_default = inputs_dir / "bplus_inputs" / "Hydro_Table_Engineering.json"
    hydro_path = Path(args.hydro).resolve() if args.hydro else hydro_default.resolve()

    # Stage results: may be generated by TR script (csv mode)
    stage_results_path = (
        Path(args.stage_results).resolve()
        if args.stage_results
        else (tr_base_dir / "stage_results.csv").resolve()
    )

    # Derived SSOT CSVs for solver/optimizer
    ssot_dir = ensure_dir(out_dir / "ssot")
    tank_ssot_csv = ssot_dir / "tank_ssot_for_solver.csv"
    hydro_ssot_csv = ssot_dir / "hydro_table_for_solver.csv"
    stage_table_csv = ssot_dir / "stage_table_unified.csv"

    if args.dry_run:
        print("=" * 80)
        print("DRY RUN - PATH RESOLUTION")
        print("=" * 80)
        print(f"Site:        {args.site}")
        print(f"Base dir:    {base_dir}")
        print(f"Inputs dir:  {inputs_dir}")
        print(f"Out dir:     {out_dir}")
        print("\n[SCRIPTS]")
        print(
            f"TR:          {tr_script}  ({'OK' if tr_script.exists() else 'MISSING'})"
        )
        print(
            f"OPS:         {ops_script}  ({'OK' if ops_script.exists() else 'MISSING'})"
        )
        print(
            f"SOLVER:      {solver_script}  ({'OK' if solver_script.exists() else 'MISSING'})"
        )
        print(
            f"OPTIMIZER:   {optimizer_script}  ({'OK' if optimizer_script.exists() else 'MISSING'})"
        )
        print(
            f"SPMT:        {spmt_script}  ({'OK' if spmt_script.exists() else 'MISSING'})"
        )
        print(
            f"BRYAN_TPL:   {bryan_template_script}  ({'OK' if bryan_template_script.exists() else 'MISSING'})"
        )
        print("\n[INPUTS]")
        print(
            f"Tank catalog:{tank_catalog}  ({'OK' if tank_catalog.exists() else 'MISSING'})"
        )
        print(
            f"Hydro:       {hydro_path}  ({'OK' if hydro_path.exists() else 'MISSING'})"
        )
        print(
            f"StageRes:    {stage_results_path}  ({'OK' if stage_results_path.exists() else 'MISSING'})"
        )
        print(
            f"SPMT config:{spmt_config}  ({'OK' if spmt_config.exists() else 'MISSING'})"
        )
        print("\n[OUTPUT SSOT]")
        print(f"Tank SSOT:   {tank_ssot_csv}")
        print(f"Hydro SSOT:  {hydro_ssot_csv}")
        print(f"Stage table: {stage_table_csv}")
        return 0

    # -------------------------------------------------------------------------
    # Step 0: AGI SPMT Unified (optional)
    # -------------------------------------------------------------------------
    spmt_out_dir = out_dir / "spmt_output"
    spmt_out_xlsx = out_dir / "AGI_SPMT_Shuttle_Output.xlsx"
    if args.from_step <= 0 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 0] AGI SPMT Unified (optional)")
        print("=" * 80)
        if not spmt_script.exists():
            print(f"[WARN] SPMT script not found: {spmt_script}")
        elif not spmt_config.exists():
            print(f"[WARN] SPMT config not found: {spmt_config}")
        else:
            ensure_dir(spmt_out_dir)
            spmt_args = [
                "--config",
                str(spmt_config),
                "--out_xlsx",
                str(spmt_out_xlsx),
                "--out_dir",
                str(spmt_out_dir),
            ]
            r0 = step_run_script(
                0,
                "SPMT_UNIFIED",
                spmt_script,
                args=spmt_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r0.ok:
                print(f"[WARN] Step-0 failed (rc={r0.returncode}). Log: {r0.log}")
            else:
                print(f"[OK] Step-0 completed. Log: {r0.log}")
                print(f"     SPMT:   {spmt_out_xlsx}")

    # -------------------------------------------------------------------------
    # Step 1: TR Excel generation (optional but kept to maintain original flow)
    # -------------------------------------------------------------------------
    if args.from_step <= 1 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 1] TR Excel Generation")
        print("=" * 80)
        r1 = step_run_script(
            1,
            "TR_EXCEL",
            tr_script,
            args=[],
            cwd=tr_base_dir,
            out_dir=out_dir,
        )
        if not r1.ok:
            print(f"[WARN] Step-1 failed (rc={r1.returncode}). Log: {r1.log}")
            print(
                "       You can still proceed if you already have stage_results.csv or do not need the Excel."
            )
        else:
            print(f"[OK] Step-1 completed. Log: {r1.log}")

    # -------------------------------------------------------------------------
    # Step 1b: Generate stage_results.csv from TR script (csv mode)
    # -------------------------------------------------------------------------
    if args.from_step <= 1 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 1b] Generate stage_results.csv (TR script csv mode)")
        print("=" * 80)

        if not stage_results_path.exists():
            r1b = step_run_script(
                2,
                "TR_STAGE_CSV",
                tr_script,
                args=["csv"],
                cwd=tr_base_dir,
                out_dir=out_dir,
            )
            if not r1b.ok:
                print(
                    f"[ERROR] Step-1b failed and stage_results.csv not found. rc={r1b.returncode}"
                )
                print(f"        Log: {r1b.log}")
                return 1
            else:
                print(f"[OK] stage_results.csv generated. Log: {r1b.log}")

        if not stage_results_path.exists():
            print(f"[ERROR] stage_results.csv still missing at: {stage_results_path}")
            return 1

    # -------------------------------------------------------------------------
    # Build SSOT CSVs needed for solver/optimizer
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("[PREP] Build SSOT CSVs (Tank/Hydro/Stage)")
    print("=" * 80)

    # Tank SSOT
    if not tank_catalog.exists():
        print(f"[ERROR] Missing tank catalog: {tank_catalog}")
        return 1
    tank_keywords = [k.strip() for k in str(args.tank_keywords).split(",") if k.strip()]
    try:
        convert_tank_catalog_json_to_solver_csv(
            tank_catalog_json=tank_catalog,
            out_csv=tank_ssot_csv,
            pump_rate_tph=float(args.pump_rate),
            include_keywords=tank_keywords,
        )
        print(f"[OK] Tank SSOT: {tank_ssot_csv}")
    except Exception as e:
        print(f"[ERROR] Failed to convert tank catalog: {type(e).__name__}: {e}")
        import traceback

        traceback.print_exc()
        return 1

    # Current_t sensor sync (PLC/IoT) -> inject into Tank SSOT
    sensor_stats: Optional[Dict[str, object]] = None
    resolved_sensor_csv = resolve_current_t_sensor_csv(
        args.current_t_csv, base_dir=base_dir, inputs_dir=inputs_dir
    )
    if resolved_sensor_csv is not None:
        try:
            sensor_stats = inject_current_t_from_sensor_csv(
                tank_ssot_csv=tank_ssot_csv,
                sensor_csv=resolved_sensor_csv,
                strategy=str(args.current_t_strategy),
                out_csv=tank_ssot_csv,
            )
            print(
                f"[OK] Current_t injected: {resolved_sensor_csv} "
                f"(exact={sensor_stats.get('updated_exact')}, group={sensor_stats.get('updated_group')})"
            )
        except Exception as e:
            print(
                f"[WARN] Current_t injection failed: {resolved_sensor_csv} ({type(e).__name__}: {e})"
            )
            sensor_stats = None
    else:
        print(
            "[WARN] No Current_t sensor CSV found. Tank SSOT Current_t remains default (0.0)."
        )

    # Tank overrides from SITE profile (use_flag/mode/pump_rate/etc.)
    tank_override_stats: Optional[Dict[str, object]] = None
    try:
        tank_override_stats = apply_tank_overrides_from_profile(
            tank_ssot_csv=tank_ssot_csv,
            profile=profile_obj,
            out_csv=tank_ssot_csv,
        )
        if int(tank_override_stats.get("overrides_applied", 0) or 0) > 0:
            print(
                f"[OK] Tank overrides applied: rules={tank_override_stats.get('overrides_applied')}, "
                f"touched={tank_override_stats.get('tanks_touched')}, "
                f"missing={len(tank_override_stats.get('missing_keys') or [])}"
            )
            if int(tank_override_stats.get("base_skipped", 0) or 0) > 0:
                print(
                    f"[WARN] Base-match overrides skipped (explicit allow required): "
                    f"{tank_override_stats.get('base_skipped_keys')}"
                )
            if tank_override_stats.get("missing_keys"):
                print(
                    f"[WARN] Tank override keys not found in SSOT: {tank_override_stats.get('missing_keys')}"
                )
    except Exception as e:
        print(f"[WARN] Tank override application failed ({type(e).__name__}: {e})")
        tank_override_stats = None

    # Exclude FWD tanks globally (after profile overrides)
    if args.exclude_fwd_tanks:
        try:
            df_tank = pd.read_csv(tank_ssot_csv, encoding="utf-8-sig")
            if "x_from_mid_m" in df_tank.columns and "use_flag" in df_tank.columns:
                x_col = pd.to_numeric(df_tank["x_from_mid_m"], errors="coerce")
                fwd_mask = x_col < 0
                fwd_count = int(fwd_mask.sum())
                if fwd_count > 0:
                    before_use = (
                        df_tank.loc[fwd_mask, "use_flag"].value_counts().to_dict()
                    )
                    df_tank.loc[fwd_mask, "use_flag"] = "N"
                    df_tank.to_csv(tank_ssot_csv, index=False, encoding="utf-8-sig")
                    print(
                        f"[OK] Excluded {fwd_count} FWD tank(s) globally (x_from_mid_m < 0)"
                    )
                    print(f"     Before: {before_use}, After: all use_flag=N")
                else:
                    print("[INFO] No FWD tanks found (x_from_mid_m < 0).")
            else:
                print(
                    "[WARN] --exclude_fwd_tanks: required columns missing in Tank SSOT"
                )
        except Exception as e:
            print(f"[WARN] FWD tank exclusion failed ({type(e).__name__}: {e})")

    # Hydro SSOT
    if not hydro_path.exists():
        print(f"[ERROR] Missing hydro input: {hydro_path}")
        print(
            "       Provide --hydro or place it at inputs_dir/bplus_inputs/Hydro_Table_Engineering.json"
        )
        return 1
    try:
        convert_hydro_engineering_json_to_solver_csv(
            hydro_json=hydro_path,
            out_csv=hydro_ssot_csv,
            lbp_m=LPP_M,
        )
        print(f"[OK] Hydro SSOT: {hydro_ssot_csv}")
    except Exception as e:
        print(f"[ERROR] Failed to convert hydro table: {type(e).__name__}: {e}")
        import traceback

        traceback.print_exc()
        return 1

    # Stage table
    if not stage_results_path.exists():
        print(f"[ERROR] Missing stage_results.csv: {stage_results_path}")
        return 1

    try:
        build_stage_table_from_stage_results(
            stage_results_csv=stage_results_path,
            out_csv=stage_table_csv,
            fwd_max_m=float(gateb_fwd_max_cd),
            aft_min_m=float(args.aft_min),
            aft_max_m_for_optimizer=float(args.aft_max),
            trim_abs_limit_m=float(args.trim_abs_limit),
            critical_only=gateb_critical_only,
            critical_stage_list=gateb_crit_list,
            critical_regex=gateb_crit_regex,
            forecast_tide_m=args.forecast_tide,
            depth_ref_m=args.depth_ref,
            ukc_min_m=args.ukc_min,
            hydro_json_path=hydro_path,
        )
    except Exception as e:
        print(f"[ERROR] Failed to build stage table: {type(e).__name__}: {e}")
        import traceback

        traceback.print_exc()
        return 1
    # Tide/UKC enrichment (optional, AGI)
    try:
        tide_table_path = (
            Path(args.tide_table).resolve() if str(args.tide_table).strip() else None
        )
        stage_schedule_path = (
            Path(args.stage_schedule).resolve()
            if str(args.stage_schedule).strip()
            else None
        )
        stage_tide_csv_path = resolve_stage_tide_csv(
            str(args.stage_tide_csv),
            base_dir=base_dir,
            inputs_dir=inputs_dir,
            out_dir=out_dir,
            site=args.site,
        )
        if stage_tide_csv_path:
            print(f"[OK] stage_tide_csv resolved: {stage_tide_csv_path}")
        # Apply profile fallbacks if not set via CLI
        depth_ref_for_enrich = args.depth_ref
        ukc_min_for_enrich = args.ukc_min
        # Debug: Check if profile_obj is available
        if profile_obj is None:
            print(f"[WARN] profile_obj is None at enrich_stage_table call site")
        else:
            print(
                f"[DEBUG] profile_obj available, keys: {list(profile_obj.keys()) if isinstance(profile_obj, dict) else 'not a dict'}"
            )
        if depth_ref_for_enrich is None and profile_obj:
            tide_ukc = profile_obj.get("tide_ukc", {})
            if isinstance(tide_ukc, dict):
                depth_ref_for_enrich = tide_ukc.get("depth_ref_m")
                if depth_ref_for_enrich is not None:
                    print(
                        f"[OK] Using depth_ref_m from profile: {depth_ref_for_enrich}"
                    )
                else:
                    print(f"[WARN] tide_ukc.depth_ref_m is None in profile")
            else:
                print(f"[WARN] profile_obj.tide_ukc is not a dict: {type(tide_ukc)}")
        elif depth_ref_for_enrich is None:
            print(
                f"[WARN] depth_ref_for_enrich is None (profile_obj={profile_obj is not None})"
            )
        if ukc_min_for_enrich is None and profile_obj:
            tide_ukc = profile_obj.get("tide_ukc", {})
            if isinstance(tide_ukc, dict):
                ukc_min_for_enrich = tide_ukc.get("ukc_min_m")
                if ukc_min_for_enrich is not None:
                    print(f"[OK] Using ukc_min_m from profile: {ukc_min_for_enrich}")
                else:
                    print(f"[WARN] tide_ukc.ukc_min_m is None in profile")
            else:
                print(f"[WARN] profile_obj.tide_ukc is not a dict: {type(tide_ukc)}")
        elif ukc_min_for_enrich is None:
            print(
                f"[WARN] ukc_min_for_enrich is None (profile_obj={profile_obj is not None})"
            )

        enrich_stage_table_with_tide_ukc(
            stage_table_csv=stage_table_csv,
            forecast_tide_m=args.forecast_tide,
            depth_ref_m=depth_ref_for_enrich,
            ukc_min_m=ukc_min_for_enrich,
            squat_m=float(args.squat),
            safety_allow_m=float(args.safety_allow),
            tide_tol_m=(
                float(args.tide_tol)
                if args.tide_tol is not None
                else DEFAULT_TIDE_TOL_M
            ),
            stage_tide_csv_path=stage_tide_csv_path,
            tide_table_path=tide_table_path,
            stage_schedule_path=stage_schedule_path,
            tide_strategy=str(args.tide_strategy),
        )
        if tide_table_path and stage_schedule_path:
            print(
                f"[OK] Tide table applied to stages: {tide_table_path.name} (schedule={stage_schedule_path.name})"
            )
    except Exception as e:
        print(
            f"[WARN] Stage table Tide/UKC enrichment skipped: {type(e).__name__}: {e}"
        )

    print(f"[OK] Stage table: {stage_table_csv}")

    # AFT-min-only FWD tank handling (stage-level SSOT selection)
    if args.exclude_fwd_tanks_aftmin_only:
        try:
            df_stage = pd.read_csv(stage_table_csv, encoding="utf-8-sig")
            if "Current_AFT_m" in df_stage.columns and "AFT_MIN_m" in df_stage.columns:
                tol = 1e-6
                df_stage["Ban_FWD_Tanks"] = (
                    pd.to_numeric(df_stage["Current_AFT_m"], errors="coerce")
                    < pd.to_numeric(df_stage["AFT_MIN_m"], errors="coerce") - tol
                )
                ban_count = int(df_stage["Ban_FWD_Tanks"].fillna(False).sum())

                # Default: all stages use global SSOT
                df_stage["Tank_SSOT_CSV"] = str(tank_ssot_csv.name)

                if ban_count > 0:
                    try:
                        df_tank = pd.read_csv(tank_ssot_csv, encoding="utf-8-sig")
                        if (
                            "x_from_mid_m" in df_tank.columns
                            and "mode" in df_tank.columns
                        ):
                            x_col = pd.to_numeric(
                                df_tank["x_from_mid_m"], errors="coerce"
                            )
                            fwd_mask = x_col < 0
                            fwd_count = int(fwd_mask.sum())
                            if fwd_count > 0:
                                df_tank_aftmin = df_tank.copy()
                                df_tank_aftmin.loc[fwd_mask, "use_flag"] = "Y"
                                df_tank_aftmin.loc[fwd_mask, "mode"] = "DISCHARGE_ONLY"
                                if (
                                    "Current_t" in df_tank_aftmin.columns
                                    and "Max_t" in df_tank_aftmin.columns
                                ):
                                    current_t_col = pd.to_numeric(
                                        df_tank_aftmin.loc[fwd_mask, "Current_t"],
                                        errors="coerce",
                                    )
                                    df_tank_aftmin.loc[fwd_mask, "Max_t"] = (
                                        current_t_col
                                    )
                                if "Min_t" in df_tank_aftmin.columns:
                                    df_tank_aftmin.loc[fwd_mask, "Min_t"] = 0.0

                                tank_ssot_csv_aftmin = (
                                    ssot_dir / "tank_ssot_for_solver__aftmin.csv"
                                )
                                df_tank_aftmin.to_csv(
                                    tank_ssot_csv_aftmin,
                                    index=False,
                                    encoding="utf-8-sig",
                                )
                                print(
                                    f"[OK] AFT-min stage SSOT created: {tank_ssot_csv_aftmin.name}"
                                )
                                print(
                                    f"     {fwd_count} FWD tank(s) set to DISCHARGE_ONLY (use_flag=Y, Min_t=0.0, Max_t=Current_t)"
                                )
                                print(
                                    f"     Global SSOT unchanged: {tank_ssot_csv.name}"
                                )

                                df_stage.loc[
                                    df_stage["Ban_FWD_Tanks"] == True,
                                    "Tank_SSOT_CSV",
                                ] = str(tank_ssot_csv_aftmin.name)
                                print(
                                    f"[OK] Stage-level Tank SSOT selector: {ban_count} AFT-min stage(s) -> {tank_ssot_csv_aftmin.name}"
                                )
                            else:
                                print("[INFO] No FWD tanks found (x_from_mid_m < 0).")
                        else:
                            print(
                                "[WARN] AFT-min handling skipped (x_from_mid_m/mode missing in Tank SSOT)"
                            )
                    except Exception as e:
                        print(
                            f"[WARN] AFT-min FWD handling failed: {type(e).__name__}: {e}"
                        )
                df_stage.to_csv(stage_table_csv, index=False, encoding="utf-8-sig")
                print(
                    f"[OK] AFT-min stage filter enabled: Ban_FWD_Tanks={ban_count} stage(s)"
                )
            else:
                print("[WARN] AFT-min filter skipped (Current_AFT_m/AFT_MIN_m missing)")
        except Exception as e:
            print(f"[WARN] AFT-min filter update failed: {type(e).__name__}: {e}")

    # Generate QA CSV (definition-split enforcement)
    stage_qa_csv = ssot_dir / "pipeline_stage_QA.csv"
    solver_summary_for_qa = None
    if args.from_step <= 3 <= args.to_step:
        candidate = out_dir / "solver_ballast_summary.csv"
        if candidate.exists():
            solver_summary_for_qa = candidate
    # Gate guard-band (P0-3): convert cm to m
    gate_guard_band_m = float(args.gate_guard_band_cm) / 100.0
    generate_stage_QA_csv(
        stage_table_csv=stage_table_csv,
        out_qa_csv=stage_qa_csv,
        fwd_max_m=float(gateb_fwd_max_cd),
        aft_min_m=float(args.aft_min),
        d_vessel_m=float(D_VESSEL_M),
        forecast_tide_m=args.forecast_tide,
        depth_ref_m=args.depth_ref,
        ukc_min_m=args.ukc_min,
        critical_only=gateb_critical_only,
        critical_stage_list=gateb_crit_list,
        critical_regex=gateb_crit_regex,
        gateb_fwd_max_m_cd=float(gateb_fwd_max_cd),
        squat_m=float(args.squat),
        safety_allow_m=float(args.safety_allow),
        hydro_json_path=hydro_path,
        strict_hardstop=False,  # Generate QA CSV but don't stop on HardStop violations
        solver_summary_csv=solver_summary_for_qa,
        hmax_wave_m=args.hmax_wave_m,
        four_corner_monitoring=args.four_corner_monitoring,
        tol_m=gate_guard_band_m,
        tide_tol_m=(
            float(args.tide_tol) if args.tide_tol is not None else DEFAULT_TIDE_TOL_M
        ),
        trim_abs_limit_m=float(args.trim_abs_limit),
        trim_limit_enforced=bool(args.trim_limit_enforced),
        freeboard_min_m=float(args.freeboard_min_m),
        freeboard_min_enforced=bool(args.freeboard_min_enforced),
    )
    print(f"[OK] Stage QA CSV (definition-split): {stage_qa_csv}")

    # -------------------------------------------------------------------------
    # Tidying: pipeline_stage_QA.csv
    # -------------------------------------------------------------------------
    if stage_qa_csv.exists():
        registry_path_tidying = (
            registry_path if args.enable_headers_ssot and registry_path else None
        )
        run_tidying_for_csv(
            file_path=stage_qa_csv,
            base_dir=base_dir,
            registry_path=registry_path_tidying,
            deliverable_id="PIPELINE_STAGE_QA_CSV",
            verbose=True,
        )

    # Gate FAIL root-cause report (auto)
    gate_report_md = out_dir / "gate_fail_report.md"
    if not args.no_gate_report:
        try:
            ukc_inputs = {
                "forecast_tide_m": args.forecast_tide,
                "depth_ref_m": args.depth_ref,
                "ukc_min_m": args.ukc_min,
                "squat_m": float(args.squat),
                "safety_allow_m": float(args.safety_allow),
            }
            generate_gate_fail_report_md(
                out_md=gate_report_md,
                site=str(args.site),
                profile_path=resolved_profile_path,
                stage_qa_csv=stage_qa_csv,
                tank_ssot_csv=tank_ssot_csv,
                sensor_stats=sensor_stats,
                ukc_inputs=ukc_inputs,
            )
            print(f"[OK] Gate FAIL report: {gate_report_md}")
            try:
                append_dnv_mitigation_section_to_gate_report(
                    report_md=gate_report_md,
                    stage_qa_csv=stage_qa_csv,
                    propeller_diameter_m=1.38,
                )
                print("[OK] DNV mitigation section appended to gate report")
            except Exception as e:
                print(
                    f"[WARN] Failed to append DNV mitigation: {type(e).__name__}: {e}"
                )
            try:
                tug_sop_md = out_dir / "TUG_Operational_SOP_DNV_ST_N001.md"
                generate_tug_operational_sop_md(
                    out_md=tug_sop_md,
                    stage_qa_csv=stage_qa_csv,
                    site=str(args.site),
                )
                print(f"[OK] TUG Operational SOP: {tug_sop_md}")
            except Exception as e:
                print(f"[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}")
        except Exception as e:
            print(f"[WARN] Gate FAIL report generation failed: {type(e).__name__}: {e}")

    if args.debug_report or args.auto_debug_report:
        debug_report_step(
            out_dir=out_dir,
            stage_qa_csv=stage_qa_csv,
            hydro_json_path=hydro_path,
        )

    # -------------------------------------------------------------------------
    # Step 2: OPS integrated report (Excel + MD)
    # -------------------------------------------------------------------------
    if args.from_step <= 2 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 2] OPS-FINAL-R3 Integrated (Excel + Markdown)")
        print("=" * 80)
        r2 = step_run_script(
            3,
            "OPS_INTEGRATED",
            ops_script,
            args=[
                "--fwd_max",
                str(gateb_fwd_max_cd),
                "--aft_min",
                str(args.aft_min),
            ],
            cwd=base_dir,
            out_dir=out_dir,
        )
        if not r2.ok:
            print(f"[ERROR] Step-2 failed (rc={r2.returncode}). Log: {r2.log}")
            return 1
        else:
            print(f"[OK] Step-2 completed. Log: {r2.log}")

        # Collect outputs if present (script writes in CWD)
        expected_ops = [
            base_dir / "OPS_FINAL_R3_AGI_Ballast_Integrated.xlsx",
            base_dir / "OPS_FINAL_R3_Report_Integrated.md",
        ]
        for p in expected_ops:
            if p.exists():
                shutil.copy2(p, out_dir / p.name)
                print(f"[OK] Collected output -> {out_dir / p.name}")

    # -------------------------------------------------------------------------
    # Step 3: Gate solver (LP) - unified gates (FWD max + AFT min + FB + UKC)
    # -------------------------------------------------------------------------
    solver_out_plan = out_dir / "solver_ballast_plan.csv"
    solver_out_sum = out_dir / "solver_ballast_summary.csv"
    solver_out_stage_plan = out_dir / "solver_ballast_stage_plan.csv"

    opt_out_plan = out_dir / "optimizer_plan.csv"
    opt_out_sum = out_dir / "optimizer_summary.csv"
    opt_out_bwrb = out_dir / "optimizer_bwrb_log.csv"
    opt_out_tanklog = out_dir / "optimizer_tank_log.csv"
    opt_excel = out_dir / "optimizer_ballast_plan.xlsx"
    opt_out_stage_plan = out_dir / "ballast_stage_plan.csv"

    if args.from_step <= 3 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 3] Ballast Gate Solver (LP, definition-split + unified gates)")
        print("=" * 80)

        # íŒŒì¼ ì¡´ìž¬ í™•ì¸ (í•„ìˆ˜ ìž…ë ¥ íŒŒì¼)
        missing_files = []
        if not stage_table_csv.exists():
            missing_files.append(f"stage_table_csv: {stage_table_csv}")
        if not tank_ssot_csv.exists():
            missing_files.append(f"tank_ssot_csv: {tank_ssot_csv}")
        if not hydro_ssot_csv.exists():
            missing_files.append(f"hydro_ssot_csv: {hydro_ssot_csv}")

        if missing_files:
            print(f"[ERROR] Step-3 cannot proceed: Missing required files:")
            for f in missing_files:
                print(f"  - {f}")
            return 1

        try:
            fwd_max_arg = "nan" if gateb_critical_only else str(gateb_fwd_max_cd)
            solver_args = [
                "--tank",
                str(tank_ssot_csv),
                "--hydro",
                str(hydro_ssot_csv),
                "--mode",
                "limit",
                "--stage",
                str(stage_table_csv),
                "--iterate_hydro",
                "2",
                "--out_plan",
                str(solver_out_plan),
                "--out_summary",
                str(solver_out_sum),
                "--out_stage_plan",
                str(solver_out_stage_plan),
                "--fwd_max",
                fwd_max_arg,
                "--aft_min",
                str(args.aft_min),
                "--d_vessel",
                str(D_VESSEL_M),
                "--fb_min",
                str(args.freeboard_min_m),
                "--squat",
                str(args.squat),
                "--safety_allow",
                str(args.safety_allow),
                "--trim_abs_limit",
                str(args.trim_abs_limit),
                "--freeboard_min_m",
                str(args.freeboard_min_m),
            ]
            if args.trim_limit_enforced:
                solver_args.append("--trim_limit_enforced")
            else:
                solver_args.append("--no-trim-limit-enforced")
            if args.freeboard_min_enforced:
                solver_args.append("--freeboard_min_enforced")
            else:
                solver_args.append("--no-freeboard-min-enforced")
            if args.forecast_tide is not None:
                solver_args += ["--forecast_tide", str(args.forecast_tide)]
            if args.depth_ref is not None:
                solver_args += ["--depth_ref", str(args.depth_ref)]
            if args.ukc_min is not None:
                solver_args += ["--ukc_min", str(args.ukc_min)]
            if args.stateful_solver:
                solver_args.append("--stateful")
            if str(args.reset_tank_state).strip():
                solver_args += [
                    "--reset_tank_state",
                    str(args.reset_tank_state).strip(),
                ]
            oper_json = str(args.tank_operability_json).strip()
            if (
                not oper_json
                and resolved_profile_path
                and Path(resolved_profile_path).exists()
            ):
                oper_json = str(resolved_profile_path)
            if oper_json:
                solver_args += ["--tank_operability_json", oper_json]
            if str(args.operational_stage_regex).strip():
                solver_args += [
                    "--operational_stage_regex",
                    str(args.operational_stage_regex).strip(),
                ]
            if args.disable_preballast_only_on_operational_stages:
                solver_args.append("--disable_preballast_only_on_operational_stages")
            if args.stateful_solver:
                trace_path = str(args.state_trace_csv).strip()
                if not trace_path:
                    trace_path = str(out_dir / "solver_state_trace.csv")
                solver_args += ["--state_trace_csv", trace_path]

            r3 = step_run_script(
                4,
                "SOLVER_LP",
                solver_script,
                args=solver_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r3.ok:
                print(f"[ERROR] Step-3 failed (rc={r3.returncode}). Log: {r3.log}")
                return 1
            else:
                print(f"[OK] Step-3 completed. Log: {r3.log}")
                print(f"     Plan:    {solver_out_plan}")
                print(f"     Summary: {solver_out_sum}")

                # -------------------------------------------------------------------------
                # Tidying: Step 3 Solver Outputs
                # -------------------------------------------------------------------------
                registry_path_tidying = (
                    registry_path
                    if args.enable_headers_ssot and registry_path
                    else None
                )
                if solver_out_plan.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_plan,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id="SOLVER_BALLAST_PLAN_CSV",
                        verbose=True,
                    )
                if solver_out_sum.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_sum,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id="SOLVER_BALLAST_SUMMARY_CSV",
                        verbose=True,
                    )
                if solver_out_stage_plan.exists():
                    run_tidying_for_csv(
                        file_path=solver_out_stage_plan,
                        base_dir=base_dir,
                        registry_path=registry_path_tidying,
                        deliverable_id="SOLVER_BALLAST_STAGE_PLAN_CSV",
                        verbose=True,
                    )

                # Append solver outputs to Gate FAIL report (if enabled)
                if (not args.no_gate_report) and "gate_report_md" in locals():
                    try:
                        append_solver_section_to_gate_report(
                            report_md=gate_report_md,
                            solver_out_summary=solver_out_sum,
                            solver_out_plan=solver_out_plan,
                            solver_out_stage_plan=solver_out_stage_plan,
                        )
                    except Exception as e:
                        print(
                            f"[WARN] Failed to append solver section to gate report: {type(e).__name__}: {e}"
                        )
                # Regenerate QA CSV with solver post-solve drafts
                try:
                    # Gate guard-band (P0-3): convert cm to m
                    gate_guard_band_m = float(args.gate_guard_band_cm) / 100.0
                    generate_stage_QA_csv(
                        stage_table_csv=stage_table_csv,
                        out_qa_csv=stage_qa_csv,
                        fwd_max_m=float(gateb_fwd_max_cd),
                        aft_min_m=float(args.aft_min),
                        d_vessel_m=float(D_VESSEL_M),
                        forecast_tide_m=args.forecast_tide,
                        depth_ref_m=args.depth_ref,
                        ukc_min_m=args.ukc_min,
                        critical_only=gateb_critical_only,
                        critical_stage_list=gateb_crit_list,
                        critical_regex=gateb_crit_regex,
                        gateb_fwd_max_m_cd=float(gateb_fwd_max_cd),
                        squat_m=float(args.squat),
                        safety_allow_m=float(args.safety_allow),
                        hydro_json_path=hydro_path,
                        strict_hardstop=False,
                        solver_summary_csv=solver_out_sum,
                        hmax_wave_m=args.hmax_wave_m,
                        four_corner_monitoring=args.four_corner_monitoring,
                        tol_m=gate_guard_band_m,
                        trim_abs_limit_m=float(args.trim_abs_limit),
                        trim_limit_enforced=bool(args.trim_limit_enforced),
                        freeboard_min_m=float(args.freeboard_min_m),
                        freeboard_min_enforced=bool(args.freeboard_min_enforced),
                    )
                    print(
                        f"[OK] Stage QA CSV updated with solver results: {stage_qa_csv}"
                    )
                except Exception as e:
                    print(
                        f"[WARN] Failed to update QA with solver results: {type(e).__name__}: {e}"
                    )
        except Exception as e:
            print(f"[ERROR] Step-3 exception: {type(e).__name__}: {e}")
            import traceback

            traceback.print_exc()
            return 1

    # -------------------------------------------------------------------------
    # Step 4: Optimizer (optional) - note: AFT_MIN is NOT a native constraint here
    # -------------------------------------------------------------------------
    if args.from_step <= 4 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 4] Ballast Optimizer Integrated (optional)")
        print("=" * 80)
        print("[NOTE] This optimizer uses AFT_Limit_m as an upper limit (MAX).")
        print(
            "       If you need Captain gate AFT >= 2.70m, enforce it via Step-3 solver (AFT_MIN)."
        )

        opt_args = [
            "--batch",
            "--tank",
            str(tank_ssot_csv),
            "--hydro",
            str(hydro_ssot_csv),
            "--stage",
            str(stage_table_csv),
            "--prefer_time",
            "--iterate_hydro",
            "2",
            "--out_plan",
            str(opt_out_plan),
            "--out_summary",
            str(opt_out_sum),
            "--out_stage_plan",
            str(opt_out_stage_plan),
            "--bwrb_out",
            str(opt_out_bwrb),
            "--tanklog_out",
            str(opt_out_tanklog),
            "--excel_out",
            str(opt_excel),
        ]

        r4 = step_run_script(
            5,
            "OPTIMIZER",
            optimizer_script,
            args=opt_args,
            cwd=base_dir,
            out_dir=out_dir,
        )
        if not r4.ok:
            print(f"[ERROR] Step-4 failed (rc={r4.returncode}). Log: {r4.log}")
            return 1
        else:
            print(f"[OK] Step-4 completed. Log: {r4.log}")
            print(f"     Excel: {opt_excel}")

    # -------------------------------------------------------------------------
    # Step 4b: Ballast Sequence & Checklist Generation (optional)
    # -------------------------------------------------------------------------
    if args.enable_sequence and args.from_step <= 4 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 4b] Ballast Sequence & Checklist Generation")
        print("=" * 80)

        try:
            seq_gen_script = resolve_script_path(
                base_dir, "ballast_sequence_generator.py", label="SEQUENCE_GEN"
            )
            checklist_gen_script = resolve_script_path(
                base_dir, "checklist_generator.py", label="CHECKLIST_GEN"
            )

            if not seq_gen_script.exists():
                print(f"[WARN] Sequence generator not found: {seq_gen_script}")
                print("[INFO] Step 4b skipped. Run standalone scripts if needed.")
            else:
                import pandas as pd

                ballast_plan_path = None
                if opt_out_plan.exists():
                    opt_df = pd.read_csv(opt_out_plan, encoding="utf-8-sig")
                    if "Stage" in opt_df.columns:
                        stages_in_opt = set(opt_df["Stage"].astype(str).unique())
                        required_stages = {
                            "Stage 5_PreBallast",
                            "Stage 6A_Critical (Opt C)",
                            "Stage 6B Tide Window",
                        }
                        if (
                            not required_stages.issubset(stages_in_opt)
                            and solver_out_plan.exists()
                        ):
                            solver_df = pd.read_csv(
                                solver_out_plan, encoding="utf-8-sig"
                            )
                            missing_stages = required_stages - stages_in_opt
                            for stage in missing_stages:
                                stage_rows = solver_df[
                                    solver_df["Stage"].astype(str) == stage
                                ]
                                if not stage_rows.empty:
                                    opt_df = pd.concat(
                                        [opt_df, stage_rows], ignore_index=True
                                    )
                            temp_plan = out_dir / "ballast_plan_merged.csv"
                            opt_df.to_csv(temp_plan, index=False, encoding="utf-8-sig")
                            ballast_plan_path = temp_plan
                            print(
                                f"[INFO] Using optimizer plan (merged with solver): {temp_plan.name}"
                            )
                        else:
                            ballast_plan_path = opt_out_plan
                            print(f"[INFO] Using optimizer plan: {opt_out_plan.name}")
                    else:
                        ballast_plan_path = opt_out_plan
                        print(f"[INFO] Using optimizer plan: {opt_out_plan.name}")
                elif solver_out_plan.exists():
                    ballast_plan_path = solver_out_plan
                    print(f"[INFO] Using solver plan: {solver_out_plan.name}")
                else:
                    print(
                        "[WARN] No ballast plan found (optimizer_plan.csv or solver_ballast_plan.csv)."
                    )

                if ballast_plan_path is not None:
                    ballast_plan_df = pd.read_csv(
                        ballast_plan_path, encoding="utf-8-sig"
                    )
                    # Ensure Stage column exists and fill empty values
                    if "Stage" not in ballast_plan_df.columns:
                        print("[WARN] ballast_plan_df missing Stage column")
                        if "Stage_ID" in ballast_plan_df.columns:
                            ballast_plan_df["Stage"] = ballast_plan_df["Stage_ID"]
                        else:
                            ballast_plan_df["Stage"] = ""
                    # Fill empty Stage values
                    empty_stage_mask = ballast_plan_df["Stage"].isna() | (
                        ballast_plan_df["Stage"].astype(str).str.strip() == ""
                    )
                    if empty_stage_mask.any():
                        # Forward fill from previous non-empty Stage
                        ballast_plan_df["Stage"] = (
                            ballast_plan_df["Stage"].fillna(method="ffill").fillna("")
                        )
                        # Replace remaining empty strings
                        ballast_plan_df.loc[
                            ballast_plan_df["Stage"].astype(str).str.strip() == "",
                            "Stage",
                        ] = ""
                        if empty_stage_mask.sum() > 0:
                            print(
                                f"[WARN] Found {empty_stage_mask.sum()} rows with empty Stage, attempted to fill"
                            )

                    fallback_plan_df = None
                    if solver_out_plan.exists():
                        try:
                            if (
                                Path(ballast_plan_path).resolve()
                                != solver_out_plan.resolve()
                            ):
                                fallback_plan_df = pd.read_csv(
                                    solver_out_plan, encoding="utf-8-sig"
                                )
                        except Exception as e:
                            print(
                                "[WARN] Failed to load solver fallback plan: "
                                f"{type(e).__name__}: {e}"
                            )

                    stage_drafts = {}
                    if stage_qa_csv.exists():
                        qa_df = pd.read_csv(stage_qa_csv, encoding="utf-8-sig")
                        for _, row in qa_df.iterrows():
                            ukc_val = row.get("UKC_min_actual_m")
                            if ukc_val is None:
                                ukc_val = row.get("UKC_min_m")
                            if ukc_val is None:
                                ukc_val = row.get("UKC_m")
                            stage_name = str(row.get("Stage", "")).strip()
                            if not stage_name:
                                continue
                            stage_drafts[stage_name] = {
                                "fwd": row.get("Draft_FWD_m", 0.0),
                                "aft": row.get("Draft_AFT_m", 0.0),
                                "trim": (
                                    (
                                        row.get("Input_Trim_cm", 0.0)
                                        or row.get("Trim_cm", 0.0)
                                    )
                                    / 100.0
                                ),
                                "ukc": ukc_val if ukc_val is not None else 0.0,
                            }

                    tank_catalog_df = None
                    if tank_catalog.exists():
                        import json

                        tank_data = json.loads(tank_catalog.read_text(encoding="utf-8"))
                        if isinstance(tank_data, dict) and "tanks" in tank_data:
                            tank_catalog_df = pd.DataFrame(tank_data["tanks"])

                    # ìˆ˜ì •: Tank SSOTì˜ ì´ˆê¸° Current_të¥¼ tank_catalog_dfì— ë³‘í•©
                    # ì´ë ‡ê²Œ í•˜ë©´ Sequence generatorê°€ Solverì™€ ë™ì¼í•œ ì´ˆê¸° ìƒíƒœë¥¼ ì‚¬ìš©í•¨
                    if tank_ssot_csv.exists() and tank_catalog_df is not None:
                        try:
                            tank_ssot_df = pd.read_csv(
                                tank_ssot_csv, encoding="utf-8-sig"
                            )
                            if (
                                "Tank" in tank_ssot_df.columns
                                and "Current_t" in tank_ssot_df.columns
                            ):
                                # tank_catalog_dfì— Current_t ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                                if "Current_t" not in tank_catalog_df.columns:
                                    tank_catalog_df["Current_t"] = 0.0

                                # Tank SSOTì˜ Current_të¡œ ì—…ë°ì´íŠ¸ (Tank ì´ë¦„ ë§¤ì¹­)
                                # tank_catalog_dfì˜ "id" ì»¬ëŸ¼ê³¼ tank_ssot_dfì˜ "Tank" ì»¬ëŸ¼ ë§¤ì¹­
                                if "id" in tank_catalog_df.columns:
                                    for _, ssot_row in tank_ssot_df.iterrows():
                                        tank_name = str(ssot_row["Tank"]).strip()
                                        current_t = float(
                                            ssot_row.get("Current_t", 0.0)
                                        )

                                        # ì •í™•í•œ ë§¤ì¹­ ë˜ëŠ” base ë§¤ì¹­ (ì˜ˆ: "FWB2.S" ë˜ëŠ” "FWB2")
                                        mask = (
                                            tank_catalog_df["id"].str.strip()
                                            == tank_name
                                        )
                                        if not mask.any():
                                            # Base ë§¤ì¹­ ì‹œë„ (ì˜ˆ: "FWB2.S" -> "FWB2")
                                            base_name = (
                                                tank_name.split(".", 1)[0]
                                                if "." in tank_name
                                                else tank_name
                                            )
                                            mask = (
                                                tank_catalog_df["id"]
                                                .str.strip()
                                                .str.startswith(base_name)
                                            )

                                        if mask.any():
                                            tank_catalog_df.loc[mask, "Current_t"] = (
                                                current_t
                                            )

                                print(
                                    f"[OK] Tank SSOT Current_t ë³‘í•©: {tank_ssot_csv.name} â†’ tank_catalog_df"
                                )
                        except Exception as e:
                            print(
                                f"[WARN] Tank SSOT Current_t ë³‘í•© ì‹¤íŒ¨: {type(e).__name__}: {e}"
                            )

                    import sys as _sys
                    from pathlib import Path as _Path

                    _base_dir = (
                        _Path(__file__).parent.parent
                        if _Path(__file__).parent.name == "tide"
                        else _Path(__file__).parent
                    )
                    if str(_base_dir) not in _sys.path:
                        _sys.path.insert(0, str(_base_dir))

                    profile_for_seq = None
                    try:
                        from ssot.gates_loader import SiteProfile, load_agi_profile

                        if resolved_profile_path and resolved_profile_path.exists():
                            profile_for_seq = SiteProfile(str(resolved_profile_path))
                        else:
                            profile_for_seq = load_agi_profile()
                    except Exception as e:
                        print(
                            f"[WARN] SiteProfile load failed for Step 4b: {type(e).__name__}: {e}"
                        )

                    _sys.path.insert(0, str(seq_gen_script.parent))
                    from ballast_sequence_generator import (
                        export_to_csv,
                        export_to_dataframe,
                        export_exec_to_csv,
                        export_option_to_csv,
                        export_to_exec_dataframe,
                        export_to_option_dataframe,
                        generate_option_plan,
                        generate_sequence_with_carryforward,
                    )

                    option_plan = generate_option_plan(
                        ballast_plan_df=ballast_plan_df,
                        profile=profile_for_seq,
                        stage_drafts=stage_drafts,
                        tank_catalog_df=tank_catalog_df,
                        fallback_plan_df=fallback_plan_df,
                    )

                    option_csv = out_dir / "BALLAST_OPTION.csv"
                    export_option_to_csv(
                        option_plan,
                        str(option_csv),
                        registry_path=(
                            registry_path if args.enable_headers_ssot else None
                        ),
                    )
                    print(
                        f"[OK] Ballast option plan: {option_csv.name} ({len(option_plan)} options)"
                    )

                    # -------------------------------------------------------------------------
                    # Tidying: BALLAST_OPTION.csv
                    # -------------------------------------------------------------------------
                    if option_csv.exists():
                        validator_registry_path = (
                            registry_path
                            if args.enable_headers_ssot and registry_path
                            else base_dir / "headers_registry.json"
                        )
                        if validator_registry_path.exists():
                            try:
                                from ssot.data_quality_validator import (
                                    DataQualityValidator,
                                )

                                validator = DataQualityValidator(
                                    registry_path=validator_registry_path
                                )
                                tank_catalog_path = (
                                    ssot_dir / "tank_ssot_for_solver.csv"
                                    if ssot_dir.exists()
                                    else None
                                )
                                tidying_results = (
                                    validator.tidy_and_validate_ballast_sequence(
                                        file_path=option_csv,
                                        tank_catalog_path=(
                                            tank_catalog_path
                                            if tank_catalog_path
                                            and tank_catalog_path.exists()
                                            else None
                                        ),
                                        deliverable_id="BALLAST_OPTION_CSV",
                                    )
                                )
                                if tidying_results["llm_context"]:
                                    print(
                                        f"[OK] Option CSV tidying: {tidying_results['validated_count']}/{tidying_results['total_rows']} rows validated"
                                    )
                                validation_report_path = (
                                    out_dir / "tidying_BALLAST_OPTION_report.json"
                                )
                                import json

                                with open(
                                    validation_report_path, "w", encoding="utf-8"
                                ) as f:
                                    json.dump(
                                        tidying_results,
                                        f,
                                        indent=2,
                                        ensure_ascii=False,
                                        default=str,
                                    )
                            except Exception as e:
                                print(
                                    f"[WARN] Option CSV tidying failed: {type(e).__name__}: {e}"
                                )

                    sequence = generate_sequence_with_carryforward(
                        ballast_plan_df=ballast_plan_df,
                        profile=profile_for_seq,
                        stage_drafts=stage_drafts,
                        tank_catalog_df=tank_catalog_df,
                        exclude_optional_stages=True,
                        fallback_plan_df=fallback_plan_df,
                    )

                    # Find optimizer Excel for Delta_t supplementation
                    optimizer_excel = out_dir / "optimizer_ballast_plan.xlsx"
                    if not optimizer_excel.exists():
                        optimizer_excel = None

                    exec_csv = out_dir / "BALLAST_EXEC.csv"
                    export_exec_to_csv(
                        sequence,
                        str(exec_csv),
                        optimizer_excel_path=optimizer_excel,
                        registry_path=(
                            registry_path if args.enable_headers_ssot else None
                        ),
                    )
                    print(
                        f"[OK] Ballast execution sequence: {exec_csv.name} ({len(sequence)} steps)"
                    )

                    # -------------------------------------------------------------------------
                    # Step 4b-1: Data Tidying & Validation (Tidying First Implementation)
                    # -------------------------------------------------------------------------
                    if exec_csv.exists():
                        try:
                            # Try to import DataQualityValidator with base_dir in path
                            import sys as _sys

                            if str(base_dir) not in _sys.path:
                                _sys.path.insert(0, str(base_dir))

                            try:
                                from ssot.data_quality_validator import (
                                    DataQualityValidator,
                                )
                            except ImportError:
                                DataQualityValidator = None

                            if DataQualityValidator is None:
                                print(
                                    "[WARN] DataQualityValidator not available (Pydantic not installed?), skipping tidying"
                                )
                            else:
                                print("\n" + "-" * 80)
                                print("[STEP 4b-1] Data Tidying & Validation")
                                print("-" * 80)

                                # Initialize validator
                                validator_registry_path = (
                                    registry_path
                                    if args.enable_headers_ssot and registry_path
                                    else base_dir / "headers_registry.json"
                                )

                                if not validator_registry_path.exists():
                                    print(
                                        f"[WARN] Registry not found: {validator_registry_path}, skipping tidying"
                                    )
                                else:
                                    validator = DataQualityValidator(
                                        registry_path=validator_registry_path
                                    )

                                # Tank catalog path
                                tank_catalog_path = (
                                    ssot_dir / "tank_ssot_for_solver.csv"
                                    if ssot_dir.exists()
                                    else None
                                )

                                # Run tidying and validation
                                tidying_results = (
                                    validator.tidy_and_validate_ballast_sequence(
                                        file_path=exec_csv,
                                        tank_catalog_path=(
                                            tank_catalog_path
                                            if tank_catalog_path
                                            and tank_catalog_path.exists()
                                            else None
                                        ),
                                        deliverable_id="BALLAST_EXEC_CSV",
                                    )
                                )

                                # Report results
                                if tidying_results["tidying"]["errors"]:
                                    print(
                                        f"[ERROR] Tidying failed: {len(tidying_results['tidying']['errors'])} errors"
                                    )
                                    for err in tidying_results["tidying"]["errors"][:5]:
                                        print(f"  - {err}")

                                if tidying_results["validation"]["errors"]:
                                    print(
                                        f"[ERROR] Validation failed: {len(tidying_results['validation']['errors'])} errors"
                                    )
                                    for err in tidying_results["validation"]["errors"][
                                        :10
                                    ]:
                                        print(f"  - {err}")

                                if tidying_results["cross_validation"]["errors"]:
                                    print(
                                        f"[ERROR] Cross-validation failed: {len(tidying_results['cross_validation']['errors'])} errors"
                                    )
                                    for err in tidying_results["cross_validation"][
                                        "errors"
                                    ][:10]:
                                        print(f"  - {err}")

                                # Save validation report
                                validation_report_path = (
                                    out_dir / "tidying_validation_report.json"
                                )

                                with open(
                                    validation_report_path, "w", encoding="utf-8"
                                ) as f:
                                    json.dump(
                                        tidying_results,
                                        f,
                                        indent=2,
                                        ensure_ascii=False,
                                        default=str,
                                    )

                                # Final status
                                if tidying_results["llm_context"]:
                                    print(
                                        f"[OK] Tidying complete: {tidying_results['validated_count']}/{tidying_results['total_rows']} rows validated"
                                    )
                                    print(
                                        f"[OK] LLM context generated ({len(tidying_results['llm_context'])} chars)"
                                    )
                                    print(
                                        f"[OK] Validation report: {validation_report_path.name}"
                                    )
                                    # LLM context is available in tidying_results["llm_context"]
                                    # Can be used for downstream LLM/AI analysis
                                else:
                                    print(
                                        f"[WARN] Validation failed, LLM input blocked (fail-fast)"
                                    )
                                    print(
                                        f"[INFO] See {validation_report_path.name} for details"
                                    )
                        except Exception as e:
                            print(
                                f"[WARN] Tidying pipeline failed: {type(e).__name__}: {e}"
                            )
                            traceback.print_exc()
                    elif exec_csv.exists() and DataQualityValidator is None:
                        print(
                            "[WARN] DataQualityValidator not available (Pydantic not installed?), skipping tidying"
                        )

                    seq_csv = out_dir / "BALLAST_SEQUENCE.csv"
                    export_to_csv(sequence, str(seq_csv))
                    print(
                        f"[OK] Ballast sequence: {seq_csv.name} ({len(sequence)} steps)"
                    )

                    try:
                        seq_xlsx = out_dir / "BALLAST_SEQUENCE.xlsx"
                        with pd.ExcelWriter(seq_xlsx, engine="openpyxl") as writer:
                            df_option = export_to_option_dataframe(option_plan)
                            df_exec = export_to_exec_dataframe(
                                sequence, optimizer_excel_path=optimizer_excel
                            )
                            df_legacy = export_to_dataframe(sequence)
                            df_option.to_excel(
                                writer, index=False, sheet_name="Ballast_Option"
                            )
                            df_exec.to_excel(
                                writer, index=False, sheet_name="Ballast_Exec"
                            )
                            df_legacy.to_excel(
                                writer, index=False, sheet_name="Ballast_Sequence"
                            )
                        print(
                            f"[OK] Ballast sequence Excel: {seq_xlsx.name} (Option + Exec + Legacy)"
                        )
                    except Exception as e:
                        print(f"[WARN] Sequence Excel export failed: {e}")

                    if checklist_gen_script.exists():
                        _sys.path.insert(0, str(checklist_gen_script.parent))
                        from checklist_generator import generate_checklist

                        checklist_md = out_dir / "BALLAST_OPERATIONS_CHECKLIST.md"
                        generate_checklist(
                            sequence=sequence,
                            profile=profile_for_seq,
                            output_path=str(checklist_md),
                        )
                        print(f"[OK] Operations checklist: {checklist_md.name}")
                    else:
                        print(
                            f"[WARN] Checklist generator not found: {checklist_gen_script}"
                        )

                    hold_points = [s for s in sequence if s.hold_point]
                    if hold_points:
                        hp_summary = out_dir / "HOLD_POINT_SUMMARY.csv"
                        hp_data = []
                        for hp in hold_points:
                            hp_data.append(
                                {
                                    "Step": hp.step,
                                    "Stage": hp.stage,
                                    "Tank": hp.tank,
                                    "Action": hp.action,
                                    "Delta_t": hp.delta_t,
                                    "Draft_FWD_m": hp.draft_fwd,
                                    "Draft_AFT_m": hp.draft_aft,
                                    "Tolerance_cm": 2.0,
                                    "Critical_Tolerance_cm": 4.0,
                                }
                            )
                        pd.DataFrame(hp_data).to_csv(
                            hp_summary, index=False, encoding="utf-8-sig"
                        )
                        print(f"[OK] Hold point summary: {hp_summary.name}")

        except Exception as e:
            print(f"[ERROR] Step 4b failed: {type(e).__name__}: {e}")
            traceback.print_exc()

    # -------------------------------------------------------------------------
    # Step 4c: Valve Lineup Generation (optional)
    # -------------------------------------------------------------------------
    if (
        args.enable_valve_lineup
        and args.enable_sequence
        and args.from_step <= 4 <= args.to_step
    ):
        print("\n" + "=" * 80)
        print("[STEP 4c] Valve Lineup Generation")
        print("=" * 80)

        try:
            seq_csv = out_dir / "BALLAST_SEQUENCE.csv"
            if not seq_csv.exists():
                print("[WARN] BALLAST_SEQUENCE.csv not found. Run Step 4b first.")
            else:
                valve_gen_script = resolve_script_path(
                    base_dir, "valve_lineup_generator.py", label="VALVE_GEN"
                )
                valve_map_path = base_dir / "valve_map.json"

                if not valve_map_path.exists():
                    print(f"[WARN] valve_map.json not found: {valve_map_path}")
                elif not valve_gen_script.exists():
                    print(
                        f"[WARN] valve_lineup_generator.py not found: {valve_gen_script}"
                    )
                else:
                    import sys as _sys

                    _sys.path.insert(0, str(valve_gen_script.parent))
                    from valve_lineup_generator import ValveLineupGenerator

                    vlg = ValveLineupGenerator(str(valve_map_path))
                    valve_output_md = out_dir / "BALLAST_SEQUENCE_WITH_VALVES.md"
                    vlg.enhance_ballast_sequence_with_valves(
                        sequence_csv=str(seq_csv),
                        output_path=str(valve_output_md),
                    )
                    print(f"[OK] Valve lineup: {valve_output_md.name}")

                    seq_xlsx = out_dir / "BALLAST_SEQUENCE.xlsx"
                    if seq_xlsx.exists():
                        try:
                            import pandas as pd
                            from openpyxl import load_workbook

                            wb = load_workbook(seq_xlsx)
                            ws = wb["Ballast_Sequence"]

                            header_row = 1
                            max_col = ws.max_column
                            valve_seq_col = None
                            valve_notes_col = None
                            for c in range(1, max_col + 1):
                                cell_val = ws.cell(header_row, c).value
                                if cell_val == "Valve_Sequence":
                                    valve_seq_col = c
                                if cell_val == "Valve_Notes":
                                    valve_notes_col = c

                            if valve_seq_col is None:
                                valve_seq_col = max_col + 1
                                ws.cell(header_row, valve_seq_col).value = (
                                    "Valve_Sequence"
                                )
                            if valve_notes_col is None:
                                valve_notes_col = max_col + 2
                                ws.cell(header_row, valve_notes_col).value = (
                                    "Valve_Notes"
                                )

                            sequence_df = pd.read_csv(seq_csv, encoding="utf-8-sig")
                            # OPTIMIZED: Build lookup dict first, then batch update
                            step_to_valve = {}
                            for _, row in sequence_df.iterrows():
                                tank_id = str(row.get("Tank", ""))
                                action = str(row.get("Action", ""))
                                step_num = row.get("Step", 0)
                                if action not in ["FILL", "DISCHARGE"]:
                                    continue
                                valve_info = vlg.get_tank_valves(tank_id, action)
                                if not valve_info.get("found"):
                                    continue
                                valve_seq = "\n".join(
                                    f"{idx + 1}. {s}"
                                    for idx, s in enumerate(
                                        valve_info.get("sequence", [])
                                    )
                                )
                                valve_notes = valve_info.get("notes", "")
                                step_to_valve[step_num] = (valve_seq, valve_notes)

                            # Batch read step numbers (values_only for performance)
                            step_col_values = []
                            for r in range(2, ws.max_row + 1):
                                step_val = ws.cell(r, 1).value
                                step_col_values.append((r, step_val))

                            # Batch update matching rows
                            updates = []
                            for r, step_val in step_col_values:
                                if step_val in step_to_valve:
                                    valve_seq, valve_notes = step_to_valve[step_val]
                                    updates.append((r, valve_seq_col, valve_seq))
                                    updates.append((r, valve_notes_col, valve_notes))

                            # Apply all updates
                            for r, col, val in updates:
                                ws.cell(r, col).value = val

                            wb.save(seq_xlsx)
                            print(f"[OK] Valve details added to Excel: {seq_xlsx.name}")
                        except Exception as e:
                            print(f"[WARN] Failed to update sequence Excel: {e}")

        except Exception as e:
            print(f"[ERROR] Step 4c failed: {type(e).__name__}: {e}")
            traceback.print_exc()

    # -------------------------------------------------------------------------
    # Step 5: Bryan Template (optional)
    # -------------------------------------------------------------------------
    if args.from_step <= 5 <= args.to_step:
        print("\n" + "=" * 80)
        print("[STEP 5] Bryan Template Integration (optional)")
        print("=" * 80)
        if not bryan_template_script.exists():
            print(f"[WARN] Bryan template script not found: {bryan_template_script}")
        elif not stage_results_path.exists():
            print(
                f"[WARN] stage_results.csv missing for Bryan template: {stage_results_path}"
            )
        else:
            bryan_template_out = out_dir / "Bryan_Submission_Data_Pack_Template.xlsx"
            bryan_populated_out = out_dir / "Bryan_Submission_Data_Pack_Populated.xlsx"
            bryan_args = [
                "one-click",
                "--stage-csv",
                str(stage_results_path),
                "--out-template",
                str(bryan_template_out),
                "--out-populated",
                str(bryan_populated_out),
                "--mode",
                "overwrite",
                "--validate",
            ]
            if spmt_out_xlsx.exists():
                bryan_args += ["--spmt-xlsx", str(spmt_out_xlsx)]
            r5 = step_run_script(
                6,
                "BRYAN_TEMPLATE",
                bryan_template_script,
                args=bryan_args,
                cwd=base_dir,
                out_dir=out_dir,
            )
            if not r5.ok:
                print(f"[WARN] Step-5 failed (rc={r5.returncode}). Log: {r5.log}")
            else:
                print(f"[OK] Step-5 completed. Log: {r5.log}")
                print(f"     Template: {bryan_template_out.name}")
                print(f"     Populated: {bryan_populated_out.name}")

    # -------------------------------------------------------------------------
    # Collect all output files to output directory
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("COLLECTING ALL OUTPUT FILES")
    print("=" * 80)
    collect_all_output_files(
        base_dir=base_dir, out_dir=out_dir, site=args.site, inputs_dir=inputs_dir
    )

    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("PIPELINE COMPLETE")
    print("=" * 80)
    print(f"Out dir: {out_dir}")
    print("Key SSOT outputs:")
    print(f" - {tank_ssot_csv.name}")
    print(f" - {hydro_ssot_csv.name}")
    print(f" - {stage_table_csv.name}")
    print(f" - pipeline_stage_QA.csv (definition-split QA)")

    # -------------------------------------------------------------------------
    # Merge all Excel files into one consolidated file
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("MERGING EXCEL FILES")
    print("=" * 80)
    merged_excel = merge_excel_files_to_one(
        out_dir=out_dir,
        site=args.site,
        base_dir=base_dir,
        include_sequence=args.enable_sequence,
    )
    if merged_excel:
        print(f"[OK] Consolidated Excel: {merged_excel.name}")

        # Optional: Excel COM full recalc to cache formula results (requires Excel + pywin32)
        com_flag = os.environ.get("EXCEL_COM_RECALC", "").strip().lower()
        com_enabled = com_flag in ("1", "true", "yes", "y")
        print(f"  [INFO] EXCEL_COM_RECALC={com_flag!r} -> enabled={com_enabled}")
        if com_enabled:
            try:
                com_script = Path(__file__).parent / "excel_com_recalc_save.py"
                out_path = os.environ.get("EXCEL_COM_RECALC_OUT", "").strip()
                if not com_script.exists():
                    print(f"  [WARN] Excel COM recalc script not found: {com_script}")
                else:
                    cmd = [sys.executable, str(com_script), "--in", str(merged_excel)]
                    if out_path:
                        cmd += ["--out", out_path]
                    print(f"  [INFO] Running Excel COM recalc: {merged_excel.name}")
                    r = subprocess.run(cmd, check=False, capture_output=True, text=True)
                    if r.stdout:
                        print(r.stdout.rstrip())
                    if r.stderr:
                        print(r.stderr.rstrip())
                    print(f"  [INFO] Excel COM recalc returncode={r.returncode}")
                    if r.returncode == 0:
                        print("  [OK] Excel COM recalc completed")
                    else:
                        print("  [WARN] Excel COM recalc failed")
            except Exception as e:
                print(f"  [WARN] Excel COM recalc skipped: {e}")

        # Inject solver results after COM recalc (if any)
        try:
            inject_all = bool(
                int(os.environ.get("EXCEL_INJECT_ALL_SHEETS", "0") or "0")
            )
            inject_debug = bool(int(os.environ.get("EXCEL_INJECT_DEBUG", "0") or "0"))
            update_excel_with_solver_results(
                merged_excel_path=merged_excel,
                qa_csv_path=stage_qa_csv,
                sheet_hint="RORO",
                update_all_matching_sheets=inject_all,
                debug=inject_debug,
            )
        except Exception as e:
            print(f"[WARN] Solver result injection skipped: {e}")

    # -------------------------------------------------------------------------
    # Ensure Gate FAIL report is generated (final check before completion)
    # -------------------------------------------------------------------------
    if not args.no_gate_report:
        gate_report_md = out_dir / "gate_fail_report.md"
        if not gate_report_md.exists():
            print("\n" + "=" * 80)
            print("FINAL GATE FAIL REPORT GENERATION")
            print("=" * 80)
            print(f"[WARN] Gate FAIL report not found. Regenerating...")
            try:
                ukc_inputs = {
                    "forecast_tide_m": args.forecast_tide,
                    "depth_ref_m": args.depth_ref,
                    "ukc_min_m": args.ukc_min,
                    "squat_m": float(args.squat),
                    "safety_allow_m": float(args.safety_allow),
                }
                generate_gate_fail_report_md(
                    out_md=gate_report_md,
                    site=str(args.site),
                    profile_path=resolved_profile_path,
                    stage_qa_csv=stage_qa_csv,
                    tank_ssot_csv=tank_ssot_csv,
                    sensor_stats=sensor_stats,
                    ukc_inputs=ukc_inputs,
                )
                print(f"[OK] Gate FAIL report regenerated: {gate_report_md}")

                try:
                    append_dnv_mitigation_section_to_gate_report(
                        report_md=gate_report_md,
                        stage_qa_csv=stage_qa_csv,
                        propeller_diameter_m=1.38,
                    )
                    print("[OK] DNV mitigation section appended to gate report")
                except Exception as e:
                    print(
                        f"[WARN] Failed to append DNV mitigation: {type(e).__name__}: {e}"
                    )
                try:
                    tug_sop_md = out_dir / "TUG_Operational_SOP_DNV_ST_N001.md"
                    generate_tug_operational_sop_md(
                        out_md=tug_sop_md,
                        stage_qa_csv=stage_qa_csv,
                        site=str(args.site),
                    )
                    print(f"[OK] TUG Operational SOP: {tug_sop_md}")
                except Exception as e:
                    print(f"[WARN] Failed to generate TUG SOP: {type(e).__name__}: {e}")

                # Append solver outputs if Step 3 was executed
                solver_out_sum = out_dir / "solver_ballast_summary.csv"
                solver_out_plan = out_dir / "solver_ballast_plan.csv"
                solver_out_stage_plan = out_dir / "solver_ballast_stage_plan.csv"
                if solver_out_sum.exists() and solver_out_plan.exists():
                    try:
                        append_solver_section_to_gate_report(
                            report_md=gate_report_md,
                            solver_out_summary=solver_out_sum,
                            solver_out_plan=solver_out_plan,
                            solver_out_stage_plan=solver_out_stage_plan,
                        )
                        print(f"[OK] Solver results appended to Gate FAIL report")
                    except Exception as e:
                        print(
                            f"[WARN] Failed to append solver section: {type(e).__name__}: {e}"
                        )
            except Exception as e:
                print(
                    f"[ERROR] Gate FAIL report generation failed: {type(e).__name__}: {e}"
                )
                traceback.print_exc()
        else:
            print(f"[OK] Gate FAIL report exists: {gate_report_md}")

    # -------------------------------------------------------------------------
    # Final output consolidation + manifest
    # -------------------------------------------------------------------------
    final_dir = None
    try:
        final_dir = create_final_output_folder(
            base_dir=base_dir, out_dir=out_dir, merged_excel=merged_excel
        )
    except Exception as e:
        print(f"[WARN] Final output consolidation failed: {type(e).__name__}: {e}")
        final_dir = None

    # -------------------------------------------------------------------------
    # Head Guard Validation (optional post-processing)
    # -------------------------------------------------------------------------
    if args.auto_head_guard and head_registry_path and head_registry_path.exists() and final_dir:
        print("\n" + "=" * 80)
        print("[POST-STEP] Head Guard Validation")
        print("=" * 80)

        try:
            head_guard_script = resolve_script_path(
                base_dir, "Head Guard.py", label="HEAD_GUARD"
            )

            if head_guard_script.exists():
                head_guard_cmd = [
                    which_python(),
                    str(head_guard_script),
                    "--registry",
                    str(head_registry_path),
                    "--final-dir",
                    str(final_dir),
                    "--manifest",
                    "HEAD_MANIFEST.json",
                ]

                result = subprocess.run(
                    head_guard_cmd,
                    cwd=base_dir,
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    timeout=300,
                )

                if result.returncode == 0:
                    print("[OK] Head Guard validation completed")
                    if result.stdout:
                        lines = result.stdout.split("\n")
                        in_summary = False
                        for line in lines:
                            if "Head Guard Validation Summary" in line:
                                in_summary = True
                            if in_summary and line.strip():
                                if line.startswith("  ") or ":" in line:
                                    print(line)
                            if in_summary and line.startswith("="):
                                break
                else:
                    print(
                        f"[WARN] Head Guard validation failed (non-critical): {result.returncode}"
                    )
                    if result.stderr:
                        print(f"[ERROR] {result.stderr[:500]}")
            else:
                print(f"[WARN] Head Guard script not found: {head_guard_script}")
                print(
                    "[INFO] You can run manually: python \"Head Guard.py\" --registry HEAD_REGISTRY_AGI_v3.0.yaml --final-dir <final_output_dir>"
                )
        except subprocess.TimeoutExpired:
            print("[WARN] Head Guard validation timeout (300s) - skipping")
        except Exception as e:
            print(
                f"[WARN] Head Guard execution failed (non-critical): {type(e).__name__}: {e}"
            )

    # -------------------------------------------------------------------------
    # Excel formula preservation (COM post-processing)
    # -------------------------------------------------------------------------
    if merged_excel and merged_excel.exists():
        print("\n" + "=" * 80)
        print("[FORMULA FINALIZE] Running COM post-processing...")
        print("=" * 80)

        import subprocess

        finalize_script = Path(__file__).parent / "ballast_excel_finalize.py"

        if finalize_script.exists():
            try:
                result = subprocess.run(
                    [sys.executable, str(finalize_script), str(merged_excel)],
                    capture_output=True,
                    text=True,
                    timeout=FORMULA_TIMEOUT_SEC,
                )

                if result.returncode == 0:
                    print("[OK] Formula finalization completed successfully")
                    for line in result.stdout.split("\n"):
                        if "[RESULT]" in line or "SUCCESS" in line:
                            print(f"  {line}")
                else:
                    print("[WARN] Formula finalization failed (non-critical)")
                    if result.stderr:
                        print(f"[ERROR] {result.stderr[:500]}")
            except subprocess.TimeoutExpired:
                print(
                    f"[WARN] Formula finalization timeout ({FORMULA_TIMEOUT_SEC}s) - skipping"
                )
            except Exception as e:
                print(f"[WARN] Formula finalization error: {e}")
        else:
            print(f"[INFO] Formula finalization script not found: {finalize_script}")
            print(
                "[INFO] You can run manually: python ballast_excel_finalize.py --auto"
            )

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
